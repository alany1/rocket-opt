{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "286c469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from matplotlib import pyplot as plt\n",
    "from tools import *\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c1df36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getObs(n):\n",
    "    obs = {}\n",
    "    for i in range(n):\n",
    "        try:\n",
    "            df = pd.read_csv(f'rocket-results/{i}.csv')\n",
    "        except:\n",
    "            print('Missing', i)\n",
    "        obs[i] = df\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a7ca1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeConfig():\n",
    "    configs = {}\n",
    "    with open('sample_list.pkl', 'rb') as f:\n",
    "        d = pickle.load(f)\n",
    "    configs.update(d)\n",
    "    with open('sample_list_100.pkl', 'rb') as f:\n",
    "        d = pickle.load(f)\n",
    "    configs.update(d)\n",
    "    with open('sample_list_200.pkl', 'rb') as f:\n",
    "        d = pickle.load(f)\n",
    "    configs.update(d)\n",
    "    with open('sample_list_300.pkl', 'rb') as f:\n",
    "        d = pickle.load(f)\n",
    "    configs.update(d)\n",
    "    return configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a6d14f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = initializeConfig()\n",
    "obs = getObs(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bebc4357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ff/y3gj2gld0zqfjtq_lhxz03t00000gn/T/ipykernel_26772/1048845639.py:5: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  data[i].append(obs[i].mean()['Stability Margin (cal)'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.465233</td>\n",
       "      <td>9.191411</td>\n",
       "      <td>6.313146</td>\n",
       "      <td>6.181568</td>\n",
       "      <td>4.253219</td>\n",
       "      <td>3.362766</td>\n",
       "      <td>2.004250</td>\n",
       "      <td>3.767645</td>\n",
       "      <td>0.004903</td>\n",
       "      <td>-5.121752</td>\n",
       "      <td>0.0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.050903</td>\n",
       "      <td>2.807665</td>\n",
       "      <td>7.250966</td>\n",
       "      <td>4.202998</td>\n",
       "      <td>7.871765</td>\n",
       "      <td>6.329711</td>\n",
       "      <td>3.563606</td>\n",
       "      <td>6.148326</td>\n",
       "      <td>77117.160000</td>\n",
       "      <td>1.278655</td>\n",
       "      <td>154.9973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.796076</td>\n",
       "      <td>5.131824</td>\n",
       "      <td>8.469040</td>\n",
       "      <td>8.168441</td>\n",
       "      <td>6.226139</td>\n",
       "      <td>7.792487</td>\n",
       "      <td>6.816335</td>\n",
       "      <td>8.795342</td>\n",
       "      <td>51216.000000</td>\n",
       "      <td>5.212006</td>\n",
       "      <td>125.0071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.590219</td>\n",
       "      <td>9.233072</td>\n",
       "      <td>6.352605</td>\n",
       "      <td>8.429602</td>\n",
       "      <td>5.672564</td>\n",
       "      <td>5.705322</td>\n",
       "      <td>5.717624</td>\n",
       "      <td>3.420081</td>\n",
       "      <td>0.004691</td>\n",
       "      <td>-2.489141</td>\n",
       "      <td>0.0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.916229</td>\n",
       "      <td>4.216994</td>\n",
       "      <td>4.711545</td>\n",
       "      <td>6.462613</td>\n",
       "      <td>4.887217</td>\n",
       "      <td>2.638681</td>\n",
       "      <td>9.605809</td>\n",
       "      <td>5.234709</td>\n",
       "      <td>0.004818</td>\n",
       "      <td>-4.231226</td>\n",
       "      <td>0.0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>2.608430</td>\n",
       "      <td>3.346465</td>\n",
       "      <td>5.201403</td>\n",
       "      <td>9.166944</td>\n",
       "      <td>6.192108</td>\n",
       "      <td>3.850187</td>\n",
       "      <td>7.960717</td>\n",
       "      <td>8.586406</td>\n",
       "      <td>70830.750000</td>\n",
       "      <td>1.673402</td>\n",
       "      <td>149.0006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>7.801407</td>\n",
       "      <td>2.628427</td>\n",
       "      <td>5.543288</td>\n",
       "      <td>9.133362</td>\n",
       "      <td>7.014197</td>\n",
       "      <td>5.730589</td>\n",
       "      <td>4.246228</td>\n",
       "      <td>6.930445</td>\n",
       "      <td>72485.310000</td>\n",
       "      <td>1.614970</td>\n",
       "      <td>150.9995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>7.364198</td>\n",
       "      <td>8.455093</td>\n",
       "      <td>4.617628</td>\n",
       "      <td>3.805883</td>\n",
       "      <td>4.280772</td>\n",
       "      <td>6.398586</td>\n",
       "      <td>8.552977</td>\n",
       "      <td>8.138172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.734951</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>7.094305</td>\n",
       "      <td>2.423077</td>\n",
       "      <td>2.934079</td>\n",
       "      <td>6.637124</td>\n",
       "      <td>4.526698</td>\n",
       "      <td>3.664447</td>\n",
       "      <td>4.901074</td>\n",
       "      <td>5.327160</td>\n",
       "      <td>73446.170000</td>\n",
       "      <td>0.853824</td>\n",
       "      <td>150.9995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>3.451784</td>\n",
       "      <td>3.164933</td>\n",
       "      <td>3.213005</td>\n",
       "      <td>9.450705</td>\n",
       "      <td>8.759146</td>\n",
       "      <td>8.331735</td>\n",
       "      <td>2.206001</td>\n",
       "      <td>6.427374</td>\n",
       "      <td>55933.920000</td>\n",
       "      <td>2.408665</td>\n",
       "      <td>131.0084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "0    9.465233  9.191411  6.313146  6.181568  4.253219  3.362766  2.004250   \n",
       "1    5.050903  2.807665  7.250966  4.202998  7.871765  6.329711  3.563606   \n",
       "2    8.796076  5.131824  8.469040  8.168441  6.226139  7.792487  6.816335   \n",
       "3    5.590219  9.233072  6.352605  8.429602  5.672564  5.705322  5.717624   \n",
       "4    2.916229  4.216994  4.711545  6.462613  4.887217  2.638681  9.605809   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "395  2.608430  3.346465  5.201403  9.166944  6.192108  3.850187  7.960717   \n",
       "396  7.801407  2.628427  5.543288  9.133362  7.014197  5.730589  4.246228   \n",
       "397  7.364198  8.455093  4.617628  3.805883  4.280772  6.398586  8.552977   \n",
       "398  7.094305  2.423077  2.934079  6.637124  4.526698  3.664447  4.901074   \n",
       "399  3.451784  3.164933  3.213005  9.450705  8.759146  8.331735  2.206001   \n",
       "\n",
       "           7             8         9         10  \n",
       "0    3.767645      0.004903 -5.121752    0.0100  \n",
       "1    6.148326  77117.160000  1.278655  154.9973  \n",
       "2    8.795342  51216.000000  5.212006  125.0071  \n",
       "3    3.420081      0.004691 -2.489141    0.0100  \n",
       "4    5.234709      0.004818 -4.231226    0.0100  \n",
       "..        ...           ...       ...       ...  \n",
       "395  8.586406  70830.750000  1.673402  149.0006  \n",
       "396  6.930445  72485.310000  1.614970  150.9995  \n",
       "397  8.138172      0.000000 -1.734951    0.0000  \n",
       "398  5.327160  73446.170000  0.853824  150.9995  \n",
       "399  6.427374  55933.920000  2.408665  131.0084  \n",
       "\n",
       "[400 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {}\n",
    "for i in range(len(config)):\n",
    "    data[i] = list(config[i]['S'].values()) + list(config[i]['B'].values())\n",
    "    data[i].append(obs[i].max()['Altitude (ft)'])\n",
    "    data[i].append(obs[i].mean()['Stability Margin (cal)'])\n",
    "    data[i].append(obs[i].max()['Time (sec)'])\n",
    "df = pd.DataFrame.from_dict(data, orient='index')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a439b409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Schord</th>\n",
       "      <th>Sspan</th>\n",
       "      <th>Ssweep</th>\n",
       "      <th>Stip</th>\n",
       "      <th>Bchord</th>\n",
       "      <th>Bspan</th>\n",
       "      <th>Bsweep</th>\n",
       "      <th>Btip</th>\n",
       "      <th>Schordspan</th>\n",
       "      <th>Schordsweep</th>\n",
       "      <th>...</th>\n",
       "      <th>SSweeptip</th>\n",
       "      <th>Bchordspan</th>\n",
       "      <th>Bchordsweep</th>\n",
       "      <th>Bchordtip</th>\n",
       "      <th>Bspansweep</th>\n",
       "      <th>BSpantip</th>\n",
       "      <th>BSweeptip</th>\n",
       "      <th>Altitude</th>\n",
       "      <th>Stability</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.465233</td>\n",
       "      <td>9.191411</td>\n",
       "      <td>6.313146</td>\n",
       "      <td>6.181568</td>\n",
       "      <td>4.253219</td>\n",
       "      <td>3.362766</td>\n",
       "      <td>2.004250</td>\n",
       "      <td>3.767645</td>\n",
       "      <td>86.998844</td>\n",
       "      <td>59.755399</td>\n",
       "      <td>...</td>\n",
       "      <td>39.025144</td>\n",
       "      <td>14.302581</td>\n",
       "      <td>8.524515</td>\n",
       "      <td>16.024619</td>\n",
       "      <td>6.739826</td>\n",
       "      <td>12.669711</td>\n",
       "      <td>7.551304</td>\n",
       "      <td>0.004903</td>\n",
       "      <td>-5.121752</td>\n",
       "      <td>0.0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.050903</td>\n",
       "      <td>2.807665</td>\n",
       "      <td>7.250966</td>\n",
       "      <td>4.202998</td>\n",
       "      <td>7.871765</td>\n",
       "      <td>6.329711</td>\n",
       "      <td>3.563606</td>\n",
       "      <td>6.148326</td>\n",
       "      <td>14.181241</td>\n",
       "      <td>36.623926</td>\n",
       "      <td>...</td>\n",
       "      <td>30.475799</td>\n",
       "      <td>49.826002</td>\n",
       "      <td>28.051867</td>\n",
       "      <td>48.398181</td>\n",
       "      <td>22.556595</td>\n",
       "      <td>38.917130</td>\n",
       "      <td>21.910210</td>\n",
       "      <td>77117.160000</td>\n",
       "      <td>1.278655</td>\n",
       "      <td>154.9973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.796076</td>\n",
       "      <td>5.131824</td>\n",
       "      <td>8.469040</td>\n",
       "      <td>8.168441</td>\n",
       "      <td>6.226139</td>\n",
       "      <td>7.792487</td>\n",
       "      <td>6.816335</td>\n",
       "      <td>8.795342</td>\n",
       "      <td>45.139920</td>\n",
       "      <td>74.494323</td>\n",
       "      <td>...</td>\n",
       "      <td>69.178850</td>\n",
       "      <td>48.517110</td>\n",
       "      <td>42.439452</td>\n",
       "      <td>54.761021</td>\n",
       "      <td>53.116208</td>\n",
       "      <td>68.537590</td>\n",
       "      <td>59.951999</td>\n",
       "      <td>51216.000000</td>\n",
       "      <td>5.212006</td>\n",
       "      <td>125.0071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.590219</td>\n",
       "      <td>9.233072</td>\n",
       "      <td>6.352605</td>\n",
       "      <td>8.429602</td>\n",
       "      <td>5.672564</td>\n",
       "      <td>5.705322</td>\n",
       "      <td>5.717624</td>\n",
       "      <td>3.420081</td>\n",
       "      <td>51.614896</td>\n",
       "      <td>35.512459</td>\n",
       "      <td>...</td>\n",
       "      <td>53.549937</td>\n",
       "      <td>32.363803</td>\n",
       "      <td>32.433584</td>\n",
       "      <td>19.400628</td>\n",
       "      <td>32.620884</td>\n",
       "      <td>19.512664</td>\n",
       "      <td>19.554736</td>\n",
       "      <td>0.004691</td>\n",
       "      <td>-2.489141</td>\n",
       "      <td>0.0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.916229</td>\n",
       "      <td>4.216994</td>\n",
       "      <td>4.711545</td>\n",
       "      <td>6.462613</td>\n",
       "      <td>4.887217</td>\n",
       "      <td>2.638681</td>\n",
       "      <td>9.605809</td>\n",
       "      <td>5.234709</td>\n",
       "      <td>12.297724</td>\n",
       "      <td>13.739946</td>\n",
       "      <td>...</td>\n",
       "      <td>30.448892</td>\n",
       "      <td>12.895807</td>\n",
       "      <td>46.945675</td>\n",
       "      <td>25.583158</td>\n",
       "      <td>25.346667</td>\n",
       "      <td>13.812727</td>\n",
       "      <td>50.283615</td>\n",
       "      <td>0.004818</td>\n",
       "      <td>-4.231226</td>\n",
       "      <td>0.0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>2.608430</td>\n",
       "      <td>3.346465</td>\n",
       "      <td>5.201403</td>\n",
       "      <td>9.166944</td>\n",
       "      <td>6.192108</td>\n",
       "      <td>3.850187</td>\n",
       "      <td>7.960717</td>\n",
       "      <td>8.586406</td>\n",
       "      <td>8.729021</td>\n",
       "      <td>13.567499</td>\n",
       "      <td>...</td>\n",
       "      <td>47.680974</td>\n",
       "      <td>23.840776</td>\n",
       "      <td>49.293623</td>\n",
       "      <td>53.167952</td>\n",
       "      <td>30.650253</td>\n",
       "      <td>33.059270</td>\n",
       "      <td>68.353950</td>\n",
       "      <td>70830.750000</td>\n",
       "      <td>1.673402</td>\n",
       "      <td>149.0006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>7.801407</td>\n",
       "      <td>2.628427</td>\n",
       "      <td>5.543288</td>\n",
       "      <td>9.133362</td>\n",
       "      <td>7.014197</td>\n",
       "      <td>5.730589</td>\n",
       "      <td>4.246228</td>\n",
       "      <td>6.930445</td>\n",
       "      <td>20.505433</td>\n",
       "      <td>43.245446</td>\n",
       "      <td>...</td>\n",
       "      <td>50.628851</td>\n",
       "      <td>40.195478</td>\n",
       "      <td>29.783881</td>\n",
       "      <td>48.611503</td>\n",
       "      <td>24.333390</td>\n",
       "      <td>39.715531</td>\n",
       "      <td>29.428252</td>\n",
       "      <td>72485.310000</td>\n",
       "      <td>1.614970</td>\n",
       "      <td>150.9995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>7.364198</td>\n",
       "      <td>8.455093</td>\n",
       "      <td>4.617628</td>\n",
       "      <td>3.805883</td>\n",
       "      <td>4.280772</td>\n",
       "      <td>6.398586</td>\n",
       "      <td>8.552977</td>\n",
       "      <td>8.138172</td>\n",
       "      <td>62.264979</td>\n",
       "      <td>34.005123</td>\n",
       "      <td>...</td>\n",
       "      <td>17.574150</td>\n",
       "      <td>27.390888</td>\n",
       "      <td>36.613344</td>\n",
       "      <td>34.837657</td>\n",
       "      <td>54.726959</td>\n",
       "      <td>52.072793</td>\n",
       "      <td>69.605594</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.734951</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>7.094305</td>\n",
       "      <td>2.423077</td>\n",
       "      <td>2.934079</td>\n",
       "      <td>6.637124</td>\n",
       "      <td>4.526698</td>\n",
       "      <td>3.664447</td>\n",
       "      <td>4.901074</td>\n",
       "      <td>5.327160</td>\n",
       "      <td>17.190049</td>\n",
       "      <td>20.815255</td>\n",
       "      <td>...</td>\n",
       "      <td>19.473850</td>\n",
       "      <td>16.587847</td>\n",
       "      <td>22.185683</td>\n",
       "      <td>24.114444</td>\n",
       "      <td>17.959727</td>\n",
       "      <td>19.521095</td>\n",
       "      <td>26.108803</td>\n",
       "      <td>73446.170000</td>\n",
       "      <td>0.853824</td>\n",
       "      <td>150.9995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>3.451784</td>\n",
       "      <td>3.164933</td>\n",
       "      <td>3.213005</td>\n",
       "      <td>9.450705</td>\n",
       "      <td>8.759146</td>\n",
       "      <td>8.331735</td>\n",
       "      <td>2.206001</td>\n",
       "      <td>6.427374</td>\n",
       "      <td>10.924665</td>\n",
       "      <td>11.090597</td>\n",
       "      <td>...</td>\n",
       "      <td>30.365158</td>\n",
       "      <td>72.978885</td>\n",
       "      <td>19.322682</td>\n",
       "      <td>56.298306</td>\n",
       "      <td>18.379814</td>\n",
       "      <td>53.551178</td>\n",
       "      <td>14.178791</td>\n",
       "      <td>55933.920000</td>\n",
       "      <td>2.408665</td>\n",
       "      <td>131.0084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Schord     Sspan    Ssweep      Stip    Bchord     Bspan    Bsweep  \\\n",
       "0    9.465233  9.191411  6.313146  6.181568  4.253219  3.362766  2.004250   \n",
       "1    5.050903  2.807665  7.250966  4.202998  7.871765  6.329711  3.563606   \n",
       "2    8.796076  5.131824  8.469040  8.168441  6.226139  7.792487  6.816335   \n",
       "3    5.590219  9.233072  6.352605  8.429602  5.672564  5.705322  5.717624   \n",
       "4    2.916229  4.216994  4.711545  6.462613  4.887217  2.638681  9.605809   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "395  2.608430  3.346465  5.201403  9.166944  6.192108  3.850187  7.960717   \n",
       "396  7.801407  2.628427  5.543288  9.133362  7.014197  5.730589  4.246228   \n",
       "397  7.364198  8.455093  4.617628  3.805883  4.280772  6.398586  8.552977   \n",
       "398  7.094305  2.423077  2.934079  6.637124  4.526698  3.664447  4.901074   \n",
       "399  3.451784  3.164933  3.213005  9.450705  8.759146  8.331735  2.206001   \n",
       "\n",
       "         Btip  Schordspan  Schordsweep  ...  SSweeptip  Bchordspan  \\\n",
       "0    3.767645   86.998844    59.755399  ...  39.025144   14.302581   \n",
       "1    6.148326   14.181241    36.623926  ...  30.475799   49.826002   \n",
       "2    8.795342   45.139920    74.494323  ...  69.178850   48.517110   \n",
       "3    3.420081   51.614896    35.512459  ...  53.549937   32.363803   \n",
       "4    5.234709   12.297724    13.739946  ...  30.448892   12.895807   \n",
       "..        ...         ...          ...  ...        ...         ...   \n",
       "395  8.586406    8.729021    13.567499  ...  47.680974   23.840776   \n",
       "396  6.930445   20.505433    43.245446  ...  50.628851   40.195478   \n",
       "397  8.138172   62.264979    34.005123  ...  17.574150   27.390888   \n",
       "398  5.327160   17.190049    20.815255  ...  19.473850   16.587847   \n",
       "399  6.427374   10.924665    11.090597  ...  30.365158   72.978885   \n",
       "\n",
       "     Bchordsweep  Bchordtip  Bspansweep   BSpantip  BSweeptip      Altitude  \\\n",
       "0       8.524515  16.024619    6.739826  12.669711   7.551304      0.004903   \n",
       "1      28.051867  48.398181   22.556595  38.917130  21.910210  77117.160000   \n",
       "2      42.439452  54.761021   53.116208  68.537590  59.951999  51216.000000   \n",
       "3      32.433584  19.400628   32.620884  19.512664  19.554736      0.004691   \n",
       "4      46.945675  25.583158   25.346667  13.812727  50.283615      0.004818   \n",
       "..           ...        ...         ...        ...        ...           ...   \n",
       "395    49.293623  53.167952   30.650253  33.059270  68.353950  70830.750000   \n",
       "396    29.783881  48.611503   24.333390  39.715531  29.428252  72485.310000   \n",
       "397    36.613344  34.837657   54.726959  52.072793  69.605594      0.000000   \n",
       "398    22.185683  24.114444   17.959727  19.521095  26.108803  73446.170000   \n",
       "399    19.322682  56.298306   18.379814  53.551178  14.178791  55933.920000   \n",
       "\n",
       "     Stability      Time  \n",
       "0    -5.121752    0.0100  \n",
       "1     1.278655  154.9973  \n",
       "2     5.212006  125.0071  \n",
       "3    -2.489141    0.0100  \n",
       "4    -4.231226    0.0100  \n",
       "..         ...       ...  \n",
       "395   1.673402  149.0006  \n",
       "396   1.614970  150.9995  \n",
       "397  -1.734951    0.0000  \n",
       "398   0.853824  150.9995  \n",
       "399   2.408665  131.0084  \n",
       "\n",
       "[400 rows x 23 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.rename(columns={0: \"Schord\", 1: \"Sspan\", 2: \"Ssweep\", 3: \"Stip\", 4: \"Bchord\", \n",
    "                        5: \"Bspan\", 6: \"Bsweep\", 7: \"Btip\", 8: \"Altitude\", 9: \"Stability\", 10: \"Time\"})\n",
    "df['Schordspan'] = df['Schord'] * df['Sspan']\n",
    "df['Schordsweep'] = df['Schord'] * df['Ssweep']\n",
    "df['Schordtip'] = df['Schord'] * df['Stip']\n",
    "df['Sspansweep'] = df['Sspan'] * df['Ssweep']\n",
    "df['SSpantip'] = df['Sspan'] * df['Stip']\n",
    "df['SSweeptip'] = df['Ssweep'] * df['Stip']\n",
    "\n",
    "df['Bchordspan'] = df['Bchord'] * df['Bspan']\n",
    "df['Bchordsweep'] = df['Bchord'] * df['Bsweep']\n",
    "df['Bchordtip'] = df['Bchord'] * df['Btip']\n",
    "df['Bspansweep'] = df['Bspan'] * df['Bsweep']\n",
    "df['BSpantip'] = df['Bspan'] * df['Btip']\n",
    "df['BSweeptip'] = df['Bsweep'] * df['Btip']\n",
    "alt = df.pop(\"Altitude\")\n",
    "stab = df.pop(\"Stability\")\n",
    "time = df.pop(\"Time\")\n",
    "\n",
    "df.insert(len(df.columns), \"Altitude\", alt)\n",
    "df.insert(len(df.columns), \"Stability\", stab)\n",
    "df.insert(len(df.columns), \"Time\", time)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "012679bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "400b39f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_data(m, c, num_examples):\n",
    " \n",
    "    \"\"\"Generate y = mX + bias(c) + noise\"\"\"\n",
    "\n",
    "    X = torch.normal(0, 1, (num_examples, len(m)))\n",
    "\n",
    "    y = torch.matmul(X, m) + c\n",
    "\n",
    "    y += torch.normal(0, 0.01, y.shape)\n",
    " \n",
    "    return X, y.reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3332c829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.2854,  0.7723],\n",
       "         [-0.2868,  0.3979],\n",
       "         [ 1.1170, -0.8443],\n",
       "         [ 0.6094,  1.2585],\n",
       "         [-1.4640, -0.7778],\n",
       "         [-1.6313, -1.2966],\n",
       "         [-0.1940, -0.8472],\n",
       "         [ 0.0800,  1.1407],\n",
       "         [-1.0706, -0.9298],\n",
       "         [-1.4800,  1.0732]]),\n",
       " tensor([[ 8.6980],\n",
       "         [ 1.6593],\n",
       "         [ 4.7598],\n",
       "         [ 6.9457],\n",
       "         [-5.3984],\n",
       "         [-7.1112],\n",
       "         [-0.4769],\n",
       "         [ 4.6005],\n",
       "         [-4.1380],\n",
       "         [-1.7762]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_data(torch.Tensor([4,2]),2,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6280a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Schord</th>\n",
       "      <th>Sspan</th>\n",
       "      <th>Ssweep</th>\n",
       "      <th>Stip</th>\n",
       "      <th>Bchord</th>\n",
       "      <th>Bspan</th>\n",
       "      <th>Bsweep</th>\n",
       "      <th>Btip</th>\n",
       "      <th>Schordspan</th>\n",
       "      <th>Schordsweep</th>\n",
       "      <th>...</th>\n",
       "      <th>SSweeptip</th>\n",
       "      <th>Bchordspan</th>\n",
       "      <th>Bchordsweep</th>\n",
       "      <th>Bchordtip</th>\n",
       "      <th>Bspansweep</th>\n",
       "      <th>BSpantip</th>\n",
       "      <th>BSweeptip</th>\n",
       "      <th>Altitude</th>\n",
       "      <th>Stability</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.050903</td>\n",
       "      <td>2.807665</td>\n",
       "      <td>7.250966</td>\n",
       "      <td>4.202998</td>\n",
       "      <td>7.871765</td>\n",
       "      <td>6.329711</td>\n",
       "      <td>3.563606</td>\n",
       "      <td>6.148326</td>\n",
       "      <td>14.181241</td>\n",
       "      <td>36.623926</td>\n",
       "      <td>...</td>\n",
       "      <td>30.475799</td>\n",
       "      <td>49.826002</td>\n",
       "      <td>28.051867</td>\n",
       "      <td>48.398181</td>\n",
       "      <td>22.556595</td>\n",
       "      <td>38.917130</td>\n",
       "      <td>21.910210</td>\n",
       "      <td>77117.16</td>\n",
       "      <td>1.278655</td>\n",
       "      <td>154.9973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.796076</td>\n",
       "      <td>5.131824</td>\n",
       "      <td>8.469040</td>\n",
       "      <td>8.168441</td>\n",
       "      <td>6.226139</td>\n",
       "      <td>7.792487</td>\n",
       "      <td>6.816335</td>\n",
       "      <td>8.795342</td>\n",
       "      <td>45.139920</td>\n",
       "      <td>74.494323</td>\n",
       "      <td>...</td>\n",
       "      <td>69.178850</td>\n",
       "      <td>48.517110</td>\n",
       "      <td>42.439452</td>\n",
       "      <td>54.761021</td>\n",
       "      <td>53.116208</td>\n",
       "      <td>68.537590</td>\n",
       "      <td>59.951999</td>\n",
       "      <td>51216.00</td>\n",
       "      <td>5.212006</td>\n",
       "      <td>125.0071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.562160</td>\n",
       "      <td>4.337768</td>\n",
       "      <td>7.930397</td>\n",
       "      <td>4.372560</td>\n",
       "      <td>9.417980</td>\n",
       "      <td>8.246008</td>\n",
       "      <td>2.728282</td>\n",
       "      <td>6.326085</td>\n",
       "      <td>37.140668</td>\n",
       "      <td>67.901332</td>\n",
       "      <td>...</td>\n",
       "      <td>34.676133</td>\n",
       "      <td>77.660732</td>\n",
       "      <td>25.694900</td>\n",
       "      <td>59.578937</td>\n",
       "      <td>22.497430</td>\n",
       "      <td>52.164942</td>\n",
       "      <td>17.259340</td>\n",
       "      <td>54082.50</td>\n",
       "      <td>4.485508</td>\n",
       "      <td>129.0079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8.703811</td>\n",
       "      <td>2.188438</td>\n",
       "      <td>9.143090</td>\n",
       "      <td>2.635864</td>\n",
       "      <td>9.087634</td>\n",
       "      <td>8.094306</td>\n",
       "      <td>6.826608</td>\n",
       "      <td>3.754130</td>\n",
       "      <td>19.047753</td>\n",
       "      <td>79.579730</td>\n",
       "      <td>...</td>\n",
       "      <td>24.099946</td>\n",
       "      <td>73.558093</td>\n",
       "      <td>62.037714</td>\n",
       "      <td>34.116158</td>\n",
       "      <td>55.256655</td>\n",
       "      <td>30.387077</td>\n",
       "      <td>25.627973</td>\n",
       "      <td>78784.51</td>\n",
       "      <td>0.613785</td>\n",
       "      <td>156.9962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.074672</td>\n",
       "      <td>3.479253</td>\n",
       "      <td>5.061755</td>\n",
       "      <td>2.201202</td>\n",
       "      <td>8.430403</td>\n",
       "      <td>6.003352</td>\n",
       "      <td>2.080965</td>\n",
       "      <td>3.394916</td>\n",
       "      <td>28.093824</td>\n",
       "      <td>40.872014</td>\n",
       "      <td>...</td>\n",
       "      <td>11.141944</td>\n",
       "      <td>50.610675</td>\n",
       "      <td>17.543371</td>\n",
       "      <td>28.620512</td>\n",
       "      <td>12.492762</td>\n",
       "      <td>20.380874</td>\n",
       "      <td>7.064700</td>\n",
       "      <td>59576.45</td>\n",
       "      <td>2.920013</td>\n",
       "      <td>136.0078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>5.247843</td>\n",
       "      <td>7.799644</td>\n",
       "      <td>8.441005</td>\n",
       "      <td>9.658211</td>\n",
       "      <td>6.937770</td>\n",
       "      <td>7.882306</td>\n",
       "      <td>6.353957</td>\n",
       "      <td>9.196173</td>\n",
       "      <td>40.931306</td>\n",
       "      <td>44.297067</td>\n",
       "      <td>...</td>\n",
       "      <td>81.524999</td>\n",
       "      <td>54.685626</td>\n",
       "      <td>44.082290</td>\n",
       "      <td>63.800932</td>\n",
       "      <td>50.083829</td>\n",
       "      <td>72.487046</td>\n",
       "      <td>58.432082</td>\n",
       "      <td>39893.78</td>\n",
       "      <td>5.851534</td>\n",
       "      <td>111.0041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>2.608430</td>\n",
       "      <td>3.346465</td>\n",
       "      <td>5.201403</td>\n",
       "      <td>9.166944</td>\n",
       "      <td>6.192108</td>\n",
       "      <td>3.850187</td>\n",
       "      <td>7.960717</td>\n",
       "      <td>8.586406</td>\n",
       "      <td>8.729021</td>\n",
       "      <td>13.567499</td>\n",
       "      <td>...</td>\n",
       "      <td>47.680974</td>\n",
       "      <td>23.840776</td>\n",
       "      <td>49.293623</td>\n",
       "      <td>53.167952</td>\n",
       "      <td>30.650253</td>\n",
       "      <td>33.059270</td>\n",
       "      <td>68.353950</td>\n",
       "      <td>70830.75</td>\n",
       "      <td>1.673402</td>\n",
       "      <td>149.0006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>7.801407</td>\n",
       "      <td>2.628427</td>\n",
       "      <td>5.543288</td>\n",
       "      <td>9.133362</td>\n",
       "      <td>7.014197</td>\n",
       "      <td>5.730589</td>\n",
       "      <td>4.246228</td>\n",
       "      <td>6.930445</td>\n",
       "      <td>20.505433</td>\n",
       "      <td>43.245446</td>\n",
       "      <td>...</td>\n",
       "      <td>50.628851</td>\n",
       "      <td>40.195478</td>\n",
       "      <td>29.783881</td>\n",
       "      <td>48.611503</td>\n",
       "      <td>24.333390</td>\n",
       "      <td>39.715531</td>\n",
       "      <td>29.428252</td>\n",
       "      <td>72485.31</td>\n",
       "      <td>1.614970</td>\n",
       "      <td>150.9995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>7.094305</td>\n",
       "      <td>2.423077</td>\n",
       "      <td>2.934079</td>\n",
       "      <td>6.637124</td>\n",
       "      <td>4.526698</td>\n",
       "      <td>3.664447</td>\n",
       "      <td>4.901074</td>\n",
       "      <td>5.327160</td>\n",
       "      <td>17.190049</td>\n",
       "      <td>20.815255</td>\n",
       "      <td>...</td>\n",
       "      <td>19.473850</td>\n",
       "      <td>16.587847</td>\n",
       "      <td>22.185683</td>\n",
       "      <td>24.114444</td>\n",
       "      <td>17.959727</td>\n",
       "      <td>19.521095</td>\n",
       "      <td>26.108803</td>\n",
       "      <td>73446.17</td>\n",
       "      <td>0.853824</td>\n",
       "      <td>150.9995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>3.451784</td>\n",
       "      <td>3.164933</td>\n",
       "      <td>3.213005</td>\n",
       "      <td>9.450705</td>\n",
       "      <td>8.759146</td>\n",
       "      <td>8.331735</td>\n",
       "      <td>2.206001</td>\n",
       "      <td>6.427374</td>\n",
       "      <td>10.924665</td>\n",
       "      <td>11.090597</td>\n",
       "      <td>...</td>\n",
       "      <td>30.365158</td>\n",
       "      <td>72.978885</td>\n",
       "      <td>19.322682</td>\n",
       "      <td>56.298306</td>\n",
       "      <td>18.379814</td>\n",
       "      <td>53.551178</td>\n",
       "      <td>14.178791</td>\n",
       "      <td>55933.92</td>\n",
       "      <td>2.408665</td>\n",
       "      <td>131.0084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>197 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Schord     Sspan    Ssweep      Stip    Bchord     Bspan    Bsweep  \\\n",
       "1    5.050903  2.807665  7.250966  4.202998  7.871765  6.329711  3.563606   \n",
       "2    8.796076  5.131824  8.469040  8.168441  6.226139  7.792487  6.816335   \n",
       "5    8.562160  4.337768  7.930397  4.372560  9.417980  8.246008  2.728282   \n",
       "6    8.703811  2.188438  9.143090  2.635864  9.087634  8.094306  6.826608   \n",
       "8    8.074672  3.479253  5.061755  2.201202  8.430403  6.003352  2.080965   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "393  5.247843  7.799644  8.441005  9.658211  6.937770  7.882306  6.353957   \n",
       "395  2.608430  3.346465  5.201403  9.166944  6.192108  3.850187  7.960717   \n",
       "396  7.801407  2.628427  5.543288  9.133362  7.014197  5.730589  4.246228   \n",
       "398  7.094305  2.423077  2.934079  6.637124  4.526698  3.664447  4.901074   \n",
       "399  3.451784  3.164933  3.213005  9.450705  8.759146  8.331735  2.206001   \n",
       "\n",
       "         Btip  Schordspan  Schordsweep  ...  SSweeptip  Bchordspan  \\\n",
       "1    6.148326   14.181241    36.623926  ...  30.475799   49.826002   \n",
       "2    8.795342   45.139920    74.494323  ...  69.178850   48.517110   \n",
       "5    6.326085   37.140668    67.901332  ...  34.676133   77.660732   \n",
       "6    3.754130   19.047753    79.579730  ...  24.099946   73.558093   \n",
       "8    3.394916   28.093824    40.872014  ...  11.141944   50.610675   \n",
       "..        ...         ...          ...  ...        ...         ...   \n",
       "393  9.196173   40.931306    44.297067  ...  81.524999   54.685626   \n",
       "395  8.586406    8.729021    13.567499  ...  47.680974   23.840776   \n",
       "396  6.930445   20.505433    43.245446  ...  50.628851   40.195478   \n",
       "398  5.327160   17.190049    20.815255  ...  19.473850   16.587847   \n",
       "399  6.427374   10.924665    11.090597  ...  30.365158   72.978885   \n",
       "\n",
       "     Bchordsweep  Bchordtip  Bspansweep   BSpantip  BSweeptip  Altitude  \\\n",
       "1      28.051867  48.398181   22.556595  38.917130  21.910210  77117.16   \n",
       "2      42.439452  54.761021   53.116208  68.537590  59.951999  51216.00   \n",
       "5      25.694900  59.578937   22.497430  52.164942  17.259340  54082.50   \n",
       "6      62.037714  34.116158   55.256655  30.387077  25.627973  78784.51   \n",
       "8      17.543371  28.620512   12.492762  20.380874   7.064700  59576.45   \n",
       "..           ...        ...         ...        ...        ...       ...   \n",
       "393    44.082290  63.800932   50.083829  72.487046  58.432082  39893.78   \n",
       "395    49.293623  53.167952   30.650253  33.059270  68.353950  70830.75   \n",
       "396    29.783881  48.611503   24.333390  39.715531  29.428252  72485.31   \n",
       "398    22.185683  24.114444   17.959727  19.521095  26.108803  73446.17   \n",
       "399    19.322682  56.298306   18.379814  53.551178  14.178791  55933.92   \n",
       "\n",
       "     Stability      Time  \n",
       "1     1.278655  154.9973  \n",
       "2     5.212006  125.0071  \n",
       "5     4.485508  129.0079  \n",
       "6     0.613785  156.9962  \n",
       "8     2.920013  136.0078  \n",
       "..         ...       ...  \n",
       "393   5.851534  111.0041  \n",
       "395   1.673402  149.0006  \n",
       "396   1.614970  150.9995  \n",
       "398   0.853824  150.9995  \n",
       "399   2.408665  131.0084  \n",
       "\n",
       "[197 rows x 23 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df = df[df['Time'] > 90]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5835131",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(2)\n",
    "raw_X = filtered_df.iloc[:,:8]\n",
    "raw_Y = filtered_df.iloc[:,-3]\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58283f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ff/y3gj2gld0zqfjtq_lhxz03t00000gn/T/ipykernel_26772/2555709260.py:2: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  train_Y = torch.from_numpy(np.array([[x] for x in raw_Y[:100]])).double()\n",
      "/var/folders/ff/y3gj2gld0zqfjtq_lhxz03t00000gn/T/ipykernel_26772/2555709260.py:5: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  test_Y = torch.from_numpy(np.array([[x] for x in raw_Y[100:]])).double()\n"
     ]
    }
   ],
   "source": [
    "train_X = torch.from_numpy(poly.fit_transform(raw_X[:100])).double()\n",
    "train_Y = torch.from_numpy(np.array([[x] for x in raw_Y[:100]])).double()\n",
    "\n",
    "test_X = torch.from_numpy(poly.fit_transform(raw_X[100:])).double()\n",
    "test_Y = torch.from_numpy(np.array([[x] for x in raw_Y[100:]])).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b808c996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_arrays(data_arrays, batch_size, train = True):\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset, batch_size, shuffle = train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "239414c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = load_arrays((train_X, train_Y), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71ccc4a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1756b6ee0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "229d4b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(45, 90)\n",
    "        self.activation1 = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(90, 60)\n",
    "        self.activation2 = torch.nn.ReLU()\n",
    "        self.linear3 = torch.nn.Linear(60, 30)\n",
    "        self.activation3 = torch.nn.ReLU()\n",
    "        self.linear4 = torch.nn.Linear(30, 10)\n",
    "        self.activation4 = torch.nn.ReLU()\n",
    "        self.linear5 = torch.nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.linear1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.activation3(x)\n",
    "        x = self.linear4(x)\n",
    "        x = self.activation4(x)\n",
    "        x = self.linear5(x)\n",
    "        \n",
    "        return 1000000*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ae9bc4db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1566936.7151],\n",
       "        [2274566.4782],\n",
       "        [2017070.0403],\n",
       "        [2039873.2304],\n",
       "        [1138180.6992],\n",
       "        [2666942.8408],\n",
       "        [1735451.2055],\n",
       "        [2522809.9438],\n",
       "        [2311030.9688],\n",
       "        [1953926.9801],\n",
       "        [2096791.0447],\n",
       "        [1596315.3750],\n",
       "        [1029697.0929],\n",
       "        [1647268.8905],\n",
       "        [1988854.2510],\n",
       "        [1808579.6445],\n",
       "        [2573164.4724],\n",
       "        [2249685.1552],\n",
       "        [1811350.1735],\n",
       "        [1157661.8302],\n",
       "        [ 896379.8506],\n",
       "        [1464640.5811],\n",
       "        [1326238.9880],\n",
       "        [1637364.0241],\n",
       "        [1352638.6554],\n",
       "        [1008419.1327],\n",
       "        [1852160.2152],\n",
       "        [1155330.0052],\n",
       "        [2422272.4514],\n",
       "        [2549873.0814],\n",
       "        [2432473.7201],\n",
       "        [1672326.8915],\n",
       "        [2063084.2377],\n",
       "        [1258311.2397],\n",
       "        [1577840.2720],\n",
       "        [ 755757.5469],\n",
       "        [1234454.0214],\n",
       "        [2894834.7077],\n",
       "        [1397544.8623],\n",
       "        [2612507.2520],\n",
       "        [1148435.4720],\n",
       "        [1342944.1734],\n",
       "        [2294690.9039],\n",
       "        [1920574.5981],\n",
       "        [2541325.6726],\n",
       "        [1483070.3694],\n",
       "        [1136217.7636],\n",
       "        [1946652.2268],\n",
       "        [1743101.9651],\n",
       "        [2274781.9736],\n",
       "        [2415808.3398],\n",
       "        [2494270.4971],\n",
       "        [1460582.0946],\n",
       "        [1772698.5066],\n",
       "        [2362228.1126],\n",
       "        [1649603.6552],\n",
       "        [1079645.4835],\n",
       "        [1033568.1949],\n",
       "        [1099053.7471],\n",
       "        [1715132.9111],\n",
       "        [2129381.2699],\n",
       "        [1676636.2755],\n",
       "        [1789912.9702],\n",
       "        [1538097.7487],\n",
       "        [1312773.7032],\n",
       "        [2037239.8508],\n",
       "        [1093136.1627],\n",
       "        [1971001.3317],\n",
       "        [2084082.2035],\n",
       "        [2133597.3965],\n",
       "        [1265237.1831],\n",
       "        [2062667.0541],\n",
       "        [1796933.6546],\n",
       "        [1527300.6655],\n",
       "        [ 965667.9601],\n",
       "        [2145859.4379],\n",
       "        [2438266.0854],\n",
       "        [1926648.3369],\n",
       "        [1839979.5391],\n",
       "        [1995674.5753],\n",
       "        [1792313.9040],\n",
       "        [2513198.6282],\n",
       "        [2398915.8106],\n",
       "        [1801539.7467],\n",
       "        [2237433.8239],\n",
       "        [1286306.3843],\n",
       "        [1852878.0038],\n",
       "        [2264849.5480],\n",
       "        [1401565.9448],\n",
       "        [1916139.1849],\n",
       "        [1945089.4314],\n",
       "        [1740187.5399],\n",
       "        [1887825.6084],\n",
       "        [2320666.8454],\n",
       "        [1995987.7628],\n",
       "        [1524935.2701],\n",
       "        [1355316.2318],\n",
       "        [1442591.5857],\n",
       "        [1620098.0377],\n",
       "        [1717055.3586]], dtype=torch.float64, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "net.double()\n",
    "net(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "94f0c631",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "425c2e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1566936.7151],\n",
       "        [2274566.4782],\n",
       "        [2017070.0403],\n",
       "        [2039873.2304],\n",
       "        [1138180.6992],\n",
       "        [2666942.8408],\n",
       "        [1735451.2055],\n",
       "        [2522809.9438],\n",
       "        [2311030.9688],\n",
       "        [1953926.9801],\n",
       "        [2096791.0447],\n",
       "        [1596315.3750],\n",
       "        [1029697.0929],\n",
       "        [1647268.8905],\n",
       "        [1988854.2510],\n",
       "        [1808579.6445],\n",
       "        [2573164.4724],\n",
       "        [2249685.1552],\n",
       "        [1811350.1735],\n",
       "        [1157661.8302],\n",
       "        [ 896379.8506],\n",
       "        [1464640.5811],\n",
       "        [1326238.9880],\n",
       "        [1637364.0241],\n",
       "        [1352638.6554],\n",
       "        [1008419.1327],\n",
       "        [1852160.2152],\n",
       "        [1155330.0052],\n",
       "        [2422272.4514],\n",
       "        [2549873.0814],\n",
       "        [2432473.7201],\n",
       "        [1672326.8915],\n",
       "        [2063084.2377],\n",
       "        [1258311.2397],\n",
       "        [1577840.2720],\n",
       "        [ 755757.5469],\n",
       "        [1234454.0214],\n",
       "        [2894834.7077],\n",
       "        [1397544.8623],\n",
       "        [2612507.2520],\n",
       "        [1148435.4720],\n",
       "        [1342944.1734],\n",
       "        [2294690.9039],\n",
       "        [1920574.5981],\n",
       "        [2541325.6726],\n",
       "        [1483070.3694],\n",
       "        [1136217.7636],\n",
       "        [1946652.2268],\n",
       "        [1743101.9651],\n",
       "        [2274781.9736],\n",
       "        [2415808.3398],\n",
       "        [2494270.4971],\n",
       "        [1460582.0946],\n",
       "        [1772698.5066],\n",
       "        [2362228.1126],\n",
       "        [1649603.6552],\n",
       "        [1079645.4835],\n",
       "        [1033568.1949],\n",
       "        [1099053.7471],\n",
       "        [1715132.9111],\n",
       "        [2129381.2699],\n",
       "        [1676636.2755],\n",
       "        [1789912.9702],\n",
       "        [1538097.7487],\n",
       "        [1312773.7032],\n",
       "        [2037239.8508],\n",
       "        [1093136.1627],\n",
       "        [1971001.3317],\n",
       "        [2084082.2035],\n",
       "        [2133597.3965],\n",
       "        [1265237.1831],\n",
       "        [2062667.0541],\n",
       "        [1796933.6546],\n",
       "        [1527300.6655],\n",
       "        [ 965667.9601],\n",
       "        [2145859.4379],\n",
       "        [2438266.0854],\n",
       "        [1926648.3369],\n",
       "        [1839979.5391],\n",
       "        [1995674.5753],\n",
       "        [1792313.9040],\n",
       "        [2513198.6282],\n",
       "        [2398915.8106],\n",
       "        [1801539.7467],\n",
       "        [2237433.8239],\n",
       "        [1286306.3843],\n",
       "        [1852878.0038],\n",
       "        [2264849.5480],\n",
       "        [1401565.9448],\n",
       "        [1916139.1849],\n",
       "        [1945089.4314],\n",
       "        [1740187.5399],\n",
       "        [1887825.6084],\n",
       "        [2320666.8454],\n",
       "        [1995987.7628],\n",
       "        [1524935.2701],\n",
       "        [1355316.2318],\n",
       "        [1442591.5857],\n",
       "        [1620098.0377],\n",
       "        [1717055.3586]], dtype=torch.float64, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = torch.optim.SGD(net.parameters(), lr= 0.0000000000000000001) # 0.000001, 0.0000000000000001 00000000000000001\n",
    "net(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f5092625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 139827123015.242493\n",
      "epoch 2, loss 39423904381.664055\n",
      "epoch 3, loss 225605133.774900\n",
      "epoch 4, loss 50556786487.609123\n",
      "epoch 5, loss 247024843114.840668\n",
      "epoch 6, loss 748533640242.335205\n",
      "epoch 7, loss 2032978792176.225586\n",
      "epoch 8, loss 5934110020903.773438\n",
      "epoch 9, loss 2919961241092301.000000\n",
      "epoch 10, loss 197567834109.109131\n",
      "epoch 11, loss 308855106184.868591\n",
      "epoch 12, loss 444558119146.787354\n",
      "epoch 13, loss 597455360495.139404\n",
      "epoch 14, loss 772918043874.622559\n",
      "epoch 15, loss 970946039950.917480\n",
      "epoch 16, loss 1191539213000.970459\n",
      "epoch 17, loss 1434697429113.050537\n",
      "epoch 18, loss 1700420551406.716553\n",
      "epoch 19, loss 1988708452239.810547\n",
      "epoch 20, loss 2299560984570.466797\n",
      "epoch 21, loss 2632978027863.569824\n",
      "epoch 22, loss 2988959428155.395508\n",
      "epoch 23, loss 3367505055334.310547\n",
      "epoch 24, loss 3768614767669.550781\n",
      "epoch 25, loss 4192288450459.741211\n",
      "epoch 26, loss 4638525949487.864258\n",
      "epoch 27, loss 5107327143831.946289\n",
      "epoch 28, loss 5598691890247.082031\n",
      "epoch 29, loss 6112620057391.617188\n",
      "epoch 30, loss 6649111483097.173828\n",
      "epoch 31, loss 7208166072088.684570\n",
      "epoch 32, loss 7789783684854.352539\n",
      "epoch 33, loss 8393964160679.897461\n",
      "epoch 34, loss 9020707377332.488281\n",
      "epoch 35, loss 9670013199914.255859\n",
      "epoch 36, loss 10341881483679.597656\n",
      "epoch 37, loss 11036312091400.519531\n",
      "epoch 38, loss 11753304928757.712891\n",
      "epoch 39, loss 12492859809744.304688\n",
      "epoch 40, loss 13254976626537.720703\n",
      "epoch 41, loss 14039655235278.900391\n",
      "epoch 42, loss 14846895508210.015625\n",
      "epoch 43, loss 15676697308005.054688\n",
      "epoch 44, loss 16529060486403.472656\n",
      "epoch 45, loss 17403984934287.195312\n",
      "epoch 46, loss 18301470478210.164062\n",
      "epoch 47, loss 19221517027526.281250\n",
      "epoch 48, loss 20164124394880.726562\n",
      "epoch 49, loss 21129292463041.464844\n",
      "epoch 50, loss 22117021126767.152344\n",
      "epoch 51, loss 23127310201361.000000\n",
      "epoch 52, loss 24160159618303.671875\n",
      "epoch 53, loss 25215569168038.148438\n",
      "epoch 54, loss 26293538755722.429688\n",
      "epoch 55, loss 27394068245122.675781\n",
      "epoch 56, loss 28517157440432.003906\n",
      "epoch 57, loss 29662806320329.445312\n",
      "epoch 58, loss 30831014688850.769531\n",
      "epoch 59, loss 32021782428619.890625\n",
      "epoch 60, loss 33235109408640.031250\n",
      "epoch 61, loss 34470995461235.593750\n",
      "epoch 62, loss 35729440438113.703125\n",
      "epoch 63, loss 37010444251452.023438\n",
      "epoch 64, loss 38314006736357.656250\n",
      "epoch 65, loss 39640127767293.304688\n",
      "epoch 66, loss 40988807198083.093750\n",
      "epoch 67, loss 42360044884518.382812\n",
      "epoch 68, loss 43753840710624.890625\n",
      "epoch 69, loss 45170194571629.937500\n",
      "epoch 70, loss 46609106230231.093750\n",
      "epoch 71, loss 48070575686037.851562\n",
      "epoch 72, loss 49554602704116.492188\n",
      "epoch 73, loss 51061187231584.523438\n",
      "epoch 74, loss 52590329040994.031250\n",
      "epoch 75, loss 54142028073818.382812\n",
      "epoch 76, loss 55716284160202.742188\n",
      "epoch 77, loss 57313097139583.062500\n",
      "epoch 78, loss 58932466885290.023438\n",
      "epoch 79, loss 60574393285000.062500\n",
      "epoch 80, loss 62238876271091.359375\n",
      "epoch 81, loss 63925915548436.757812\n",
      "epoch 82, loss 65635511124021.539062\n",
      "epoch 83, loss 67367662752574.906250\n",
      "epoch 84, loss 69122370365178.679688\n",
      "epoch 85, loss 70899633795561.703125\n",
      "epoch 86, loss 72699452944409.625000\n",
      "epoch 87, loss 74521827694836.765625\n",
      "epoch 88, loss 76366757846274.156250\n",
      "epoch 89, loss 78234243348615.218750\n",
      "epoch 90, loss 80124284024335.921875\n",
      "epoch 91, loss 82036879670820.953125\n",
      "epoch 92, loss 83972030153042.578125\n",
      "epoch 93, loss 85929735408955.000000\n",
      "epoch 94, loss 87909995289041.140625\n",
      "epoch 95, loss 89912809664982.843750\n",
      "epoch 96, loss 91938178370392.703125\n",
      "epoch 97, loss 93986101279212.578125\n",
      "epoch 98, loss 96056578379701.718750\n",
      "epoch 99, loss 98149609379954.281250\n",
      "epoch 100, loss 100265194084118.578125\n",
      "epoch 101, loss 102403332515569.203125\n",
      "epoch 102, loss 104564024514900.265625\n",
      "epoch 103, loss 106747269870687.562500\n",
      "epoch 104, loss 108953068488239.421875\n",
      "epoch 105, loss 111181420284271.234375\n",
      "epoch 106, loss 113432324983900.312500\n",
      "epoch 107, loss 115705782582791.562500\n",
      "epoch 108, loss 118001792923192.687500\n",
      "epoch 109, loss 120320355868366.421875\n",
      "epoch 110, loss 122661471231513.953125\n",
      "epoch 111, loss 125025138952883.859375\n",
      "epoch 112, loss 127411358883614.203125\n",
      "epoch 113, loss 129820130801952.625000\n",
      "epoch 114, loss 132251454634570.593750\n",
      "epoch 115, loss 134705330243359.421875\n",
      "epoch 116, loss 137181757504518.500000\n",
      "epoch 117, loss 139680736378864.078125\n",
      "epoch 118, loss 142202266563182.812500\n",
      "epoch 119, loss 144746347931801.687500\n",
      "epoch 120, loss 147312980414084.125000\n",
      "epoch 121, loss 149902163878785.687500\n",
      "epoch 122, loss 152513898231231.375000\n",
      "epoch 123, loss 155148183342779.531250\n",
      "epoch 124, loss 157805018864294.875000\n",
      "epoch 125, loss 160484404817501.968750\n",
      "epoch 126, loss 163186341122995.406250\n",
      "epoch 127, loss 165910827481305.125000\n",
      "epoch 128, loss 168657863855714.125000\n",
      "epoch 129, loss 171427450127216.187500\n",
      "epoch 130, loss 174219586218799.031250\n",
      "epoch 131, loss 177034271906166.468750\n",
      "epoch 132, loss 179871507014913.406250\n",
      "epoch 133, loss 182731291482670.968750\n",
      "epoch 134, loss 185613625165699.406250\n",
      "epoch 135, loss 188518507957103.625000\n",
      "epoch 136, loss 191445939707291.687500\n",
      "epoch 137, loss 194395920222693.468750\n",
      "epoch 138, loss 197368449408769.468750\n",
      "epoch 139, loss 200363527079264.468750\n",
      "epoch 140, loss 203381153183877.531250\n",
      "epoch 141, loss 206421327530014.531250\n",
      "epoch 142, loss 209484049989733.437500\n",
      "epoch 143, loss 212569320479253.875000\n",
      "epoch 144, loss 215677138797637.625000\n",
      "epoch 145, loss 218807504783541.968750\n",
      "epoch 146, loss 221960418449534.406250\n",
      "epoch 147, loss 225135879507550.000000\n",
      "epoch 148, loss 228333887804845.031250\n",
      "epoch 149, loss 231554443316934.750000\n",
      "epoch 150, loss 234797545908653.562500\n",
      "epoch 151, loss 238063195319091.187500\n",
      "epoch 152, loss 241351391638560.406250\n",
      "epoch 153, loss 244662134484569.718750\n",
      "epoch 154, loss 247995423823638.312500\n",
      "epoch 155, loss 251351259607753.093750\n",
      "epoch 156, loss 254729641504180.000000\n",
      "epoch 157, loss 258130569609592.906250\n",
      "epoch 158, loss 261554043658029.531250\n",
      "epoch 159, loss 265000063466343.875000\n",
      "epoch 160, loss 268468629051374.968750\n",
      "epoch 161, loss 271959740134593.687500\n",
      "epoch 162, loss 275473396667286.000000\n",
      "epoch 163, loss 279009598387030.093750\n",
      "epoch 164, loss 282568345297596.750000\n",
      "epoch 165, loss 286149637264031.625000\n",
      "epoch 166, loss 289753474066543.187500\n",
      "epoch 167, loss 293379855612948.500000\n",
      "epoch 168, loss 297028781689251.875000\n",
      "epoch 169, loss 300700252390789.250000\n",
      "epoch 170, loss 304394267301486.062500\n",
      "epoch 171, loss 308110826336172.812500\n",
      "epoch 172, loss 311849929551061.562500\n",
      "epoch 173, loss 315611576826178.250000\n",
      "epoch 174, loss 319395767747919.625000\n",
      "epoch 175, loss 323202502337326.312500\n",
      "epoch 176, loss 327031780550455.000000\n",
      "epoch 177, loss 330883602103137.000000\n",
      "epoch 178, loss 334757966833020.187500\n",
      "epoch 179, loss 338654874741804.375000\n",
      "epoch 180, loss 342574325610594.625000\n",
      "epoch 181, loss 346516319247182.562500\n",
      "epoch 182, loss 350480855653893.812500\n",
      "epoch 183, loss 354467934632498.312500\n",
      "epoch 184, loss 358477555942443.750000\n",
      "epoch 185, loss 362509719585244.500000\n",
      "epoch 186, loss 366564425485154.562500\n",
      "epoch 187, loss 370641673348071.062500\n",
      "epoch 188, loss 374741463238808.000000\n",
      "epoch 189, loss 378863794915272.625000\n",
      "epoch 190, loss 383008668217225.062500\n",
      "epoch 191, loss 387176083005783.187500\n",
      "epoch 192, loss 391366039161123.687500\n",
      "epoch 193, loss 395578536431035.812500\n",
      "epoch 194, loss 399813574824397.187500\n",
      "epoch 195, loss 404071154143156.000000\n",
      "epoch 196, loss 408351274255166.250000\n",
      "epoch 197, loss 412653935097184.625000\n",
      "epoch 198, loss 416979136526457.750000\n",
      "epoch 199, loss 421326878322483.062500\n",
      "epoch 200, loss 425697160354776.750000\n",
      "epoch 201, loss 430089982628449.062500\n",
      "epoch 202, loss 434505344830057.625000\n",
      "epoch 203, loss 438943246853780.625000\n",
      "epoch 204, loss 443403688705497.625000\n",
      "epoch 205, loss 447886670319385.437500\n",
      "epoch 206, loss 452392191277102.562500\n",
      "epoch 207, loss 456920251505847.125000\n",
      "epoch 208, loss 461470850872720.937500\n",
      "epoch 209, loss 466043989474548.000000\n",
      "epoch 210, loss 470639666894854.937500\n",
      "epoch 211, loss 475257883077601.437500\n",
      "epoch 212, loss 479898637991750.375000\n",
      "epoch 213, loss 484561931367272.500000\n",
      "epoch 214, loss 489247763223363.625000\n",
      "epoch 215, loss 493956133026957.625000\n",
      "epoch 216, loss 498687040986876.937500\n",
      "epoch 217, loss 503440486945946.250000\n",
      "epoch 218, loss 508216470884694.187500\n",
      "epoch 219, loss 513014992498016.062500\n",
      "epoch 220, loss 517836051763800.562500\n",
      "epoch 221, loss 522679648452667.250000\n",
      "epoch 222, loss 527545782426718.625000\n",
      "epoch 223, loss 532434453576232.750000\n",
      "epoch 224, loss 537345661652007.500000\n",
      "epoch 225, loss 542279406678748.312500\n",
      "epoch 226, loss 547235688499343.250000\n",
      "epoch 227, loss 552214506851640.625000\n",
      "epoch 228, loss 557215861605070.875000\n",
      "epoch 229, loss 562239752913785.437500\n",
      "epoch 230, loss 567286180335649.250000\n",
      "epoch 231, loss 572355143798420.750000\n",
      "epoch 232, loss 577446643239155.125000\n",
      "epoch 233, loss 582560678426869.500000\n",
      "epoch 234, loss 587697249411109.250000\n",
      "epoch 235, loss 592856355824546.875000\n",
      "epoch 236, loss 598037997787811.500000\n",
      "epoch 237, loss 603242174871540.875000\n",
      "epoch 238, loss 608468887161204.250000\n",
      "epoch 239, loss 613718134438202.125000\n",
      "epoch 240, loss 618989916587046.750000\n",
      "epoch 241, loss 624284233432492.375000\n",
      "epoch 242, loss 629601084978903.250000\n",
      "epoch 243, loss 634940470997046.750000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 244, loss 640302391297596.125000\n",
      "epoch 245, loss 645686845828272.500000\n",
      "epoch 246, loss 651093834368717.875000\n",
      "epoch 247, loss 656523356889615.000000\n",
      "epoch 248, loss 661975413134833.375000\n",
      "epoch 249, loss 667450003133601.250000\n",
      "epoch 250, loss 672947126411766.500000\n",
      "epoch 251, loss 678466783161873.750000\n",
      "epoch 252, loss 684008973070844.750000\n",
      "epoch 253, loss 689573696090736.375000\n",
      "epoch 254, loss 695160952428126.375000\n",
      "epoch 255, loss 700770741261315.250000\n",
      "epoch 256, loss 706403062991775.250000\n",
      "epoch 257, loss 712057917066321.875000\n",
      "epoch 258, loss 717735303869677.375000\n",
      "epoch 259, loss 723435222660196.000000\n",
      "epoch 260, loss 729157673734271.250000\n",
      "epoch 261, loss 734902656952544.750000\n",
      "epoch 262, loss 740670171945780.125000\n",
      "epoch 263, loss 746460218792434.750000\n",
      "epoch 264, loss 752272797069180.500000\n",
      "epoch 265, loss 758107907217342.500000\n",
      "epoch 266, loss 763965548405923.500000\n",
      "epoch 267, loss 769845720817317.875000\n",
      "epoch 268, loss 775748424262975.250000\n",
      "epoch 269, loss 781673658719563.500000\n",
      "epoch 270, loss 787621423965912.500000\n",
      "epoch 271, loss 793591719905651.875000\n",
      "epoch 272, loss 799584546242558.125000\n",
      "epoch 273, loss 805599903122699.375000\n",
      "epoch 274, loss 811637790356411.875000\n",
      "epoch 275, loss 817698207602132.500000\n",
      "epoch 276, loss 823781154919756.750000\n",
      "epoch 277, loss 829886631969290.875000\n",
      "epoch 278, loss 836014638847533.125000\n",
      "epoch 279, loss 842165175137096.375000\n",
      "epoch 280, loss 848338240924379.250000\n",
      "epoch 281, loss 854533836238719.625000\n",
      "epoch 282, loss 860751960686441.625000\n",
      "epoch 283, loss 866992614052245.750000\n",
      "epoch 284, loss 873255796387962.500000\n",
      "epoch 285, loss 879541507664889.625000\n",
      "epoch 286, loss 885849747410082.250000\n",
      "epoch 287, loss 892180515699990.375000\n",
      "epoch 288, loss 898533812483900.625000\n",
      "epoch 289, loss 904909637563670.375000\n",
      "epoch 290, loss 911307990491308.500000\n",
      "epoch 291, loss 917728871493133.125000\n",
      "epoch 292, loss 924172280475484.125000\n",
      "epoch 293, loss 930638217053869.750000\n",
      "epoch 294, loss 937126680988467.250000\n",
      "epoch 295, loss 943637672502263.500000\n",
      "epoch 296, loss 950171191277926.500000\n",
      "epoch 297, loss 956727237240003.250000\n",
      "epoch 298, loss 963305810252549.625000\n",
      "epoch 299, loss 969906910014760.625000\n",
      "epoch 300, loss 976530536564695.000000\n",
      "epoch 301, loss 983176689753167.375000\n",
      "epoch 302, loss 989845369493423.250000\n",
      "epoch 303, loss 996536575453130.250000\n",
      "epoch 304, loss 1003250307659506.875000\n",
      "epoch 305, loss 1009986566065137.750000\n",
      "epoch 306, loss 1016745350317225.000000\n",
      "epoch 307, loss 1023526660421465.000000\n",
      "epoch 308, loss 1030330496188913.125000\n",
      "epoch 309, loss 1037156857359104.375000\n",
      "epoch 310, loss 1044005744133778.375000\n",
      "epoch 311, loss 1050877156165292.750000\n",
      "epoch 312, loss 1057771093519525.250000\n",
      "epoch 313, loss 1064687555424182.500000\n",
      "epoch 314, loss 1071626542509565.000000\n",
      "epoch 315, loss 1078588054099421.500000\n",
      "epoch 316, loss 1085572090403773.125000\n",
      "epoch 317, loss 1092578651036464.125000\n",
      "epoch 318, loss 1099607736076135.500000\n",
      "epoch 319, loss 1106659345254617.500000\n",
      "epoch 320, loss 1113733478574761.625000\n",
      "epoch 321, loss 1120830135653053.125000\n",
      "epoch 322, loss 1127949316585799.250000\n",
      "epoch 323, loss 1135091021250510.000000\n",
      "epoch 324, loss 1142255249449813.000000\n",
      "epoch 325, loss 1149442000939015.000000\n",
      "epoch 326, loss 1156651275728113.250000\n",
      "epoch 327, loss 1163883073448615.250000\n",
      "epoch 328, loss 1171137394112490.000000\n",
      "epoch 329, loss 1178414237653788.500000\n",
      "epoch 330, loss 1185713604213426.500000\n",
      "epoch 331, loss 1193035493059002.250000\n",
      "epoch 332, loss 1200379904335455.250000\n",
      "epoch 333, loss 1207746837877291.000000\n",
      "epoch 334, loss 1215136293455736.000000\n",
      "epoch 335, loss 1222548271184846.750000\n",
      "epoch 336, loss 1229982770936091.000000\n",
      "epoch 337, loss 1237439792535481.500000\n",
      "epoch 338, loss 1244919335284692.250000\n",
      "epoch 339, loss 1252421399712740.250000\n",
      "epoch 340, loss 1259945985465909.000000\n",
      "epoch 341, loss 1267493092394630.000000\n",
      "epoch 342, loss 1275062720290428.000000\n",
      "epoch 343, loss 1282654869358805.750000\n",
      "epoch 344, loss 1290269539378379.250000\n",
      "epoch 345, loss 1297906729944945.500000\n",
      "epoch 346, loss 1305566440950699.500000\n",
      "epoch 347, loss 1313248672319250.250000\n",
      "epoch 348, loss 1320953423918582.000000\n",
      "epoch 349, loss 1328680695870603.250000\n",
      "epoch 350, loss 1336430487564186.250000\n",
      "epoch 351, loss 1344202799177664.250000\n",
      "epoch 352, loss 1351997630631012.250000\n",
      "epoch 353, loss 1359814981679980.000000\n",
      "epoch 354, loss 1367654852102704.250000\n",
      "epoch 355, loss 1375517241770766.250000\n",
      "epoch 356, loss 1383402150776004.500000\n",
      "epoch 357, loss 1391309578965218.500000\n",
      "epoch 358, loss 1399239526037845.750000\n",
      "epoch 359, loss 1407191991612282.500000\n",
      "epoch 360, loss 1415166976048833.500000\n",
      "epoch 361, loss 1423164478867940.250000\n",
      "epoch 362, loss 1431184500207129.000000\n",
      "epoch 363, loss 1439227039697917.500000\n",
      "epoch 364, loss 1447292097328463.250000\n",
      "epoch 365, loss 1455379672841985.500000\n",
      "epoch 366, loss 1463489766040225.250000\n",
      "epoch 367, loss 1471622377073159.000000\n",
      "epoch 368, loss 1479777505619835.750000\n",
      "epoch 369, loss 1487955151626825.250000\n",
      "epoch 370, loss 1496155314930720.750000\n",
      "epoch 371, loss 1504377995501828.250000\n",
      "epoch 372, loss 1512623192638274.000000\n",
      "epoch 373, loss 1520890907012757.500000\n",
      "epoch 374, loss 1529181138212276.500000\n",
      "epoch 375, loss 1537493885931909.750000\n",
      "epoch 376, loss 1545829149980434.000000\n",
      "epoch 377, loss 1554186930378105.000000\n",
      "epoch 378, loss 1562567226938491.500000\n",
      "epoch 379, loss 1570970039739153.250000\n",
      "epoch 380, loss 1579395368461736.250000\n",
      "epoch 381, loss 1587843212990043.500000\n",
      "epoch 382, loss 1596313573087995.000000\n",
      "epoch 383, loss 1604806448760390.750000\n",
      "epoch 384, loss 1613321839984472.250000\n",
      "epoch 385, loss 1621859746327031.000000\n",
      "epoch 386, loss 1630420167803694.000000\n",
      "epoch 387, loss 1639003104182307.250000\n",
      "epoch 388, loss 1647608555473474.000000\n",
      "epoch 389, loss 1656236521430211.250000\n",
      "epoch 390, loss 1664887002082024.250000\n",
      "epoch 391, loss 1673559996892100.250000\n",
      "epoch 392, loss 1682255506427698.250000\n",
      "epoch 393, loss 1690973529916149.000000\n",
      "epoch 394, loss 1699714067468648.750000\n",
      "epoch 395, loss 1708477118933698.250000\n",
      "epoch 396, loss 1717262684082002.500000\n",
      "epoch 397, loss 1726070762926370.000000\n",
      "epoch 398, loss 1734901355262974.750000\n",
      "epoch 399, loss 1743754460853504.750000\n",
      "epoch 400, loss 1752630079877555.250000\n",
      "epoch 401, loss 1761528211956013.000000\n",
      "epoch 402, loss 1770448856915655.750000\n",
      "epoch 403, loss 1779392014584690.500000\n",
      "epoch 404, loss 1788357685288614.500000\n",
      "epoch 405, loss 1797345868585081.500000\n",
      "epoch 406, loss 1806356564247925.750000\n",
      "epoch 407, loss 1815389772102559.250000\n",
      "epoch 408, loss 1824445492092421.000000\n",
      "epoch 409, loss 1833523724445826.250000\n",
      "epoch 410, loss 1842624468391882.000000\n",
      "epoch 411, loss 1851747723805462.000000\n",
      "epoch 412, loss 1860893491153758.750000\n",
      "epoch 413, loss 1870061769808844.250000\n",
      "epoch 414, loss 1879252559982026.000000\n",
      "epoch 415, loss 1888465861259427.000000\n",
      "epoch 416, loss 1897701673737582.500000\n",
      "epoch 417, loss 1906959996821016.750000\n",
      "epoch 418, loss 1916240830976764.500000\n",
      "epoch 419, loss 1925544175738093.500000\n",
      "epoch 420, loss 1934870030947316.500000\n",
      "epoch 421, loss 1944218396956059.000000\n",
      "epoch 422, loss 1953589273059578.000000\n",
      "epoch 423, loss 1962982659218812.750000\n",
      "epoch 424, loss 1972398555025280.750000\n",
      "epoch 425, loss 1981836960905211.250000\n",
      "epoch 426, loss 1991297876325656.750000\n",
      "epoch 427, loss 2000781301764914.250000\n",
      "epoch 428, loss 2010287236483603.000000\n",
      "epoch 429, loss 2019815680416151.750000\n",
      "epoch 430, loss 2029366633390960.000000\n",
      "epoch 431, loss 2038940095686226.250000\n",
      "epoch 432, loss 2048536066984840.750000\n",
      "epoch 433, loss 2058154546985193.250000\n",
      "epoch 434, loss 2067795535481197.500000\n",
      "epoch 435, loss 2077459032374478.750000\n",
      "epoch 436, loss 2087145037855818.500000\n",
      "epoch 437, loss 2096853551530339.250000\n",
      "epoch 438, loss 2106584573482772.500000\n",
      "epoch 439, loss 2116338103198611.500000\n",
      "epoch 440, loss 2126114140586706.500000\n",
      "epoch 441, loss 2135912685852774.750000\n",
      "epoch 442, loss 2145733738613929.250000\n",
      "epoch 443, loss 2155577298570117.000000\n",
      "epoch 444, loss 2165443366163367.250000\n",
      "epoch 445, loss 2175331940862452.250000\n",
      "epoch 446, loss 2185243022668241.500000\n",
      "epoch 447, loss 2195176611572749.750000\n",
      "epoch 448, loss 2205132706585632.750000\n",
      "epoch 449, loss 2215111308327587.000000\n",
      "epoch 450, loss 2225112416562825.500000\n",
      "epoch 451, loss 2235136031233144.250000\n",
      "epoch 452, loss 2245182152383750.750000\n",
      "epoch 453, loss 2255250779243885.500000\n",
      "epoch 454, loss 2265341912100517.500000\n",
      "epoch 455, loss 2275455550939086.000000\n",
      "epoch 456, loss 2285591695312555.500000\n",
      "epoch 457, loss 2295750345137947.500000\n",
      "epoch 458, loss 2305931500301120.500000\n",
      "epoch 459, loss 2316135160905455.000000\n",
      "epoch 460, loss 2326361326373388.000000\n",
      "epoch 461, loss 2336609997274788.000000\n",
      "epoch 462, loss 2346881172795238.500000\n",
      "epoch 463, loss 2357174853270956.500000\n",
      "epoch 464, loss 2367491037960812.000000\n",
      "epoch 465, loss 2377829727539506.500000\n",
      "epoch 466, loss 2388190920800658.500000\n",
      "epoch 467, loss 2398574618338416.500000\n",
      "epoch 468, loss 2408980820262129.500000\n",
      "epoch 469, loss 2419409525943122.000000\n",
      "epoch 470, loss 2429860735788611.000000\n",
      "epoch 471, loss 2440334449026772.000000\n",
      "epoch 472, loss 2450830665682413.000000\n",
      "epoch 473, loss 2461349385440940.000000\n",
      "epoch 474, loss 2471890608930730.000000\n",
      "epoch 475, loss 2482454335404827.500000\n",
      "epoch 476, loss 2493040564641811.500000\n",
      "epoch 477, loss 2503649296944929.000000\n",
      "epoch 478, loss 2514280531878187.500000\n",
      "epoch 479, loss 2524934269412674.500000\n",
      "epoch 480, loss 2535610509230356.500000\n",
      "epoch 481, loss 2546309251144342.000000\n",
      "epoch 482, loss 2557030495709310.000000\n",
      "epoch 483, loss 2567774241800246.000000\n",
      "epoch 484, loss 2578540490346458.000000\n",
      "epoch 485, loss 2589329240174274.000000\n",
      "epoch 486, loss 2600140491816649.500000\n",
      "epoch 487, loss 2610974244690247.500000\n",
      "epoch 488, loss 2621830499206914.500000\n",
      "epoch 489, loss 2632709254686565.000000\n",
      "epoch 490, loss 2643610511517248.500000\n",
      "epoch 491, loss 2654534269090687.500000\n",
      "epoch 492, loss 2665480527270110.000000\n",
      "epoch 493, loss 2676449286612352.500000\n",
      "epoch 494, loss 2687440546364067.500000\n",
      "epoch 495, loss 2698454306264961.500000\n",
      "epoch 496, loss 2709490566486355.500000\n",
      "epoch 497, loss 2720549327280045.500000\n",
      "epoch 498, loss 2731630587567648.500000\n",
      "epoch 499, loss 2742734347795367.500000\n",
      "epoch 500, loss 2753860607878927.500000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 501, loss 2765009367614385.500000\n",
      "epoch 502, loss 2776180626488079.500000\n",
      "epoch 503, loss 2787374384874160.500000\n",
      "epoch 504, loss 2798590642073861.000000\n",
      "epoch 505, loss 2809829398620149.500000\n",
      "epoch 506, loss 2821090653962644.000000\n",
      "epoch 507, loss 2832374408526866.000000\n",
      "epoch 508, loss 2843680660932860.000000\n",
      "epoch 509, loss 2855009412286959.500000\n",
      "epoch 510, loss 2866360661896798.500000\n",
      "epoch 511, loss 2877734409895244.500000\n",
      "epoch 512, loss 2889130655748676.000000\n",
      "epoch 513, loss 2900549399773234.500000\n",
      "epoch 514, loss 2911990641795797.000000\n",
      "epoch 515, loss 2923454381370756.000000\n",
      "epoch 516, loss 2934940618764234.000000\n",
      "epoch 517, loss 2946449353420466.000000\n",
      "epoch 518, loss 2957980585261154.500000\n",
      "epoch 519, loss 2969534314320726.500000\n",
      "epoch 520, loss 2981110540358222.000000\n",
      "epoch 521, loss 2992709263199506.000000\n",
      "epoch 522, loss 3004330482628981.000000\n",
      "epoch 523, loss 3015974198766639.500000\n",
      "epoch 524, loss 3027640411767589.000000\n",
      "epoch 525, loss 3039329121006698.000000\n",
      "epoch 526, loss 3051040325932496.000000\n",
      "epoch 527, loss 3062774027065358.500000\n",
      "epoch 528, loss 3074530224471578.000000\n",
      "epoch 529, loss 3086308917522267.000000\n",
      "epoch 530, loss 3098110106222560.000000\n",
      "epoch 531, loss 3109933790291536.000000\n",
      "epoch 532, loss 3121779970195428.000000\n",
      "epoch 533, loss 3133648645268914.000000\n",
      "epoch 534, loss 3145539815607515.500000\n",
      "epoch 535, loss 3157453480542281.000000\n",
      "epoch 536, loss 3169389640141165.500000\n",
      "epoch 537, loss 3181348294699936.500000\n",
      "epoch 538, loss 3193329444175763.000000\n",
      "epoch 539, loss 3205333087900786.000000\n",
      "epoch 540, loss 3217359225768777.500000\n",
      "epoch 541, loss 3229407857918532.000000\n",
      "epoch 542, loss 3241478984194407.000000\n",
      "epoch 543, loss 3253572604153372.000000\n",
      "epoch 544, loss 3265688718218481.500000\n",
      "epoch 545, loss 3277827325888873.500000\n",
      "epoch 546, loss 3289988426739566.500000\n",
      "epoch 547, loss 3302172021175895.000000\n",
      "epoch 548, loss 3314378108763445.000000\n",
      "epoch 549, loss 3326606689466375.500000\n",
      "epoch 550, loss 3338857763392386.000000\n",
      "epoch 551, loss 3351131330216825.500000\n",
      "epoch 552, loss 3363427389120164.500000\n",
      "epoch 553, loss 3375745940929843.000000\n",
      "epoch 554, loss 3388086985501085.500000\n",
      "epoch 555, loss 3400450522174918.000000\n",
      "epoch 556, loss 3412836551130086.500000\n",
      "epoch 557, loss 3425245071661803.500000\n",
      "epoch 558, loss 3437676084367782.000000\n",
      "epoch 559, loss 3450129588986000.500000\n",
      "epoch 560, loss 3462605585146494.000000\n",
      "epoch 561, loss 3475104072599934.000000\n",
      "epoch 562, loss 3487625051132919.000000\n",
      "epoch 563, loss 3500168520859565.000000\n",
      "epoch 564, loss 3512734482064432.500000\n",
      "epoch 565, loss 3525322934255447.000000\n",
      "epoch 566, loss 3537933876943661.000000\n",
      "epoch 567, loss 3550567310671646.500000\n",
      "epoch 568, loss 3563223234714486.000000\n",
      "epoch 569, loss 3575901649147108.000000\n",
      "epoch 570, loss 3588602553622470.500000\n",
      "epoch 571, loss 3601325948601216.000000\n",
      "epoch 572, loss 3614071833363008.000000\n",
      "epoch 573, loss 3626840208262986.000000\n",
      "epoch 574, loss 3639631072666235.000000\n",
      "epoch 575, loss 3652444426732262.000000\n",
      "epoch 576, loss 3665280270304731.500000\n",
      "epoch 577, loss 3678138603045852.000000\n",
      "epoch 578, loss 3691019424937897.500000\n",
      "epoch 579, loss 3703922736248641.500000\n",
      "epoch 580, loss 3716848536382028.000000\n",
      "epoch 581, loss 3729796825124103.500000\n",
      "epoch 582, loss 3742767602508957.500000\n",
      "epoch 583, loss 3755760868605540.000000\n",
      "epoch 584, loss 3768776623188441.000000\n",
      "epoch 585, loss 3781814865648006.500000\n",
      "epoch 586, loss 3794875596330641.500000\n",
      "epoch 587, loss 3807958815099078.500000\n",
      "epoch 588, loss 3821064521642264.500000\n",
      "epoch 589, loss 3834192715755371.000000\n",
      "epoch 590, loss 3847343397148470.000000\n",
      "epoch 591, loss 3860516566476240.500000\n",
      "epoch 592, loss 3873712222879575.500000\n",
      "epoch 593, loss 3886930366499126.500000\n",
      "epoch 594, loss 3900170997101513.500000\n",
      "epoch 595, loss 3913434114875220.000000\n",
      "epoch 596, loss 3926719719065019.500000\n",
      "epoch 597, loss 3940027809949396.500000\n",
      "epoch 598, loss 3953358387416113.500000\n",
      "epoch 599, loss 3966711450822371.000000\n",
      "epoch 600, loss 3980087000703234.000000\n",
      "epoch 601, loss 3993485036585446.000000\n",
      "epoch 602, loss 4006905558404606.000000\n",
      "epoch 603, loss 4020348566195519.500000\n",
      "epoch 604, loss 4033814059595938.500000\n",
      "epoch 605, loss 4047302038550050.000000\n",
      "epoch 606, loss 4060812502840528.000000\n",
      "epoch 607, loss 4074345453052606.500000\n",
      "epoch 608, loss 4087900888022594.000000\n",
      "epoch 609, loss 4101478807862408.500000\n",
      "epoch 610, loss 4115079212903362.000000\n",
      "epoch 611, loss 4128702102219054.000000\n",
      "epoch 612, loss 4142347476528413.500000\n",
      "epoch 613, loss 4156015334821300.000000\n",
      "epoch 614, loss 4169705677601325.000000\n",
      "epoch 615, loss 4183418504171492.500000\n",
      "epoch 616, loss 4197153815125920.000000\n",
      "epoch 617, loss 4210911610116992.000000\n",
      "epoch 618, loss 4224691888777721.000000\n",
      "epoch 619, loss 4238494651349656.500000\n",
      "epoch 620, loss 4252319897532629.000000\n",
      "epoch 621, loss 4266167626770540.000000\n",
      "epoch 622, loss 4280037839427425.500000\n",
      "epoch 623, loss 4293930534900046.000000\n",
      "epoch 624, loss 4307845713513954.500000\n",
      "epoch 625, loss 4321783375352717.000000\n",
      "epoch 626, loss 4335743519526727.000000\n",
      "epoch 627, loss 4349726146315460.000000\n",
      "epoch 628, loss 4363731255616839.500000\n",
      "epoch 629, loss 4377758847039903.500000\n",
      "epoch 630, loss 4391808920622337.500000\n",
      "epoch 631, loss 4405881476342224.500000\n",
      "epoch 632, loss 4419976513937614.500000\n",
      "epoch 633, loss 4434094033207596.000000\n",
      "epoch 634, loss 4448234034195003.000000\n",
      "epoch 635, loss 4462396516746713.500000\n",
      "epoch 636, loss 4476581480481886.500000\n",
      "epoch 637, loss 4490788924986858.000000\n",
      "epoch 638, loss 4505018850942486.000000\n",
      "epoch 639, loss 4519271257960480.000000\n",
      "epoch 640, loss 4533546145888593.000000\n",
      "epoch 641, loss 4547843514265770.000000\n",
      "epoch 642, loss 4562163363099438.000000\n",
      "epoch 643, loss 4576505692587608.000000\n",
      "epoch 644, loss 4590870502395282.000000\n",
      "epoch 645, loss 4605257791952616.000000\n",
      "epoch 646, loss 4619667561656019.000000\n",
      "epoch 647, loss 4634099811271779.000000\n",
      "epoch 648, loss 4648554541187319.000000\n",
      "epoch 649, loss 4663031750496776.000000\n",
      "epoch 650, loss 4677531438756899.000000\n",
      "epoch 651, loss 4692053606566359.000000\n",
      "epoch 652, loss 4706598253528600.000000\n",
      "epoch 653, loss 4721165379770161.000000\n",
      "epoch 654, loss 4735754984649613.000000\n",
      "epoch 655, loss 4750367068430853.000000\n",
      "epoch 656, loss 4765001631141181.000000\n",
      "epoch 657, loss 4779658671906104.000000\n",
      "epoch 658, loss 4794338191424450.000000\n",
      "epoch 659, loss 4809040189024358.000000\n",
      "epoch 660, loss 4823764664700908.000000\n",
      "epoch 661, loss 4838511618413692.000000\n",
      "epoch 662, loss 4853281050488362.000000\n",
      "epoch 663, loss 4868072959708970.000000\n",
      "epoch 664, loss 4882887346669995.000000\n",
      "epoch 665, loss 4897724211346045.000000\n",
      "epoch 666, loss 4912583552939332.000000\n",
      "epoch 667, loss 4927465371469405.000000\n",
      "epoch 668, loss 4942369667367387.000000\n",
      "epoch 669, loss 4957296439654822.000000\n",
      "epoch 670, loss 4972245689072982.000000\n",
      "epoch 671, loss 4987217415342520.000000\n",
      "epoch 672, loss 5002211618126093.000000\n",
      "epoch 673, loss 5017228297383875.000000\n",
      "epoch 674, loss 5032267452855962.000000\n",
      "epoch 675, loss 5047329084346244.000000\n",
      "epoch 676, loss 5062413191735868.000000\n",
      "epoch 677, loss 5077519774582897.000000\n",
      "epoch 678, loss 5092648833432925.000000\n",
      "epoch 679, loss 5107800367659859.000000\n",
      "epoch 680, loss 5122974377461169.000000\n",
      "epoch 681, loss 5138170862341196.000000\n",
      "epoch 682, loss 5153389822545787.000000\n",
      "epoch 683, loss 5168631257629137.000000\n",
      "epoch 684, loss 5183895167913206.000000\n",
      "epoch 685, loss 5199181552720211.000000\n",
      "epoch 686, loss 5214490412146992.000000\n",
      "epoch 687, loss 5229821746077732.000000\n",
      "epoch 688, loss 5245175554362676.000000\n",
      "epoch 689, loss 5260551836803110.000000\n",
      "epoch 690, loss 5275950593547896.000000\n",
      "epoch 691, loss 5291371824127232.000000\n",
      "epoch 692, loss 5306815528423953.000000\n",
      "epoch 693, loss 5322281706464294.000000\n",
      "epoch 694, loss 5337770358005994.000000\n",
      "epoch 695, loss 5353281482887669.000000\n",
      "epoch 696, loss 5368815081311137.000000\n",
      "epoch 697, loss 5384371152667128.000000\n",
      "epoch 698, loss 5399949696610767.000000\n",
      "epoch 699, loss 5415550713375008.000000\n",
      "epoch 700, loss 5431174203419372.000000\n",
      "epoch 701, loss 5446820165701309.000000\n",
      "epoch 702, loss 5462488600610556.000000\n",
      "epoch 703, loss 5478179507688961.000000\n",
      "epoch 704, loss 5493892886876188.000000\n",
      "epoch 705, loss 5509628738609619.000000\n",
      "epoch 706, loss 5525387061257079.000000\n",
      "epoch 707, loss 5541167856447126.000000\n",
      "epoch 708, loss 5556971123143528.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 709, loss 5572796861256486.000000\n",
      "epoch 710, loss 5588645070869524.000000\n",
      "epoch 711, loss 5604515751749888.000000\n",
      "epoch 712, loss 5620408904013013.000000\n",
      "epoch 713, loss 5636324526587001.000000\n",
      "epoch 714, loss 5652262620386443.000000\n",
      "epoch 715, loss 5668223184917395.000000\n",
      "epoch 716, loss 5684206220104699.000000\n",
      "epoch 717, loss 5700211725060312.000000\n",
      "epoch 718, loss 5716239700378194.000000\n",
      "epoch 719, loss 5732290146079698.000000\n",
      "epoch 720, loss 5748363061948422.000000\n",
      "epoch 721, loss 5764458447554498.000000\n",
      "epoch 722, loss 5780576302794563.000000\n",
      "epoch 723, loss 5796716627714294.000000\n",
      "epoch 724, loss 5812879422010808.000000\n",
      "epoch 725, loss 5829064685670221.000000\n",
      "epoch 726, loss 5845272418774284.000000\n",
      "epoch 727, loss 5861502620821757.000000\n",
      "epoch 728, loss 5877755292301292.000000\n",
      "epoch 729, loss 5894030431954665.000000\n",
      "epoch 730, loss 5910328040367551.000000\n",
      "epoch 731, loss 5926648117322527.000000\n",
      "epoch 732, loss 5942990662428740.000000\n",
      "epoch 733, loss 5959355676026130.000000\n",
      "epoch 734, loss 5975743158063718.000000\n",
      "epoch 735, loss 5992153108223817.000000\n",
      "epoch 736, loss 6008585525922556.000000\n",
      "epoch 737, loss 6025040411222604.000000\n",
      "epoch 738, loss 6041517764092211.000000\n",
      "epoch 739, loss 6058017584200952.000000\n",
      "epoch 740, loss 6074539871542040.000000\n",
      "epoch 741, loss 6091084626248764.000000\n",
      "epoch 742, loss 6107651848516116.000000\n",
      "epoch 743, loss 6124241537537834.000000\n",
      "epoch 744, loss 6140853692914083.000000\n",
      "epoch 745, loss 6157488315368172.000000\n",
      "epoch 746, loss 6174145404296719.000000\n",
      "epoch 747, loss 6190824960065837.000000\n",
      "epoch 748, loss 6207526981585469.000000\n",
      "epoch 749, loss 6224251468930615.000000\n",
      "epoch 750, loss 6240998422822243.000000\n",
      "epoch 751, loss 6257767842645175.000000\n",
      "epoch 752, loss 6274559727701714.000000\n",
      "epoch 753, loss 6291374078786314.000000\n",
      "epoch 754, loss 6308210894639407.000000\n",
      "epoch 755, loss 6325070175505332.000000\n",
      "epoch 756, loss 6341951922513272.000000\n",
      "epoch 757, loss 6358856133900918.000000\n",
      "epoch 758, loss 6375782810148618.000000\n",
      "epoch 759, loss 6392731950902467.000000\n",
      "epoch 760, loss 6409703556610994.000000\n",
      "epoch 761, loss 6426697627036728.000000\n",
      "epoch 762, loss 6443714161641523.000000\n",
      "epoch 763, loss 6460753160445202.000000\n",
      "epoch 764, loss 6477814623375068.000000\n",
      "epoch 765, loss 6494898550434008.000000\n",
      "epoch 766, loss 6512004940496013.000000\n",
      "epoch 767, loss 6529133794850747.000000\n",
      "epoch 768, loss 6546285112885523.000000\n",
      "epoch 769, loss 6563458894470851.000000\n",
      "epoch 770, loss 6580655139044708.000000\n",
      "epoch 771, loss 6597873846759762.000000\n",
      "epoch 772, loss 6615115017304015.000000\n",
      "epoch 773, loss 6632378650928133.000000\n",
      "epoch 774, loss 6649664746664852.000000\n",
      "epoch 775, loss 6666973305285079.000000\n",
      "epoch 776, loss 6684304327134744.000000\n",
      "epoch 777, loss 6701657810988111.000000\n",
      "epoch 778, loss 6719033756441700.000000\n",
      "epoch 779, loss 6736432164501373.000000\n",
      "epoch 780, loss 6753853034502269.000000\n",
      "epoch 781, loss 6771296365818897.000000\n",
      "epoch 782, loss 6788762159375621.000000\n",
      "epoch 783, loss 6806250414364293.000000\n",
      "epoch 784, loss 6823761130717172.000000\n",
      "epoch 785, loss 6841294308063535.000000\n",
      "epoch 786, loss 6858849946496192.000000\n",
      "epoch 787, loss 6876428045473597.000000\n",
      "epoch 788, loss 6894028606168870.000000\n",
      "epoch 789, loss 6911651627540449.000000\n",
      "epoch 790, loss 6929297109281050.000000\n",
      "epoch 791, loss 6946965051924306.000000\n",
      "epoch 792, loss 6964655454212573.000000\n",
      "epoch 793, loss 6982368316634765.000000\n",
      "epoch 794, loss 7000103639357887.000000\n",
      "epoch 795, loss 7017861422113626.000000\n",
      "epoch 796, loss 7035641663839956.000000\n",
      "epoch 797, loss 7053444365264762.000000\n",
      "epoch 798, loss 7071269526366546.000000\n",
      "epoch 799, loss 7089117147505853.000000\n",
      "epoch 800, loss 7106987227369974.000000\n",
      "epoch 801, loss 7124879766127361.000000\n",
      "epoch 802, loss 7142794764158565.000000\n",
      "epoch 803, loss 7160732221082463.000000\n",
      "epoch 804, loss 7178692136787187.000000\n",
      "epoch 805, loss 7196674510850445.000000\n",
      "epoch 806, loss 7214679343410436.000000\n",
      "epoch 807, loss 7232706633967380.000000\n",
      "epoch 808, loss 7250756383166461.000000\n",
      "epoch 809, loss 7268828590264759.000000\n",
      "epoch 810, loss 7286923254777613.000000\n",
      "epoch 811, loss 7305040377499606.000000\n",
      "epoch 812, loss 7323179957291412.000000\n",
      "epoch 813, loss 7341341994849974.000000\n",
      "epoch 814, loss 7359526489483264.000000\n",
      "epoch 815, loss 7377733441542995.000000\n",
      "epoch 816, loss 7395962851231917.000000\n",
      "epoch 817, loss 7414214716945897.000000\n",
      "epoch 818, loss 7432489039988454.000000\n",
      "epoch 819, loss 7450785819807910.000000\n",
      "epoch 820, loss 7469105055299834.000000\n",
      "epoch 821, loss 7487446747718916.000000\n",
      "epoch 822, loss 7505810897109540.000000\n",
      "epoch 823, loss 7524197501840494.000000\n",
      "epoch 824, loss 7542606562571702.000000\n",
      "epoch 825, loss 7561038079042257.000000\n",
      "epoch 826, loss 7579492051463328.000000\n",
      "epoch 827, loss 7597968479037819.000000\n",
      "epoch 828, loss 7616467362556530.000000\n",
      "epoch 829, loss 7634988701337400.000000\n",
      "epoch 830, loss 7653532495438496.000000\n",
      "epoch 831, loss 7672098744506209.000000\n",
      "epoch 832, loss 7690687448591209.000000\n",
      "epoch 833, loss 7709298607380142.000000\n",
      "epoch 834, loss 7727932220896233.000000\n",
      "epoch 835, loss 7746588288447539.000000\n",
      "epoch 836, loss 7765266810973487.000000\n",
      "epoch 837, loss 7783967787231310.000000\n",
      "epoch 838, loss 7802691218250514.000000\n",
      "epoch 839, loss 7821437102791739.000000\n",
      "epoch 840, loss 7840205441079461.000000\n",
      "epoch 841, loss 7858996232777595.000000\n",
      "epoch 842, loss 7877809478045619.000000\n",
      "epoch 843, loss 7896645176628065.000000\n",
      "epoch 844, loss 7915503328662790.000000\n",
      "epoch 845, loss 7934383933915040.000000\n",
      "epoch 846, loss 7953286992182711.000000\n",
      "epoch 847, loss 7972212503105262.000000\n",
      "epoch 848, loss 7991160467057079.000000\n",
      "epoch 849, loss 8010130884207672.000000\n",
      "epoch 850, loss 8029123753406811.000000\n",
      "epoch 851, loss 8048139075127672.000000\n",
      "epoch 852, loss 8067176848986962.000000\n",
      "epoch 853, loss 8086237074459026.000000\n",
      "epoch 854, loss 8105319752380887.000000\n",
      "epoch 855, loss 8124424881855145.000000\n",
      "epoch 856, loss 8143552463296415.000000\n",
      "epoch 857, loss 8162702495710436.000000\n",
      "epoch 858, loss 8181874979650797.000000\n",
      "epoch 859, loss 8201069915243653.000000\n",
      "epoch 860, loss 8220287301546376.000000\n",
      "epoch 861, loss 8239527138996431.000000\n",
      "epoch 862, loss 8258789427110395.000000\n",
      "epoch 863, loss 8278074166292995.000000\n",
      "epoch 864, loss 8297381355760006.000000\n",
      "epoch 865, loss 8316710995818248.000000\n",
      "epoch 866, loss 8336063086912161.000000\n",
      "epoch 867, loss 8355437626997860.000000\n",
      "epoch 868, loss 8374834618114924.000000\n",
      "epoch 869, loss 8394254058731387.000000\n",
      "epoch 870, loss 8413695948730714.000000\n",
      "epoch 871, loss 8433160288721155.000000\n",
      "epoch 872, loss 8452647078326505.000000\n",
      "epoch 873, loss 8472156317202061.000000\n",
      "epoch 874, loss 8491688006041454.000000\n",
      "epoch 875, loss 8511242143104125.000000\n",
      "epoch 876, loss 8530818729835827.000000\n",
      "epoch 877, loss 8550417765153132.000000\n",
      "epoch 878, loss 8570039248886472.000000\n",
      "epoch 879, loss 8589683181050984.000000\n",
      "epoch 880, loss 8609349561174614.000000\n",
      "epoch 881, loss 8629038391179579.000000\n",
      "epoch 882, loss 8648749668852264.000000\n",
      "epoch 883, loss 8668483394042981.000000\n",
      "epoch 884, loss 8688239567139363.000000\n",
      "epoch 885, loss 8708018187960137.000000\n",
      "epoch 886, loss 8727819256400310.000000\n",
      "epoch 887, loss 8747642772161106.000000\n",
      "epoch 888, loss 8767488735366482.000000\n",
      "epoch 889, loss 8787357145775392.000000\n",
      "epoch 890, loss 8807248003005011.000000\n",
      "epoch 891, loss 8827161307633987.000000\n",
      "epoch 892, loss 8847097059445985.000000\n",
      "epoch 893, loss 8867055257624362.000000\n",
      "epoch 894, loss 8887035902310006.000000\n",
      "epoch 895, loss 8907038993174382.000000\n",
      "epoch 896, loss 8927064531173597.000000\n",
      "epoch 897, loss 8947112514468764.000000\n",
      "epoch 898, loss 8967182943745961.000000\n",
      "epoch 899, loss 8987275819349702.000000\n",
      "epoch 900, loss 9007391140505318.000000\n",
      "epoch 901, loss 9027528907177150.000000\n",
      "epoch 902, loss 9047689119375982.000000\n",
      "epoch 903, loss 9067871777094402.000000\n",
      "epoch 904, loss 9088076879987308.000000\n",
      "epoch 905, loss 9108304427382034.000000\n",
      "epoch 906, loss 9128554419447956.000000\n",
      "epoch 907, loss 9148826856987476.000000\n",
      "epoch 908, loss 9169121739349784.000000\n",
      "epoch 909, loss 9189439066101632.000000\n",
      "epoch 910, loss 9209778836550800.000000\n",
      "epoch 911, loss 9230141052087960.000000\n",
      "epoch 912, loss 9250525712205292.000000\n",
      "epoch 913, loss 9270932815198408.000000\n",
      "epoch 914, loss 9291362361947052.000000\n",
      "epoch 915, loss 9311814352861698.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 916, loss 9332288787313744.000000\n",
      "epoch 917, loss 9352785665270516.000000\n",
      "epoch 918, loss 9373304987140392.000000\n",
      "epoch 919, loss 9393846751724188.000000\n",
      "epoch 920, loss 9414410959414810.000000\n",
      "epoch 921, loss 9434997609804840.000000\n",
      "epoch 922, loss 9455606702582182.000000\n",
      "epoch 923, loss 9476238238815928.000000\n",
      "epoch 924, loss 9496892217141604.000000\n",
      "epoch 925, loss 9517568637666218.000000\n",
      "epoch 926, loss 9538267501423130.000000\n",
      "epoch 927, loss 9558988806790950.000000\n",
      "epoch 928, loss 9579732553499660.000000\n",
      "epoch 929, loss 9600498742128664.000000\n",
      "epoch 930, loss 9621287372744420.000000\n",
      "epoch 931, loss 9642098445216202.000000\n",
      "epoch 932, loss 9662931958547928.000000\n",
      "epoch 933, loss 9683787913627848.000000\n",
      "epoch 934, loss 9704666309469696.000000\n",
      "epoch 935, loss 9725567146199662.000000\n",
      "epoch 936, loss 9746490423853830.000000\n",
      "epoch 937, loss 9767436142197080.000000\n",
      "epoch 938, loss 9788404301313668.000000\n",
      "epoch 939, loss 9809394900745990.000000\n",
      "epoch 940, loss 9830407940812106.000000\n",
      "epoch 941, loss 9851443421016536.000000\n",
      "epoch 942, loss 9872501341565474.000000\n",
      "epoch 943, loss 9893581701475328.000000\n",
      "epoch 944, loss 9914684501707088.000000\n",
      "epoch 945, loss 9935809741563904.000000\n",
      "epoch 946, loss 9956957420037402.000000\n",
      "epoch 947, loss 9978127538631856.000000\n",
      "epoch 948, loss 9999320096621592.000000\n",
      "epoch 949, loss 10020535093340834.000000\n",
      "epoch 950, loss 10041772529443614.000000\n",
      "epoch 951, loss 10063032404340198.000000\n",
      "epoch 952, loss 10084314718185556.000000\n",
      "epoch 953, loss 10105619469966832.000000\n",
      "epoch 954, loss 10126946660564304.000000\n",
      "epoch 955, loss 10148296289492562.000000\n",
      "epoch 956, loss 10169668356929136.000000\n",
      "epoch 957, loss 10191062862094056.000000\n",
      "epoch 958, loss 10212479805004644.000000\n",
      "epoch 959, loss 10233919185950894.000000\n",
      "epoch 960, loss 10255381004524376.000000\n",
      "epoch 961, loss 10276865260250716.000000\n",
      "epoch 962, loss 10298371953488592.000000\n",
      "epoch 963, loss 10319901084381660.000000\n",
      "epoch 964, loss 10341452651976728.000000\n",
      "epoch 965, loss 10363026657057994.000000\n",
      "epoch 966, loss 10384623098723966.000000\n",
      "epoch 967, loss 10406241976827358.000000\n",
      "epoch 968, loss 10427883291802996.000000\n",
      "epoch 969, loss 10449547043811050.000000\n",
      "epoch 970, loss 10471233231600168.000000\n",
      "epoch 971, loss 10492941855625762.000000\n",
      "epoch 972, loss 10514672914954720.000000\n",
      "epoch 973, loss 10536426410538980.000000\n",
      "epoch 974, loss 10558202342412706.000000\n",
      "epoch 975, loss 10580000709767736.000000\n",
      "epoch 976, loss 10601821512534534.000000\n",
      "epoch 977, loss 10623664751152722.000000\n",
      "epoch 978, loss 10645530424505842.000000\n",
      "epoch 979, loss 10667418532878598.000000\n",
      "epoch 980, loss 10689329076200020.000000\n",
      "epoch 981, loss 10711262053776902.000000\n",
      "epoch 982, loss 10733217466922522.000000\n",
      "epoch 983, loss 10755195314231184.000000\n",
      "epoch 984, loss 10777195596352428.000000\n",
      "epoch 985, loss 10799218312465948.000000\n",
      "epoch 986, loss 10821263462598270.000000\n",
      "epoch 987, loss 10843331046720424.000000\n",
      "epoch 988, loss 10865421064856980.000000\n",
      "epoch 989, loss 10887533516157730.000000\n",
      "epoch 990, loss 10909668401309816.000000\n",
      "epoch 991, loss 10931825719684932.000000\n",
      "epoch 992, loss 10954005471391238.000000\n",
      "epoch 993, loss 10976207656923614.000000\n",
      "epoch 994, loss 10998432275287116.000000\n",
      "epoch 995, loss 11020679326831304.000000\n",
      "epoch 996, loss 11042948811336788.000000\n",
      "epoch 997, loss 11065240728461676.000000\n",
      "epoch 998, loss 11087555077926354.000000\n",
      "epoch 999, loss 11109891859965832.000000\n",
      "epoch 1000, loss 11132251074280760.000000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    " \n",
    "for epoch in range(num_epochs):\n",
    " \n",
    "    for X, y in data_iter:\n",
    "\n",
    "        l = loss(net(X) , 1000000*y)\n",
    "\n",
    "        trainer.zero_grad() #sets gradients to zero\n",
    "\n",
    "        l.backward() # back propagation\n",
    "\n",
    "        trainer.step() # parameter update\n",
    "\n",
    "    l = loss(net(train_X), train_Y)\n",
    "\n",
    "    print(f'epoch {epoch + 1}, loss {l:f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1e1de357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10550948.333813772"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(l.detach().numpy()/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f7f8ce88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10550798.238641398"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt((loss(net(test_X), test_Y)/100).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1c43c008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN: weird landscape. Need to start with less stepsize (e.g. 0.000001) and then grow to 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e9f88037",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(45, 90)\n",
    "        self.activation1 = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(90, 60)\n",
    "        self.activation2 = torch.nn.ReLU()\n",
    "        self.linear3 = torch.nn.Linear(60, 30)\n",
    "        self.activation3 = torch.nn.ReLU()\n",
    "        self.linear4 = torch.nn.Linear(30, 10)\n",
    "        self.activation4 = torch.nn.ReLU()\n",
    "        self.linear5 = torch.nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.linear1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.activation3(x)\n",
    "        x = self.linear4(x)\n",
    "        x = self.activation4(x)\n",
    "        x = self.linear5(x)\n",
    "        \n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5f5a47cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (linear1): Linear(in_features=45, out_features=90, bias=True)\n",
       "  (activation1): ReLU()\n",
       "  (linear2): Linear(in_features=90, out_features=60, bias=True)\n",
       "  (activation2): ReLU()\n",
       "  (linear3): Linear(in_features=60, out_features=30, bias=True)\n",
       "  (activation3): ReLU()\n",
       "  (linear4): Linear(in_features=30, out_features=10, bias=True)\n",
       "  (activation4): ReLU()\n",
       "  (linear5): Linear(in_features=10, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classif = Classifier()\n",
    "classif.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e6c6ed5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(2)\n",
    "raw_X = df.iloc[:,:8]\n",
    "raw_Y = df.iloc[:,-1] > 90\n",
    "batch_size = 10\n",
    "\n",
    "train_X = torch.from_numpy(poly.fit_transform(raw_X[:300])).double()\n",
    "train_Y = torch.from_numpy(np.array([[x] for x in raw_Y[:300]])).double()\n",
    "\n",
    "test_X = torch.from_numpy(poly.fit_transform(raw_X[300:])).double()\n",
    "test_Y = torch.from_numpy(np.array([[x] for x in raw_Y[300:]])).double()\n",
    "data_iter = load_arrays((train_X, train_Y), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f849bcf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2999],\n",
       "        [0.3276],\n",
       "        [0.2004],\n",
       "        [0.2773],\n",
       "        [0.3127],\n",
       "        [0.2808],\n",
       "        [0.2493],\n",
       "        [0.3111],\n",
       "        [0.3474],\n",
       "        [0.2254],\n",
       "        [0.2568],\n",
       "        [0.2449],\n",
       "        [0.2603],\n",
       "        [0.3927],\n",
       "        [0.1509],\n",
       "        [0.2647],\n",
       "        [0.2353],\n",
       "        [0.2110],\n",
       "        [0.2310],\n",
       "        [0.2591],\n",
       "        [0.2586],\n",
       "        [0.2787],\n",
       "        [0.2922],\n",
       "        [0.3526],\n",
       "        [0.3100],\n",
       "        [0.3289],\n",
       "        [0.3177],\n",
       "        [0.3005],\n",
       "        [0.3636],\n",
       "        [0.3315],\n",
       "        [0.2637],\n",
       "        [0.2690],\n",
       "        [0.2666],\n",
       "        [0.2947],\n",
       "        [0.1991],\n",
       "        [0.2061],\n",
       "        [0.2883],\n",
       "        [0.3119],\n",
       "        [0.3452],\n",
       "        [0.2944],\n",
       "        [0.3350],\n",
       "        [0.2982],\n",
       "        [0.3994],\n",
       "        [0.2790],\n",
       "        [0.2979],\n",
       "        [0.2751],\n",
       "        [0.3520],\n",
       "        [0.2998],\n",
       "        [0.2388],\n",
       "        [0.3068],\n",
       "        [0.2633],\n",
       "        [0.2935],\n",
       "        [0.3250],\n",
       "        [0.2560],\n",
       "        [0.3561],\n",
       "        [0.2792],\n",
       "        [0.3135],\n",
       "        [0.2796],\n",
       "        [0.2100],\n",
       "        [0.2375],\n",
       "        [0.2704],\n",
       "        [0.2314],\n",
       "        [0.2662],\n",
       "        [0.1673],\n",
       "        [0.3857],\n",
       "        [0.2389],\n",
       "        [0.2495],\n",
       "        [0.2578],\n",
       "        [0.2487],\n",
       "        [0.3123],\n",
       "        [0.2132],\n",
       "        [0.3467],\n",
       "        [0.2218],\n",
       "        [0.2360],\n",
       "        [0.2333],\n",
       "        [0.2652],\n",
       "        [0.2890],\n",
       "        [0.1883],\n",
       "        [0.2414],\n",
       "        [0.3356],\n",
       "        [0.2519],\n",
       "        [0.3211],\n",
       "        [0.2856],\n",
       "        [0.2775],\n",
       "        [0.2882],\n",
       "        [0.3362],\n",
       "        [0.2125],\n",
       "        [0.3444],\n",
       "        [0.3296],\n",
       "        [0.2355],\n",
       "        [0.1724],\n",
       "        [0.2971],\n",
       "        [0.2352],\n",
       "        [0.2913],\n",
       "        [0.3289],\n",
       "        [0.2471],\n",
       "        [0.2705],\n",
       "        [0.2871],\n",
       "        [0.2300],\n",
       "        [0.2311],\n",
       "        [0.1897],\n",
       "        [0.2583],\n",
       "        [0.2712],\n",
       "        [0.2813],\n",
       "        [0.2224],\n",
       "        [0.2907],\n",
       "        [0.2805],\n",
       "        [0.2992],\n",
       "        [0.3191],\n",
       "        [0.3089],\n",
       "        [0.2864],\n",
       "        [0.2797],\n",
       "        [0.2878],\n",
       "        [0.1991],\n",
       "        [0.2811],\n",
       "        [0.2710],\n",
       "        [0.3731],\n",
       "        [0.3331],\n",
       "        [0.3143],\n",
       "        [0.3010],\n",
       "        [0.3352],\n",
       "        [0.3192],\n",
       "        [0.2690],\n",
       "        [0.2382],\n",
       "        [0.2439],\n",
       "        [0.2236],\n",
       "        [0.2532],\n",
       "        [0.3364],\n",
       "        [0.3363],\n",
       "        [0.3123],\n",
       "        [0.1773],\n",
       "        [0.2803],\n",
       "        [0.2633],\n",
       "        [0.2857],\n",
       "        [0.3563],\n",
       "        [0.3567],\n",
       "        [0.2808],\n",
       "        [0.2587],\n",
       "        [0.1810],\n",
       "        [0.3002],\n",
       "        [0.2924],\n",
       "        [0.1921],\n",
       "        [0.2665],\n",
       "        [0.2324],\n",
       "        [0.3410],\n",
       "        [0.2006],\n",
       "        [0.3116],\n",
       "        [0.3263],\n",
       "        [0.2779],\n",
       "        [0.2123],\n",
       "        [0.2596],\n",
       "        [0.3013],\n",
       "        [0.2894],\n",
       "        [0.2901],\n",
       "        [0.2911],\n",
       "        [0.3084],\n",
       "        [0.2241],\n",
       "        [0.2610],\n",
       "        [0.2721],\n",
       "        [0.2079],\n",
       "        [0.2147],\n",
       "        [0.2428],\n",
       "        [0.2637],\n",
       "        [0.2640],\n",
       "        [0.2279],\n",
       "        [0.2223],\n",
       "        [0.3289],\n",
       "        [0.2211],\n",
       "        [0.1959],\n",
       "        [0.3379],\n",
       "        [0.2377],\n",
       "        [0.2734],\n",
       "        [0.2353],\n",
       "        [0.2237],\n",
       "        [0.1967],\n",
       "        [0.2391],\n",
       "        [0.2603],\n",
       "        [0.1980],\n",
       "        [0.3303],\n",
       "        [0.2939],\n",
       "        [0.2850],\n",
       "        [0.3406],\n",
       "        [0.2413],\n",
       "        [0.2621],\n",
       "        [0.3706],\n",
       "        [0.2824],\n",
       "        [0.2201],\n",
       "        [0.2430],\n",
       "        [0.2856],\n",
       "        [0.3359],\n",
       "        [0.3112],\n",
       "        [0.2340],\n",
       "        [0.2787],\n",
       "        [0.2379],\n",
       "        [0.2396],\n",
       "        [0.2089],\n",
       "        [0.2807],\n",
       "        [0.2657],\n",
       "        [0.2786],\n",
       "        [0.2319],\n",
       "        [0.3448],\n",
       "        [0.2661],\n",
       "        [0.3159],\n",
       "        [0.2991],\n",
       "        [0.3471],\n",
       "        [0.3233],\n",
       "        [0.3256],\n",
       "        [0.2980],\n",
       "        [0.3497],\n",
       "        [0.3139],\n",
       "        [0.2278],\n",
       "        [0.2857],\n",
       "        [0.1902],\n",
       "        [0.3014],\n",
       "        [0.2147],\n",
       "        [0.2797],\n",
       "        [0.2997],\n",
       "        [0.2958],\n",
       "        [0.3222],\n",
       "        [0.2948],\n",
       "        [0.3446],\n",
       "        [0.2649],\n",
       "        [0.3302],\n",
       "        [0.3333],\n",
       "        [0.2791],\n",
       "        [0.2261],\n",
       "        [0.2116],\n",
       "        [0.2852],\n",
       "        [0.1962],\n",
       "        [0.2145],\n",
       "        [0.2876],\n",
       "        [0.2902],\n",
       "        [0.2995],\n",
       "        [0.2830],\n",
       "        [0.3335],\n",
       "        [0.2227],\n",
       "        [0.3159],\n",
       "        [0.3659],\n",
       "        [0.2825],\n",
       "        [0.2823],\n",
       "        [0.2325],\n",
       "        [0.1896],\n",
       "        [0.2736],\n",
       "        [0.3144],\n",
       "        [0.3315],\n",
       "        [0.1937],\n",
       "        [0.2936],\n",
       "        [0.2644],\n",
       "        [0.2776],\n",
       "        [0.2772],\n",
       "        [0.3088],\n",
       "        [0.3410],\n",
       "        [0.2447],\n",
       "        [0.2652],\n",
       "        [0.2331],\n",
       "        [0.3349],\n",
       "        [0.3364],\n",
       "        [0.3039],\n",
       "        [0.3296],\n",
       "        [0.2714],\n",
       "        [0.3457],\n",
       "        [0.2304],\n",
       "        [0.2695],\n",
       "        [0.3149],\n",
       "        [0.3115],\n",
       "        [0.2251],\n",
       "        [0.2186],\n",
       "        [0.2625],\n",
       "        [0.3345],\n",
       "        [0.2833],\n",
       "        [0.2914],\n",
       "        [0.3416],\n",
       "        [0.2511],\n",
       "        [0.3068],\n",
       "        [0.3003],\n",
       "        [0.2705],\n",
       "        [0.2206],\n",
       "        [0.3013],\n",
       "        [0.3368],\n",
       "        [0.2408],\n",
       "        [0.2580],\n",
       "        [0.2750],\n",
       "        [0.2846],\n",
       "        [0.3066],\n",
       "        [0.2462],\n",
       "        [0.2272],\n",
       "        [0.2986],\n",
       "        [0.3570],\n",
       "        [0.2511],\n",
       "        [0.1808],\n",
       "        [0.2269],\n",
       "        [0.2177],\n",
       "        [0.2740],\n",
       "        [0.2882],\n",
       "        [0.2903],\n",
       "        [0.3005],\n",
       "        [0.2388],\n",
       "        [0.2819],\n",
       "        [0.2401],\n",
       "        [0.3254]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classif(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b0cca9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d3098223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.000382\n",
      "epoch 51, loss 0.000266\n",
      "epoch 101, loss 0.000203\n",
      "epoch 151, loss 0.000162\n",
      "epoch 201, loss 0.000134\n"
     ]
    }
   ],
   "source": [
    "trainer = torch.optim.SGD(classif.parameters(), lr=0.01)\n",
    "num_epochs = 250\n",
    "for epoch in range(num_epochs):\n",
    " \n",
    "    for X, y in data_iter:\n",
    "        X.requires_grad = True\n",
    "        l = loss(classif(X) ,y)\n",
    "\n",
    "        trainer.zero_grad() #sets gradients to zero\n",
    "\n",
    "        l.backward() # back propagation\n",
    "\n",
    "        trainer.step() # parameter update\n",
    "        \n",
    "    l = loss(classif(train_X), train_Y)\n",
    "    if epoch % 50 == 0:\n",
    "        print(f'epoch {epoch + 1}, loss {l:f}')\n",
    "        #print(X.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "00e21fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5019, dtype=torch.float64, grad_fn=<BinaryCrossEntropyBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(classif(test_X), test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "334df7e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8300])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((classif(test_X) >= 0.5) == test_Y)/len(test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367a970f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordinateAscentNN(stab_classifier, alt_predictor, initial = [0,0,0,0,0,0,0,0], inc = 0.0000001, max_iter = 10000, confidence = 0.95):\n",
    "    '''\n",
    "    Keep increasing in direction until unstable. \n",
    "    '''\n",
    "    x = initial\n",
    "    old = qr.predict(poly.fit_transform([x]))\n",
    "    for _ in range(max_iter):\n",
    "        #for i in range(8):\n",
    "        y = np.array(x)\n",
    "        # y[i] += np.sign(alt_predictor.coef_[i])*inc\n",
    "        # print(y, partials(x)*inc)\n",
    "        y += partials(x) * inc\n",
    "        if np.all(y > 2.5) and qr.predict(poly.fit_transform([y])) > old:\n",
    "            nxt = stab_classifier.predict_proba(poly.fit_transform([y]))[0,1]\n",
    "            if nxt > confidence or np.random.rand() > 0.5:\n",
    "                if nxt <= confidence:\n",
    "                    print('Non greedy step')\n",
    "                x = y[:]\n",
    "                #print(qr.predict(poly.fit_transform([y])) - old)\n",
    "                old = qr.predict(poly.fit_transform([y]))\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        print('No more feasible directions at iteration', _, '.', qr.predict(poly.fit_transform([x])) - qr.predict(poly.fit_transform([initial])))\n",
    "        break\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45082784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(x):\n",
    "    L = [torch.Tensor([1]), x]\n",
    "    for i in range(len(x)):\n",
    "        for j in range(i, len(x)):\n",
    "            L.append(torch.Tensor([x[i]*x[j]]))\n",
    "    return torch.cat(L).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48c2492",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca1b45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor([3,10,6,7,9,2.1,5,2])\n",
    "# x.requires_grad = True\n",
    "z = transform(x)\n",
    "z.requires_grad = True\n",
    "out = net(z)\n",
    "#torch.autograd.grad(outputs = out, inputs = x, retain_graph = True)\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4b7bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cbebd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d325a43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c943a0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377c15ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
