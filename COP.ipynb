{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "04784754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from matplotlib import pyplot as plt\n",
    "from tools import *\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torch import nn\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from scipy.optimize import LinearConstraint, NonlinearConstraint, Bounds, minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ee728a",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4aed84d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getObs(n):\n",
    "    obs = {}\n",
    "    for i in range(n):\n",
    "        try:\n",
    "            df = pd.read_csv(f'rocket-results/{i}.csv')\n",
    "        except:\n",
    "            print('Missing', i)\n",
    "        obs[i] = df\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4fffecbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeConfig():\n",
    "    configs = {}\n",
    "    with open('sample_list.pkl', 'rb') as f:\n",
    "        d = pickle.load(f)\n",
    "    configs.update(d)\n",
    "    with open('sample_list_100.pkl', 'rb') as f:\n",
    "        d = pickle.load(f)\n",
    "    configs.update(d)\n",
    "    with open('sample_list_200.pkl', 'rb') as f:\n",
    "        d = pickle.load(f)\n",
    "    configs.update(d)\n",
    "    with open('sample_list_300.pkl', 'rb') as f:\n",
    "        d = pickle.load(f)\n",
    "    configs.update(d)\n",
    "    return configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2e8120dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = initializeConfig()\n",
    "obs = getObs(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ca05fc41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         3.734223\n",
       "1         7.875106\n",
       "2         2.509849\n",
       "3         7.372139\n",
       "4         8.753891\n",
       "5         7.977354\n",
       "6         7.539283\n",
       "7         8.558976\n",
       "8     32187.640000\n",
       "9         5.071216\n",
       "10       99.001530\n",
       "Name: 284, dtype: float64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {}\n",
    "for i in range(len(config)):\n",
    "    data[i] = list(config[i]['S'].values()) + list(config[i]['B'].values())\n",
    "    data[i].append(obs[i].max()['Altitude (ft)'])\n",
    "    data[i].append(obs[i].mean()['Stability Margin (cal)'])\n",
    "    data[i].append(obs[i].max()['Time (sec)'])\n",
    "df = pd.DataFrame.from_dict(data, orient='index')\n",
    "df.iloc[284]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fa42c786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Schord</th>\n",
       "      <th>Sspan</th>\n",
       "      <th>Ssweep</th>\n",
       "      <th>Stip</th>\n",
       "      <th>Bchord</th>\n",
       "      <th>Bspan</th>\n",
       "      <th>Bsweep</th>\n",
       "      <th>Btip</th>\n",
       "      <th>Schordspan</th>\n",
       "      <th>Schordsweep</th>\n",
       "      <th>...</th>\n",
       "      <th>SSweeptip</th>\n",
       "      <th>Bchordspan</th>\n",
       "      <th>Bchordsweep</th>\n",
       "      <th>Bchordtip</th>\n",
       "      <th>Bspansweep</th>\n",
       "      <th>BSpantip</th>\n",
       "      <th>BSweeptip</th>\n",
       "      <th>Altitude</th>\n",
       "      <th>Stability</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.465233</td>\n",
       "      <td>9.191411</td>\n",
       "      <td>6.313146</td>\n",
       "      <td>6.181568</td>\n",
       "      <td>4.253219</td>\n",
       "      <td>3.362766</td>\n",
       "      <td>2.004250</td>\n",
       "      <td>3.767645</td>\n",
       "      <td>86.998844</td>\n",
       "      <td>59.755399</td>\n",
       "      <td>...</td>\n",
       "      <td>39.025144</td>\n",
       "      <td>14.302581</td>\n",
       "      <td>8.524515</td>\n",
       "      <td>16.024619</td>\n",
       "      <td>6.739826</td>\n",
       "      <td>12.669711</td>\n",
       "      <td>7.551304</td>\n",
       "      <td>0.004903</td>\n",
       "      <td>-5.121752</td>\n",
       "      <td>0.0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.050903</td>\n",
       "      <td>2.807665</td>\n",
       "      <td>7.250966</td>\n",
       "      <td>4.202998</td>\n",
       "      <td>7.871765</td>\n",
       "      <td>6.329711</td>\n",
       "      <td>3.563606</td>\n",
       "      <td>6.148326</td>\n",
       "      <td>14.181241</td>\n",
       "      <td>36.623926</td>\n",
       "      <td>...</td>\n",
       "      <td>30.475799</td>\n",
       "      <td>49.826002</td>\n",
       "      <td>28.051867</td>\n",
       "      <td>48.398181</td>\n",
       "      <td>22.556595</td>\n",
       "      <td>38.917130</td>\n",
       "      <td>21.910210</td>\n",
       "      <td>77117.160000</td>\n",
       "      <td>1.278655</td>\n",
       "      <td>154.9973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.796076</td>\n",
       "      <td>5.131824</td>\n",
       "      <td>8.469040</td>\n",
       "      <td>8.168441</td>\n",
       "      <td>6.226139</td>\n",
       "      <td>7.792487</td>\n",
       "      <td>6.816335</td>\n",
       "      <td>8.795342</td>\n",
       "      <td>45.139920</td>\n",
       "      <td>74.494323</td>\n",
       "      <td>...</td>\n",
       "      <td>69.178850</td>\n",
       "      <td>48.517110</td>\n",
       "      <td>42.439452</td>\n",
       "      <td>54.761021</td>\n",
       "      <td>53.116208</td>\n",
       "      <td>68.537590</td>\n",
       "      <td>59.951999</td>\n",
       "      <td>51216.000000</td>\n",
       "      <td>5.212006</td>\n",
       "      <td>125.0071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.590219</td>\n",
       "      <td>9.233072</td>\n",
       "      <td>6.352605</td>\n",
       "      <td>8.429602</td>\n",
       "      <td>5.672564</td>\n",
       "      <td>5.705322</td>\n",
       "      <td>5.717624</td>\n",
       "      <td>3.420081</td>\n",
       "      <td>51.614896</td>\n",
       "      <td>35.512459</td>\n",
       "      <td>...</td>\n",
       "      <td>53.549937</td>\n",
       "      <td>32.363803</td>\n",
       "      <td>32.433584</td>\n",
       "      <td>19.400628</td>\n",
       "      <td>32.620884</td>\n",
       "      <td>19.512664</td>\n",
       "      <td>19.554736</td>\n",
       "      <td>0.004691</td>\n",
       "      <td>-2.489141</td>\n",
       "      <td>0.0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.916229</td>\n",
       "      <td>4.216994</td>\n",
       "      <td>4.711545</td>\n",
       "      <td>6.462613</td>\n",
       "      <td>4.887217</td>\n",
       "      <td>2.638681</td>\n",
       "      <td>9.605809</td>\n",
       "      <td>5.234709</td>\n",
       "      <td>12.297724</td>\n",
       "      <td>13.739946</td>\n",
       "      <td>...</td>\n",
       "      <td>30.448892</td>\n",
       "      <td>12.895807</td>\n",
       "      <td>46.945675</td>\n",
       "      <td>25.583158</td>\n",
       "      <td>25.346667</td>\n",
       "      <td>13.812727</td>\n",
       "      <td>50.283615</td>\n",
       "      <td>0.004818</td>\n",
       "      <td>-4.231226</td>\n",
       "      <td>0.0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>2.608430</td>\n",
       "      <td>3.346465</td>\n",
       "      <td>5.201403</td>\n",
       "      <td>9.166944</td>\n",
       "      <td>6.192108</td>\n",
       "      <td>3.850187</td>\n",
       "      <td>7.960717</td>\n",
       "      <td>8.586406</td>\n",
       "      <td>8.729021</td>\n",
       "      <td>13.567499</td>\n",
       "      <td>...</td>\n",
       "      <td>47.680974</td>\n",
       "      <td>23.840776</td>\n",
       "      <td>49.293623</td>\n",
       "      <td>53.167952</td>\n",
       "      <td>30.650253</td>\n",
       "      <td>33.059270</td>\n",
       "      <td>68.353950</td>\n",
       "      <td>70830.750000</td>\n",
       "      <td>1.673402</td>\n",
       "      <td>149.0006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>7.801407</td>\n",
       "      <td>2.628427</td>\n",
       "      <td>5.543288</td>\n",
       "      <td>9.133362</td>\n",
       "      <td>7.014197</td>\n",
       "      <td>5.730589</td>\n",
       "      <td>4.246228</td>\n",
       "      <td>6.930445</td>\n",
       "      <td>20.505433</td>\n",
       "      <td>43.245446</td>\n",
       "      <td>...</td>\n",
       "      <td>50.628851</td>\n",
       "      <td>40.195478</td>\n",
       "      <td>29.783881</td>\n",
       "      <td>48.611503</td>\n",
       "      <td>24.333390</td>\n",
       "      <td>39.715531</td>\n",
       "      <td>29.428252</td>\n",
       "      <td>72485.310000</td>\n",
       "      <td>1.614970</td>\n",
       "      <td>150.9995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>7.364198</td>\n",
       "      <td>8.455093</td>\n",
       "      <td>4.617628</td>\n",
       "      <td>3.805883</td>\n",
       "      <td>4.280772</td>\n",
       "      <td>6.398586</td>\n",
       "      <td>8.552977</td>\n",
       "      <td>8.138172</td>\n",
       "      <td>62.264979</td>\n",
       "      <td>34.005123</td>\n",
       "      <td>...</td>\n",
       "      <td>17.574150</td>\n",
       "      <td>27.390888</td>\n",
       "      <td>36.613344</td>\n",
       "      <td>34.837657</td>\n",
       "      <td>54.726959</td>\n",
       "      <td>52.072793</td>\n",
       "      <td>69.605594</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.734951</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>7.094305</td>\n",
       "      <td>2.423077</td>\n",
       "      <td>2.934079</td>\n",
       "      <td>6.637124</td>\n",
       "      <td>4.526698</td>\n",
       "      <td>3.664447</td>\n",
       "      <td>4.901074</td>\n",
       "      <td>5.327160</td>\n",
       "      <td>17.190049</td>\n",
       "      <td>20.815255</td>\n",
       "      <td>...</td>\n",
       "      <td>19.473850</td>\n",
       "      <td>16.587847</td>\n",
       "      <td>22.185683</td>\n",
       "      <td>24.114444</td>\n",
       "      <td>17.959727</td>\n",
       "      <td>19.521095</td>\n",
       "      <td>26.108803</td>\n",
       "      <td>73446.170000</td>\n",
       "      <td>0.853824</td>\n",
       "      <td>150.9995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>3.451784</td>\n",
       "      <td>3.164933</td>\n",
       "      <td>3.213005</td>\n",
       "      <td>9.450705</td>\n",
       "      <td>8.759146</td>\n",
       "      <td>8.331735</td>\n",
       "      <td>2.206001</td>\n",
       "      <td>6.427374</td>\n",
       "      <td>10.924665</td>\n",
       "      <td>11.090597</td>\n",
       "      <td>...</td>\n",
       "      <td>30.365158</td>\n",
       "      <td>72.978885</td>\n",
       "      <td>19.322682</td>\n",
       "      <td>56.298306</td>\n",
       "      <td>18.379814</td>\n",
       "      <td>53.551178</td>\n",
       "      <td>14.178791</td>\n",
       "      <td>55933.920000</td>\n",
       "      <td>2.408665</td>\n",
       "      <td>131.0084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Schord     Sspan    Ssweep      Stip    Bchord     Bspan    Bsweep  \\\n",
       "0    9.465233  9.191411  6.313146  6.181568  4.253219  3.362766  2.004250   \n",
       "1    5.050903  2.807665  7.250966  4.202998  7.871765  6.329711  3.563606   \n",
       "2    8.796076  5.131824  8.469040  8.168441  6.226139  7.792487  6.816335   \n",
       "3    5.590219  9.233072  6.352605  8.429602  5.672564  5.705322  5.717624   \n",
       "4    2.916229  4.216994  4.711545  6.462613  4.887217  2.638681  9.605809   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "395  2.608430  3.346465  5.201403  9.166944  6.192108  3.850187  7.960717   \n",
       "396  7.801407  2.628427  5.543288  9.133362  7.014197  5.730589  4.246228   \n",
       "397  7.364198  8.455093  4.617628  3.805883  4.280772  6.398586  8.552977   \n",
       "398  7.094305  2.423077  2.934079  6.637124  4.526698  3.664447  4.901074   \n",
       "399  3.451784  3.164933  3.213005  9.450705  8.759146  8.331735  2.206001   \n",
       "\n",
       "         Btip  Schordspan  Schordsweep  ...  SSweeptip  Bchordspan  \\\n",
       "0    3.767645   86.998844    59.755399  ...  39.025144   14.302581   \n",
       "1    6.148326   14.181241    36.623926  ...  30.475799   49.826002   \n",
       "2    8.795342   45.139920    74.494323  ...  69.178850   48.517110   \n",
       "3    3.420081   51.614896    35.512459  ...  53.549937   32.363803   \n",
       "4    5.234709   12.297724    13.739946  ...  30.448892   12.895807   \n",
       "..        ...         ...          ...  ...        ...         ...   \n",
       "395  8.586406    8.729021    13.567499  ...  47.680974   23.840776   \n",
       "396  6.930445   20.505433    43.245446  ...  50.628851   40.195478   \n",
       "397  8.138172   62.264979    34.005123  ...  17.574150   27.390888   \n",
       "398  5.327160   17.190049    20.815255  ...  19.473850   16.587847   \n",
       "399  6.427374   10.924665    11.090597  ...  30.365158   72.978885   \n",
       "\n",
       "     Bchordsweep  Bchordtip  Bspansweep   BSpantip  BSweeptip      Altitude  \\\n",
       "0       8.524515  16.024619    6.739826  12.669711   7.551304      0.004903   \n",
       "1      28.051867  48.398181   22.556595  38.917130  21.910210  77117.160000   \n",
       "2      42.439452  54.761021   53.116208  68.537590  59.951999  51216.000000   \n",
       "3      32.433584  19.400628   32.620884  19.512664  19.554736      0.004691   \n",
       "4      46.945675  25.583158   25.346667  13.812727  50.283615      0.004818   \n",
       "..           ...        ...         ...        ...        ...           ...   \n",
       "395    49.293623  53.167952   30.650253  33.059270  68.353950  70830.750000   \n",
       "396    29.783881  48.611503   24.333390  39.715531  29.428252  72485.310000   \n",
       "397    36.613344  34.837657   54.726959  52.072793  69.605594      0.000000   \n",
       "398    22.185683  24.114444   17.959727  19.521095  26.108803  73446.170000   \n",
       "399    19.322682  56.298306   18.379814  53.551178  14.178791  55933.920000   \n",
       "\n",
       "     Stability      Time  \n",
       "0    -5.121752    0.0100  \n",
       "1     1.278655  154.9973  \n",
       "2     5.212006  125.0071  \n",
       "3    -2.489141    0.0100  \n",
       "4    -4.231226    0.0100  \n",
       "..         ...       ...  \n",
       "395   1.673402  149.0006  \n",
       "396   1.614970  150.9995  \n",
       "397  -1.734951    0.0000  \n",
       "398   0.853824  150.9995  \n",
       "399   2.408665  131.0084  \n",
       "\n",
       "[400 rows x 23 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.rename(columns={0: \"Schord\", 1: \"Sspan\", 2: \"Ssweep\", 3: \"Stip\", 4: \"Bchord\", \n",
    "                        5: \"Bspan\", 6: \"Bsweep\", 7: \"Btip\", 8: \"Altitude\", 9: \"Stability\", 10: \"Time\"})\n",
    "df['Schordspan'] = df['Schord'] * df['Sspan']\n",
    "df['Schordsweep'] = df['Schord'] * df['Ssweep']\n",
    "df['Schordtip'] = df['Schord'] * df['Stip']\n",
    "df['Sspansweep'] = df['Sspan'] * df['Ssweep']\n",
    "df['SSpantip'] = df['Sspan'] * df['Stip']\n",
    "df['SSweeptip'] = df['Ssweep'] * df['Stip']\n",
    "\n",
    "df['Bchordspan'] = df['Bchord'] * df['Bspan']\n",
    "df['Bchordsweep'] = df['Bchord'] * df['Bsweep']\n",
    "df['Bchordtip'] = df['Bchord'] * df['Btip']\n",
    "df['Bspansweep'] = df['Bspan'] * df['Bsweep']\n",
    "df['BSpantip'] = df['Bspan'] * df['Btip']\n",
    "df['BSweeptip'] = df['Bsweep'] * df['Btip']\n",
    "alt = df.pop(\"Altitude\")\n",
    "stab = df.pop(\"Stability\")\n",
    "time = df.pop(\"Time\")\n",
    "\n",
    "df.insert(len(df.columns), \"Altitude\", alt)\n",
    "df.insert(len(df.columns), \"Stability\", stab)\n",
    "df.insert(len(df.columns), \"Time\", time)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c2f07700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Schord</th>\n",
       "      <th>Sspan</th>\n",
       "      <th>Ssweep</th>\n",
       "      <th>Stip</th>\n",
       "      <th>Bchord</th>\n",
       "      <th>Bspan</th>\n",
       "      <th>Bsweep</th>\n",
       "      <th>Btip</th>\n",
       "      <th>Schordspan</th>\n",
       "      <th>Schordsweep</th>\n",
       "      <th>...</th>\n",
       "      <th>SSweeptip</th>\n",
       "      <th>Bchordspan</th>\n",
       "      <th>Bchordsweep</th>\n",
       "      <th>Bchordtip</th>\n",
       "      <th>Bspansweep</th>\n",
       "      <th>BSpantip</th>\n",
       "      <th>BSweeptip</th>\n",
       "      <th>Altitude</th>\n",
       "      <th>Stability</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.050903</td>\n",
       "      <td>2.807665</td>\n",
       "      <td>7.250966</td>\n",
       "      <td>4.202998</td>\n",
       "      <td>7.871765</td>\n",
       "      <td>6.329711</td>\n",
       "      <td>3.563606</td>\n",
       "      <td>6.148326</td>\n",
       "      <td>14.181241</td>\n",
       "      <td>36.623926</td>\n",
       "      <td>...</td>\n",
       "      <td>30.475799</td>\n",
       "      <td>49.826002</td>\n",
       "      <td>28.051867</td>\n",
       "      <td>48.398181</td>\n",
       "      <td>22.556595</td>\n",
       "      <td>38.917130</td>\n",
       "      <td>21.910210</td>\n",
       "      <td>77117.16</td>\n",
       "      <td>1.278655</td>\n",
       "      <td>154.9973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.796076</td>\n",
       "      <td>5.131824</td>\n",
       "      <td>8.469040</td>\n",
       "      <td>8.168441</td>\n",
       "      <td>6.226139</td>\n",
       "      <td>7.792487</td>\n",
       "      <td>6.816335</td>\n",
       "      <td>8.795342</td>\n",
       "      <td>45.139920</td>\n",
       "      <td>74.494323</td>\n",
       "      <td>...</td>\n",
       "      <td>69.178850</td>\n",
       "      <td>48.517110</td>\n",
       "      <td>42.439452</td>\n",
       "      <td>54.761021</td>\n",
       "      <td>53.116208</td>\n",
       "      <td>68.537590</td>\n",
       "      <td>59.951999</td>\n",
       "      <td>51216.00</td>\n",
       "      <td>5.212006</td>\n",
       "      <td>125.0071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.562160</td>\n",
       "      <td>4.337768</td>\n",
       "      <td>7.930397</td>\n",
       "      <td>4.372560</td>\n",
       "      <td>9.417980</td>\n",
       "      <td>8.246008</td>\n",
       "      <td>2.728282</td>\n",
       "      <td>6.326085</td>\n",
       "      <td>37.140668</td>\n",
       "      <td>67.901332</td>\n",
       "      <td>...</td>\n",
       "      <td>34.676133</td>\n",
       "      <td>77.660732</td>\n",
       "      <td>25.694900</td>\n",
       "      <td>59.578937</td>\n",
       "      <td>22.497430</td>\n",
       "      <td>52.164942</td>\n",
       "      <td>17.259340</td>\n",
       "      <td>54082.50</td>\n",
       "      <td>4.485508</td>\n",
       "      <td>129.0079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8.703811</td>\n",
       "      <td>2.188438</td>\n",
       "      <td>9.143090</td>\n",
       "      <td>2.635864</td>\n",
       "      <td>9.087634</td>\n",
       "      <td>8.094306</td>\n",
       "      <td>6.826608</td>\n",
       "      <td>3.754130</td>\n",
       "      <td>19.047753</td>\n",
       "      <td>79.579730</td>\n",
       "      <td>...</td>\n",
       "      <td>24.099946</td>\n",
       "      <td>73.558093</td>\n",
       "      <td>62.037714</td>\n",
       "      <td>34.116158</td>\n",
       "      <td>55.256655</td>\n",
       "      <td>30.387077</td>\n",
       "      <td>25.627973</td>\n",
       "      <td>78784.51</td>\n",
       "      <td>0.613785</td>\n",
       "      <td>156.9962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.074672</td>\n",
       "      <td>3.479253</td>\n",
       "      <td>5.061755</td>\n",
       "      <td>2.201202</td>\n",
       "      <td>8.430403</td>\n",
       "      <td>6.003352</td>\n",
       "      <td>2.080965</td>\n",
       "      <td>3.394916</td>\n",
       "      <td>28.093824</td>\n",
       "      <td>40.872014</td>\n",
       "      <td>...</td>\n",
       "      <td>11.141944</td>\n",
       "      <td>50.610675</td>\n",
       "      <td>17.543371</td>\n",
       "      <td>28.620512</td>\n",
       "      <td>12.492762</td>\n",
       "      <td>20.380874</td>\n",
       "      <td>7.064700</td>\n",
       "      <td>59576.45</td>\n",
       "      <td>2.920013</td>\n",
       "      <td>136.0078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>5.247843</td>\n",
       "      <td>7.799644</td>\n",
       "      <td>8.441005</td>\n",
       "      <td>9.658211</td>\n",
       "      <td>6.937770</td>\n",
       "      <td>7.882306</td>\n",
       "      <td>6.353957</td>\n",
       "      <td>9.196173</td>\n",
       "      <td>40.931306</td>\n",
       "      <td>44.297067</td>\n",
       "      <td>...</td>\n",
       "      <td>81.524999</td>\n",
       "      <td>54.685626</td>\n",
       "      <td>44.082290</td>\n",
       "      <td>63.800932</td>\n",
       "      <td>50.083829</td>\n",
       "      <td>72.487046</td>\n",
       "      <td>58.432082</td>\n",
       "      <td>39893.78</td>\n",
       "      <td>5.851534</td>\n",
       "      <td>111.0041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>2.608430</td>\n",
       "      <td>3.346465</td>\n",
       "      <td>5.201403</td>\n",
       "      <td>9.166944</td>\n",
       "      <td>6.192108</td>\n",
       "      <td>3.850187</td>\n",
       "      <td>7.960717</td>\n",
       "      <td>8.586406</td>\n",
       "      <td>8.729021</td>\n",
       "      <td>13.567499</td>\n",
       "      <td>...</td>\n",
       "      <td>47.680974</td>\n",
       "      <td>23.840776</td>\n",
       "      <td>49.293623</td>\n",
       "      <td>53.167952</td>\n",
       "      <td>30.650253</td>\n",
       "      <td>33.059270</td>\n",
       "      <td>68.353950</td>\n",
       "      <td>70830.75</td>\n",
       "      <td>1.673402</td>\n",
       "      <td>149.0006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>7.801407</td>\n",
       "      <td>2.628427</td>\n",
       "      <td>5.543288</td>\n",
       "      <td>9.133362</td>\n",
       "      <td>7.014197</td>\n",
       "      <td>5.730589</td>\n",
       "      <td>4.246228</td>\n",
       "      <td>6.930445</td>\n",
       "      <td>20.505433</td>\n",
       "      <td>43.245446</td>\n",
       "      <td>...</td>\n",
       "      <td>50.628851</td>\n",
       "      <td>40.195478</td>\n",
       "      <td>29.783881</td>\n",
       "      <td>48.611503</td>\n",
       "      <td>24.333390</td>\n",
       "      <td>39.715531</td>\n",
       "      <td>29.428252</td>\n",
       "      <td>72485.31</td>\n",
       "      <td>1.614970</td>\n",
       "      <td>150.9995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>7.094305</td>\n",
       "      <td>2.423077</td>\n",
       "      <td>2.934079</td>\n",
       "      <td>6.637124</td>\n",
       "      <td>4.526698</td>\n",
       "      <td>3.664447</td>\n",
       "      <td>4.901074</td>\n",
       "      <td>5.327160</td>\n",
       "      <td>17.190049</td>\n",
       "      <td>20.815255</td>\n",
       "      <td>...</td>\n",
       "      <td>19.473850</td>\n",
       "      <td>16.587847</td>\n",
       "      <td>22.185683</td>\n",
       "      <td>24.114444</td>\n",
       "      <td>17.959727</td>\n",
       "      <td>19.521095</td>\n",
       "      <td>26.108803</td>\n",
       "      <td>73446.17</td>\n",
       "      <td>0.853824</td>\n",
       "      <td>150.9995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>3.451784</td>\n",
       "      <td>3.164933</td>\n",
       "      <td>3.213005</td>\n",
       "      <td>9.450705</td>\n",
       "      <td>8.759146</td>\n",
       "      <td>8.331735</td>\n",
       "      <td>2.206001</td>\n",
       "      <td>6.427374</td>\n",
       "      <td>10.924665</td>\n",
       "      <td>11.090597</td>\n",
       "      <td>...</td>\n",
       "      <td>30.365158</td>\n",
       "      <td>72.978885</td>\n",
       "      <td>19.322682</td>\n",
       "      <td>56.298306</td>\n",
       "      <td>18.379814</td>\n",
       "      <td>53.551178</td>\n",
       "      <td>14.178791</td>\n",
       "      <td>55933.92</td>\n",
       "      <td>2.408665</td>\n",
       "      <td>131.0084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>197 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Schord     Sspan    Ssweep      Stip    Bchord     Bspan    Bsweep  \\\n",
       "1    5.050903  2.807665  7.250966  4.202998  7.871765  6.329711  3.563606   \n",
       "2    8.796076  5.131824  8.469040  8.168441  6.226139  7.792487  6.816335   \n",
       "5    8.562160  4.337768  7.930397  4.372560  9.417980  8.246008  2.728282   \n",
       "6    8.703811  2.188438  9.143090  2.635864  9.087634  8.094306  6.826608   \n",
       "8    8.074672  3.479253  5.061755  2.201202  8.430403  6.003352  2.080965   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "393  5.247843  7.799644  8.441005  9.658211  6.937770  7.882306  6.353957   \n",
       "395  2.608430  3.346465  5.201403  9.166944  6.192108  3.850187  7.960717   \n",
       "396  7.801407  2.628427  5.543288  9.133362  7.014197  5.730589  4.246228   \n",
       "398  7.094305  2.423077  2.934079  6.637124  4.526698  3.664447  4.901074   \n",
       "399  3.451784  3.164933  3.213005  9.450705  8.759146  8.331735  2.206001   \n",
       "\n",
       "         Btip  Schordspan  Schordsweep  ...  SSweeptip  Bchordspan  \\\n",
       "1    6.148326   14.181241    36.623926  ...  30.475799   49.826002   \n",
       "2    8.795342   45.139920    74.494323  ...  69.178850   48.517110   \n",
       "5    6.326085   37.140668    67.901332  ...  34.676133   77.660732   \n",
       "6    3.754130   19.047753    79.579730  ...  24.099946   73.558093   \n",
       "8    3.394916   28.093824    40.872014  ...  11.141944   50.610675   \n",
       "..        ...         ...          ...  ...        ...         ...   \n",
       "393  9.196173   40.931306    44.297067  ...  81.524999   54.685626   \n",
       "395  8.586406    8.729021    13.567499  ...  47.680974   23.840776   \n",
       "396  6.930445   20.505433    43.245446  ...  50.628851   40.195478   \n",
       "398  5.327160   17.190049    20.815255  ...  19.473850   16.587847   \n",
       "399  6.427374   10.924665    11.090597  ...  30.365158   72.978885   \n",
       "\n",
       "     Bchordsweep  Bchordtip  Bspansweep   BSpantip  BSweeptip  Altitude  \\\n",
       "1      28.051867  48.398181   22.556595  38.917130  21.910210  77117.16   \n",
       "2      42.439452  54.761021   53.116208  68.537590  59.951999  51216.00   \n",
       "5      25.694900  59.578937   22.497430  52.164942  17.259340  54082.50   \n",
       "6      62.037714  34.116158   55.256655  30.387077  25.627973  78784.51   \n",
       "8      17.543371  28.620512   12.492762  20.380874   7.064700  59576.45   \n",
       "..           ...        ...         ...        ...        ...       ...   \n",
       "393    44.082290  63.800932   50.083829  72.487046  58.432082  39893.78   \n",
       "395    49.293623  53.167952   30.650253  33.059270  68.353950  70830.75   \n",
       "396    29.783881  48.611503   24.333390  39.715531  29.428252  72485.31   \n",
       "398    22.185683  24.114444   17.959727  19.521095  26.108803  73446.17   \n",
       "399    19.322682  56.298306   18.379814  53.551178  14.178791  55933.92   \n",
       "\n",
       "     Stability      Time  \n",
       "1     1.278655  154.9973  \n",
       "2     5.212006  125.0071  \n",
       "5     4.485508  129.0079  \n",
       "6     0.613785  156.9962  \n",
       "8     2.920013  136.0078  \n",
       "..         ...       ...  \n",
       "393   5.851534  111.0041  \n",
       "395   1.673402  149.0006  \n",
       "396   1.614970  150.9995  \n",
       "398   0.853824  150.9995  \n",
       "399   2.408665  131.0084  \n",
       "\n",
       "[197 rows x 23 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df = df[df['Time'] > 90]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d818a4",
   "metadata": {},
   "source": [
    "# NN Stability Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c1563f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(45, 90)\n",
    "        self.activation1 = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(90, 60)\n",
    "        self.activation2 = torch.nn.ReLU()\n",
    "        self.linear3 = torch.nn.Linear(60, 30)\n",
    "        self.activation3 = torch.nn.ReLU()\n",
    "        self.linear4 = torch.nn.Linear(30, 10)\n",
    "        self.activation4 = torch.nn.ReLU()\n",
    "        self.linear5 = torch.nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.linear1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.activation3(x)\n",
    "        x = self.linear4(x)\n",
    "        x = self.activation4(x)\n",
    "        x = self.linear5(x)\n",
    "        \n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ff9703e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "def load_arrays(data_arrays, batch_size, train = True):\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset, batch_size, shuffle = train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "756fc5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classif = Classifier()\n",
    "classif.double()\n",
    "\n",
    "poly = PolynomialFeatures(2)\n",
    "\n",
    "raw_X = df.iloc[:,:8]\n",
    "raw_Y = df.iloc[:,-1] > 90\n",
    "batch_size = 10\n",
    "idxs = np.array(list(range(0, 300)))# + list(range(0))) # FOR CV\n",
    "\n",
    "train_X = torch.from_numpy(poly.fit_transform(raw_X.iloc[idxs])).double()\n",
    "train_Y = torch.from_numpy(np.array([[x] for x in raw_Y.iloc[idxs]])).double()\n",
    "\n",
    "test_X = torch.from_numpy(poly.fit_transform(raw_X[300:400])).double()\n",
    "test_Y = torch.from_numpy(np.array([[x] for x in raw_Y[300:400]])).double()\n",
    "data_iter = load_arrays((train_X, train_Y), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "18b6a402",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.BCELoss()\n",
    "trainer = torch.optim.SGD(classif.parameters(), lr=0.01)\n",
    "num_epochs = 250\n",
    "def train(num_epochs, trainer, loss):\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        for X, y in data_iter:\n",
    "            X.requires_grad = True\n",
    "            l = loss(classif(X) ,y)\n",
    "\n",
    "            trainer.zero_grad() #sets gradients to zero\n",
    "\n",
    "            l.backward() # back propagation\n",
    "\n",
    "            trainer.step() # parameter update\n",
    "\n",
    "        l = loss(classif(train_X), train_Y)\n",
    "        if epoch % 50 == 0:\n",
    "            print(f'epoch {epoch + 1}, loss {l:f}')\n",
    "            #print(X.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "48b477d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.542237\n",
      "epoch 51, loss 1.138355\n",
      "epoch 101, loss 0.001929\n",
      "epoch 151, loss 0.000670\n",
      "epoch 201, loss 0.000374\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, tensor([0.9100]))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(num_epochs, trainer, loss), sum((classif(test_X) >= 0.5) == test_Y)/len(test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c65752f",
   "metadata": {},
   "source": [
    "# Altitude Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a3954000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 0.9929351496740758\n",
      "Test RMSE Quad 1815.417729777189\n",
      "Test RMSE ridge 2056.5256021326427\n",
      "Test RMSE lasso 1815.019990798251\n",
      "Train RMSE 1186.314188487517\n"
     ]
    }
   ],
   "source": [
    "poly = PolynomialFeatures(2)\n",
    "raw = filtered_df.iloc[:,:8]\n",
    "qr = LinearRegression()\n",
    "idxs = np.array(list(range(150)))# + list(range(50)))\n",
    "\n",
    "ridge = Ridge(alpha=0.6)\n",
    "ridge.fit(poly.fit_transform(raw.iloc[idxs]), filtered_df.iloc[idxs, -3])\n",
    "lasso = Lasso(alpha=.2)\n",
    "lasso.fit(poly.fit_transform(raw.iloc[idxs]), filtered_df.iloc[idxs, -3])\n",
    "\n",
    "qr.fit(poly.fit_transform(raw.iloc[idxs]), filtered_df.iloc[idxs, -3])\n",
    "print('R2', ridge.score(poly.fit_transform(raw.iloc[idxs]), filtered_df.iloc[idxs, -3]))\n",
    "print('Test RMSE Quad', np.sqrt(MSE(filtered_df.iloc[150:, -3], qr.predict(poly.fit_transform(raw[150:])))))\n",
    "print('Test RMSE ridge', np.sqrt(MSE(filtered_df.iloc[150:, -3], ridge.predict(poly.fit_transform(raw[150:])))))\n",
    "print('Test RMSE lasso', np.sqrt(MSE(filtered_df.iloc[150:, -3], lasso.predict(poly.fit_transform(raw[150:])))))\n",
    "\n",
    "print('Train RMSE', np.sqrt(MSE(filtered_df.iloc[idxs, -3], ridge.predict(poly.fit_transform(raw.iloc[idxs])))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c5748c",
   "metadata": {},
   "source": [
    "# Solving COP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3aeef2ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (linear1): Linear(in_features=45, out_features=90, bias=True)\n",
       "  (activation1): ReLU()\n",
       "  (linear2): Linear(in_features=90, out_features=60, bias=True)\n",
       "  (activation2): ReLU()\n",
       "  (linear3): Linear(in_features=60, out_features=30, bias=True)\n",
       "  (activation3): ReLU()\n",
       "  (linear4): Linear(in_features=30, out_features=10, bias=True)\n",
       "  (activation4): ReLU()\n",
       "  (linear5): Linear(in_features=10, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stab_constraint(X):\n",
    "    with torch.no_grad():\n",
    "        return classif(torch.from_numpy(poly.fit_transform([X])))[0] - 0.6\n",
    "def l0(X):\n",
    "    return X[0]-2\n",
    "def l1(X):\n",
    "    return X[1]-2\n",
    "def l2(X):\n",
    "    return X[2]-2\n",
    "def l3(X):\n",
    "    return X[3]-2\n",
    "def l4(X):\n",
    "    return X[4]-2\n",
    "def l5(X):\n",
    "    return X[5]-2\n",
    "def l6(X):\n",
    "    return X[6]-2\n",
    "def l7(X):\n",
    "    return X[7]-2\n",
    "\n",
    "def u0(X):\n",
    "    return 10-X[0]\n",
    "def u1(X):\n",
    "    return 10-X[1]\n",
    "def u2(X):\n",
    "    return 10-X[2]\n",
    "def u3(X):\n",
    "    return 10-X[3]\n",
    "def u4(X):\n",
    "    return 10-X[4]\n",
    "def u5(X):\n",
    "    return 10-X[5]\n",
    "def u6(X):\n",
    "    return 10-X[6]\n",
    "def u7(X):\n",
    "    return 10-X[7]\n",
    "def objective(X):\n",
    "    return -ridge.predict(poly.fit_transform([X]))[0]\n",
    "classif.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a84dce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ineq_cons = {'type': 'ineq',\n",
    "             'fun' : stab_constraint}\n",
    "L0 = {'type': 'ineq',\n",
    "             'fun' : l0}\n",
    "L1 = {'type': 'ineq',\n",
    "             'fun' : l1}\n",
    "L2 = {'type': 'ineq',\n",
    "             'fun' : l2}\n",
    "L3 = {'type': 'ineq',\n",
    "             'fun' : l3}\n",
    "L4 = {'type': 'ineq',\n",
    "             'fun' : l4}\n",
    "L5 = {'type': 'ineq',\n",
    "             'fun' : l5}\n",
    "L6 = {'type': 'ineq',\n",
    "             'fun' : l6}\n",
    "L7 = {'type': 'ineq',\n",
    "             'fun' : l7}\n",
    "U0 = {'type': 'ineq',\n",
    "             'fun' : u0}\n",
    "U1 = {'type': 'ineq',\n",
    "             'fun' : u1}\n",
    "U2 = {'type': 'ineq',\n",
    "             'fun' : u2}\n",
    "U3 = {'type': 'ineq',\n",
    "             'fun' : u3}\n",
    "U4 = {'type': 'ineq',\n",
    "             'fun' : u4}\n",
    "U5 = {'type': 'ineq',\n",
    "             'fun' : u5}\n",
    "U6 = {'type': 'ineq',\n",
    "             'fun' : u6}\n",
    "U7 = {'type': 'ineq',\n",
    "             'fun' : u7}\n",
    "# x = minimize(objective, X0, method='trust-constr', options = {'disp':True}).x#, constraints = [ineq_cons], options={'ftol': 1e-9, 'disp': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1043e9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "samples = filtered_df.iloc[np.random.choice(len(filtered_df), 197), :8]\n",
    "results = []\n",
    "for i in range(len(samples)):\n",
    "    X0 = np.array(samples.iloc[i])\n",
    "    res = minimize(objective, X0, method='COBYLA', \n",
    "               constraints = [ineq_cons, L0, L1, L2, L3, L4, L5, L6, L7, U0, U1, U2, U3, U4, U5, U6, U7], \n",
    "               options = {'verbose':1, 'display':1, 'maxiter':10}) # constraints = [constraint], options = {'verbose':1})#, bounds = bounds)\n",
    "    results.append((X0, res.x, -res.fun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e76fd65b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([3.84330328, 3.11903088, 8.63367165, 8.88974916, 8.81972552,\n",
       "         3.69938965, 8.75334237, 8.74292895]),\n",
       "  array([3.75545987, 2.17043145, 9.87274423, 8.8384022 , 9.82653983,\n",
       "         3.52100479, 9.77981865, 9.75197606]),\n",
       "  102075.54725547766),\n",
       " (array([2.32452262, 2.58871554, 9.32749721, 9.99315489, 4.52452962,\n",
       "         7.02169385, 4.2840132 , 2.97129558]),\n",
       "  array([ 2.        ,  2.        , 10.        ,  9.78758507,  4.42929722,\n",
       "          6.43681893,  5.50151296,  3.9827708 ]),\n",
       "  97462.75542251446),\n",
       " (array([3.02924659, 2.74524202, 6.29978985, 7.17708261, 5.44099747,\n",
       "         3.88833632, 4.58883198, 2.70679481]),\n",
       "  array([2.85636718, 2.        , 7.84065985, 7.09442702, 5.40725822,\n",
       "         3.5565845 , 5.65175583, 3.72085619]),\n",
       "  97361.97930286969),\n",
       " (array([8.76254164, 2.40324077, 8.77518014, 3.82322874, 4.85104019,\n",
       "         4.60577037, 8.11653639, 8.55373399]),\n",
       "  array([ 8.1369557 ,  2.        , 10.        ,  3.77095472,  4.85471749,\n",
       "          5.18932401,  9.34236309,  8.51885573]),\n",
       "  96251.13056213336),\n",
       " (array([5.21541801, 3.2452491 , 8.89373312, 5.15294394, 7.57586859,\n",
       "         4.11137217, 7.01092286, 2.74712375]),\n",
       "  array([ 5.1324772 ,  2.46115215, 10.        ,  5.12496828,  7.56551266,\n",
       "          4.7157413 ,  8.03789493,  3.7589865 ]),\n",
       "  93546.65471376994),\n",
       " (array([5.21541801, 3.2452491 , 8.89373312, 5.15294394, 7.57586859,\n",
       "         4.11137217, 7.01092286, 2.74712375]),\n",
       "  array([ 5.1324772 ,  2.46115215, 10.        ,  5.12496828,  7.56551266,\n",
       "          4.7157413 ,  8.03789493,  3.7589865 ]),\n",
       "  93546.65471376994),\n",
       " (array([8.80520968, 2.50792814, 8.8845873 , 9.7151944 , 9.88507815,\n",
       "         3.43088659, 2.56398478, 7.8957911 ]),\n",
       "  array([ 8.32593158,  2.        , 10.        ,  9.51590434,  9.85671949,\n",
       "          4.03149811,  3.8750507 ,  8.92684695]),\n",
       "  93437.65446152823),\n",
       " (array([3.83671996, 2.70671316, 8.90347259, 7.96412085, 2.9236093 ,\n",
       "         9.71053206, 9.80305837, 5.86719892]),\n",
       "  array([ 3.77576202,  2.12497861, 10.        ,  7.9384921 ,  2.91174135,\n",
       "          9.65726066, 10.        ,  5.86356078]),\n",
       "  92237.85053475856),\n",
       " (array([5.05090265, 2.80766466, 7.25096646, 4.20299825, 7.8717653 ,\n",
       "         6.32971129, 3.56360566, 6.14832627]),\n",
       "  array([4.85592295, 2.        , 8.6841829 , 4.16717012, 7.84538968,\n",
       "         6.00573436, 4.68624293, 6.14633867]),\n",
       "  91973.75531129685),\n",
       " (array([4.17974297, 2.25156054, 7.04646572, 8.54041952, 8.84656144,\n",
       "         8.72235424, 8.18360111, 6.28557734]),\n",
       "  array([3.8616342 , 2.        , 8.7684656 , 8.39891604, 8.79484005,\n",
       "         8.36335651, 9.32143717, 6.90654728]),\n",
       "  90749.81555244593)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(results, key = lambda x: x[2], reverse = True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b87204",
   "metadata": {},
   "source": [
    "# Alternative Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bbdd33b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordinateAscentQuad(stab_classifier, alt_predictor, initial = [0,0,0,0,0,0,0,0], inc = 0.0000001, max_iter = 10000, confidence = 0.95):\n",
    "    '''\n",
    "    Keep increasing in direction until unstable. \n",
    "    '''\n",
    "    M = {}\n",
    "    c = 9\n",
    "    for i in range(8):\n",
    "        for j in range(i, 8):\n",
    "            M[(i,j)] = c\n",
    "            c += 1\n",
    "    def partials(x):\n",
    "        grad = []\n",
    "        coef = alt_predictor.coef_\n",
    "        for i in range(8):\n",
    "            g = coef[1 + i]\n",
    "            for j in range(8):\n",
    "                if i == j:\n",
    "                    g += 2*x[i]*coef[M[i,i]]\n",
    "                else:\n",
    "                    g += coef[M[min([i,j]), max([i,j])]]*x[j]\n",
    "            grad.append(g)\n",
    "        return np.array(grad)\n",
    "    \n",
    "    x = initial\n",
    "    old = qr.predict(poly.fit_transform([x]))\n",
    "    for _ in range(max_iter):\n",
    "        #for i in range(8):\n",
    "        y = np.array(x)\n",
    "        # y[i] += np.sign(alt_predictor.coef_[i])*inc\n",
    "        # print(y, partials(x)*inc)\n",
    "        y += partials(x) * inc\n",
    "        if np.all(y > 2) and qr.predict(poly.fit_transform([y])) > old: #((10 >= y[0] >= 6.0 and 5 >= y[1] >= 2 and 8 >= y[2] >= 3.5 and 4.0 >= y[3] >= 2.0) and (11 >= y[4] > 4 and 7 > y[5] > 3.5 and 8 > y[6] > 4 and 7 > y[7] > 2.5)) and qr.predict(poly.fit_transform([y])) > old:\n",
    "            nxt = stab_classifier(torch.from_numpy(poly.fit_transform([y])))[0]\n",
    "            if nxt > confidence or np.random.rand() > 0.5:\n",
    "                if nxt <= confidence:\n",
    "                    print('Non greedy step')\n",
    "                x = y[:]\n",
    "                #print(qr.predict(poly.fit_transform([y])) - old)\n",
    "                old = qr.predict(poly.fit_transform([y]))\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        print('No more feasible directions at iteration', _, '.', qr.predict(poly.fit_transform([x])) - qr.predict(poly.fit_transform([initial])))\n",
    "        break\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a5f23cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more feasible directions at iteration 1875 . [26382.75]\n",
      "[7.74386775 4.06369632 7.97899711 9.64484403 8.47690503 7.27085673\n",
      " 4.46140229 6.17626983] [58815.74838186] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[83609.39980815] [7.5577634  2.00026277 8.39165309 9.56958246 8.43403031 6.93506399\n",
      " 4.57966638 6.18334785] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 182 . [2933.]\n",
      "[7.06856866 2.21573269 6.34825617 7.88644084 7.55687063 9.58805763\n",
      " 3.48553571 3.46890728] [71884.60703795] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[74597.80185381] [7.04800105 2.00092853 6.39187744 7.88150927 7.54899506 9.56380815\n",
      " 3.5006234  3.47009681] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 2164 . [33209.25]\n",
      "[5.5656446  4.48493532 9.23656697 7.38481998 7.2770088  6.54463189\n",
      " 2.93160599 9.79820983] [60089.75505921] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[91561.99560445] [5.32070827 2.0000599  9.70568487 7.31456898 7.26325397 6.07687465\n",
      " 3.10665025 9.76508986] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 75 . [1374.75]\n",
      "[7.06647414 2.09162001 2.58717138 7.25293221 4.98726764 4.46944891\n",
      " 4.64931443 8.29512877] [70554.36590841] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[71783.88311094] [7.06008987 2.00082973 2.61530784 7.25125746 4.98640447 4.45783903\n",
      " 4.65177139 8.29431118] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 3155 . [34580.]\n",
      "[7.97029877 4.9929493  4.98303034 7.82559806 4.23966158 7.0562222\n",
      " 5.97291621 8.46008862] [46948.78347787] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[79007.31230149] [7.73861941 2.00114963 5.78805652 7.75228797 4.19103394 6.69251329\n",
      " 6.07724165 8.39844345] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 1219 . [10120.5]\n",
      "[2.08143832 5.49587179 6.29274552 9.61414548 9.6211931  5.716947\n",
      " 7.03471605 4.12567358] [49721.52987011] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[59498.44413124] [2.00000886 4.47186368 6.6105176  9.53687729 9.60444271 5.56417091\n",
      " 7.04849509 4.14080396] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 4375 . [42227.25]\n",
      "[2.59348001 5.82705433 3.91710052 4.40979897 7.12855959 7.86119938\n",
      " 2.97554615 9.19828572] [40635.06974869] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[80500.91565927] [2.23302238 2.00109383 5.09579209 4.3371274  7.08399086 7.38560991\n",
      " 3.22997022 9.09927453] tensor([[0.9999]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 0 . [0.]\n",
      "[6.412598   8.27427726 8.51503918 8.93291554 4.91323368 8.51247272\n",
      " 2.84247125 8.45886467] [36565.23176888] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[36565.23176888] [6.412598   8.27427726 8.51503918 8.93291554 4.91323368 8.51247272\n",
      " 2.84247125 8.45886467] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 1075 . [15188.]\n",
      "[3.45178366 3.16493323 3.21300463 9.45070468 8.7591459  8.33173528\n",
      " 2.20600069 6.42737392] [56510.52006091] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[70465.36227598] [3.35980291 2.00020115 3.53963782 9.40614807 8.72867322 8.21059529\n",
      " 2.27825668 6.42592998] tensor([[0.9999]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 4719 . [19304.]\n",
      "[2.11829325 6.82476744 2.0357269  9.60730567 3.60323836 8.49364242\n",
      " 6.92712904 3.5485901 ] [32835.25559196] tensor([[0.9998]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[50815.44729406] [2.0000387  4.27432686 3.21081951 9.37976132 3.45460939 8.53811302\n",
      " 6.88542586 3.48032127] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 2375 . [27719.]\n",
      "[2.26868462 5.67271368 9.03633636 2.63628808 9.05678772 5.00173664\n",
      " 2.85754855 8.3474022 ] [56986.84388283] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[83797.00716631] [2.00013405 3.31466068 9.59220712 2.58704515 9.06386337 4.46706277\n",
      " 3.0179097  8.3217514 ] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 1075 . [15188.]\n",
      "[3.45178366 3.16493323 3.21300463 9.45070468 8.7591459  8.33173528\n",
      " 2.20600069 6.42737392] [56510.52006091] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[70465.36227598] [3.35980291 2.00020115 3.53963782 9.40614807 8.72867322 8.21059529\n",
      " 2.27825668 6.42592998] tensor([[0.9999]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 1720 . [24718.5]\n",
      "[4.75209249 3.86500166 2.10559135 4.8232818  7.21272293 6.38729759\n",
      " 8.33755428 9.08530604] [50751.4268605] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[73755.8709201] [4.60279809 2.00092095 2.7337576  4.78783876 7.20624493 6.22591287\n",
      " 8.37375528 9.07230486] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 3155 . [35296.75]\n",
      "[7.53381978 5.0247256  4.36767679 2.42057232 8.78170563 6.50598406\n",
      " 6.48295633 9.8428026 ] [45963.32589217] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[79512.99323384] [7.24038391 2.00084355 5.27710872 2.42570458 8.77783465 6.07018421\n",
      " 6.61436267 9.80800135] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 3 . [1.25]\n",
      "[8.93118239 8.0127891  9.97709212 8.00819807 2.91740485 9.66763027\n",
      " 4.27111457 6.41234245] [37972.04280442] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[37973.13971972] [8.93111459 8.01227792 9.97710693 8.00814859 2.91732233 9.66744726\n",
      " 4.27119661 6.41222651] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 4167 . [43346.5]\n",
      "[4.14827727 5.9201598  7.34836891 9.24578232 4.57740199 9.50791698\n",
      " 7.98383891 4.3721734 ] [46323.79039916] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[87794.6055046] [3.78893587 2.00105512 8.19171818 9.0370904  4.45267537 9.1877288\n",
      " 8.13430574 4.34917316] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 2307 . [29302.5]\n",
      "[8.45286853 4.36425617 6.61358073 9.71639807 9.59923875 5.77161054\n",
      " 2.09760667 8.35827203] [53992.70835055] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[81236.7851904] [8.25987199 2.00021167 7.16701825 9.63978747 9.56760154 5.30168149\n",
      " 2.24260721 8.35260658] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "Non greedy step\n",
      "Non greedy step\n",
      "[4.0997709  4.72308756 6.04574065 4.31566137 4.58856511 5.25662271\n",
      " 7.65743655 6.56904682] [57316.27789995] tensor([[0.9976]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[78370.17578349] [3.93632757 2.93090427 6.55655441 4.27181846 4.57724329 5.01228224\n",
      " 7.69494387 6.55169858] tensor([[0.5972]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 1219 . [10120.5]\n",
      "[2.08143832 5.49587179 6.29274552 9.61414548 9.6211931  5.716947\n",
      " 7.03471605 4.12567358] [49721.52987011] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[59498.44413124] [2.00000886 4.47186368 6.6105176  9.53687729 9.60444271 5.56417091\n",
      " 7.04849509 4.14080396] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 1875 . [26382.75]\n",
      "[7.74386775 4.06369632 7.97899711 9.64484403 8.47690503 7.27085673\n",
      " 4.46140229 6.17626983] [58815.74838186] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[83609.39980815] [7.5577634  2.00026277 8.39165309 9.56958246 8.43403031 6.93506399\n",
      " 4.57966638 6.18334785] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 5207 . [40465.75]\n",
      "[7.63168609 5.96373284 2.15863472 3.58381163 6.85386144 7.45234988\n",
      " 8.66903784 3.46879002] [35588.95338002] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[73309.03429523] [7.29416653 2.00029515 3.65643495 3.55291147 6.7191173  7.17461302\n",
      " 8.68391253 3.48188815] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more feasible directions at iteration 1098 . [16946.25]\n",
      "[6.27364679 3.25691991 5.53904012 4.37568764 6.32528265 7.43092455\n",
      " 3.56693159 9.59640105] [62792.35400913] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[78557.44929311] [6.15324117 2.0002209  5.84465548 4.36734079 6.31173851 7.25869161\n",
      " 3.64875556 9.57538051] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 1566 . [22629.5]\n",
      "[2.12717718 4.24935594 4.83282894 9.48940368 6.69310519 4.95351523\n",
      " 4.76269905 3.49531252] [57273.48107284] tensor([[0.9994]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[77921.15288727] [2.00003738 2.55510722 5.33879829 9.39859211 6.66004585 4.71934416\n",
      " 4.80233832 3.50660002] tensor([[0.9958]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 3972 . [33281.]\n",
      "[7.95462855 5.20594564 2.04653061 7.76957186 8.74530383 7.54793829\n",
      " 5.16137886 7.01085736] [37898.16037398] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[68797.90852483] [7.73132528 2.00058436 3.16339209 7.68962515 8.66086158 7.21704695\n",
      " 5.27232236 6.99697346] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "Non greedy step\n",
      "Non greedy step\n",
      "Non greedy step\n",
      "[7.90470929 6.3330749  8.19267455 4.62380641 8.59141207 7.24683375\n",
      " 9.46541789 6.35831765] [45409.78745304] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[86939.30578373] [7.47508409 2.31884821 9.11629962 4.53220745 8.54867795 6.62318208\n",
      " 9.59089462 6.37106306] tensor([[0.5939]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 123 . [916.5]\n",
      "[6.25437912 4.24696162 3.97600304 7.53631715 2.00278013 7.8963462\n",
      " 2.3164338  8.83421963] [49460.83302599] tensor([[0.9999]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[50294.37836862] [6.24760674 4.15106731 4.00496783 7.53445263 2.00002088 7.88607157\n",
      " 2.32204894 8.82948513] tensor([[0.9999]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 673 . [13343.75]\n",
      "[3.41457626 2.88066793 9.95030403 6.65990444 5.80259326 7.74979651\n",
      " 2.11154534 2.27863421] [79292.20127463] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[91634.27842607] [ 3.32111588  2.00122664 10.09947702  6.63399798  5.77761355  7.610062\n",
      "  2.17564988  2.28127251] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[5.0058681  6.10893665 4.29003155 4.94664391 8.75459312 8.26696262\n",
      " 8.48254088 5.12644021] [40089.54665431] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[72174.09637142] [4.68081868 2.69266904 5.39356281 4.84462493 8.67373917 7.97564257\n",
      " 8.56795901 5.13870262] tensor([[0.6013]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 795 . [18229.75]\n",
      "[3.84330328 3.11903088 8.63367165 8.88974916 8.81972552 3.69938965\n",
      " 8.75334237 8.74292895] [83527.50545338] tensor([[0.9991]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[100727.91931075] [3.74520435 2.00065464 8.89324905 8.84050737 8.8256509  3.51651045\n",
      " 8.78215399 8.75084274] tensor([[0.9979]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 1720 . [24718.5]\n",
      "[4.75209249 3.86500166 2.10559135 4.8232818  7.21272293 6.38729759\n",
      " 8.33755428 9.08530604] [50751.4268605] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[73755.8709201] [4.60279809 2.00092095 2.7337576  4.78783876 7.20624493 6.22591287\n",
      " 8.37375528 9.07230486] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[5.21541801 3.2452491  8.89373312 5.15294394 7.57586859 4.11137217\n",
      " 7.01092286 2.74712375] [80778.74342559] tensor([[0.0006]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[80778.74342559] [5.21541801 3.2452491  8.89373312 5.15294394 7.57586859 4.11137217\n",
      " 7.01092286 2.74712375] tensor([[0.0006]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 3562 . [38736.5]\n",
      "[2.37535311 6.02267168 8.37377495 4.53309991 7.7243974  6.89612848\n",
      " 4.43453858 3.57922737] [50911.8254316] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[87926.52169395] [2.0000005  2.62861719 9.15732539 4.41773882 7.65308496 6.30705897\n",
      " 4.62783081 3.58448059] tensor([[0.9996]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 3407 . [31065.]\n",
      "[7.59534288 4.98718601 2.62978363 5.69085127 8.31056625 9.62661298\n",
      " 9.06865306 9.8931685 ] [40538.78925346] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[70632.07733244] [7.34101642 2.00074153 3.56696159 5.64789319 8.26856374 9.48936966\n",
      " 9.17396804 9.8550342 ] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 4167 . [43346.5]\n",
      "[4.14827727 5.9201598  7.34836891 9.24578232 4.57740199 9.50791698\n",
      " 7.98383891 4.3721734 ] [46323.79039916] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[87794.6055046] [3.78893587 2.00105512 8.19171818 9.0370904  4.45267537 9.1877288\n",
      " 8.13430574 4.34917316] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 795 . [18229.75]\n",
      "[3.84330328 3.11903088 8.63367165 8.88974916 8.81972552 3.69938965\n",
      " 8.75334237 8.74292895] [83527.50545338] tensor([[0.9991]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[100727.91931075] [3.74520435 2.00065464 8.89324905 8.84050737 8.8256509  3.51651045\n",
      " 8.78215399 8.75084274] tensor([[0.9979]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 1534 . [21974.]\n",
      "[6.7654802  3.66965415 2.85853634 3.91202373 9.98809675 6.17321435\n",
      " 9.28874301 5.78085487] [53814.6229849] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[74398.85771367] [6.61921564 2.0007284  3.40247689 3.88766228 9.96988965 6.00103013\n",
      " 9.31614426 5.80283129] tensor([[0.9838]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 245 . [4827.75]\n",
      "[7.65964728 2.31993718 6.55140501 7.95313613 3.32003131 6.73966247\n",
      " 9.09835921 7.06541188] [78749.85716012] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[83203.95234223] [7.63281091 2.00109189 6.62484268 7.94469576 3.3158334  6.70567441\n",
      " 9.10682444 7.06380378] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 1375 . [22830.25]\n",
      "[2.15196124 4.35963291 9.26497618 7.90888302 2.78174951 5.73667108\n",
      " 4.98554116 7.05954161] [67870.91201759] tensor([[0.9996]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[89162.28972006] [2.00001255 2.71890125 9.60677412 7.839691   2.76723088 5.4780076\n",
      " 5.05879261 7.03483445] tensor([[0.9961]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 22 . [346.25]\n",
      "[9.7911429  2.02615873 4.52231431 3.79395457 6.29207474 8.47450088\n",
      " 6.39128321 2.82500429] [70610.03028692] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[70926.79840709] [9.78870479 2.00080834 4.52862879 3.79397963 6.29120047 8.47201702\n",
      " 6.39246692 2.82516635] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 5288 . [46723.75]\n",
      "[5.45449515 6.41719777 7.02404165 3.21488489 2.19329451 8.42931613\n",
      " 4.94438794 3.32288331] [44879.79218062] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[88464.67038784] [4.97499928 2.00084033 8.05586031 3.17178548 2.02225434 7.88283834\n",
      " 5.1836895  3.22157981] tensor([[0.9999]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 687 . [11591.75]\n",
      "[4.48406123 2.84138945 6.15259486 2.40760362 7.76871409 9.821738\n",
      " 6.55722958 9.38768387] [70051.23631958] tensor([[0.9997]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[81143.11752599] [4.39229423 2.00059311 6.34067012 2.40285152 7.75945774 9.7406191\n",
      " 6.61471416 9.37935542] tensor([[0.9725]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 2970 . [39255.5]\n",
      "[5.14102609 5.07067598 7.82167251 5.88382289 4.616266   5.83559506\n",
      " 2.12023149 2.63866538] [55630.58360061] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[91488.76682215] [4.84268405 2.0001729  8.51254126 5.8057545  4.52764355 5.26720454\n",
      " 2.29140563 2.62520257] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more feasible directions at iteration 1961 . [26365.]\n",
      "[7.83616152 4.12446818 8.26146515 7.8197602  2.56544452 9.52279921\n",
      " 6.45425932 4.03578958] [59397.24175063] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[84005.11141747] [7.63101305 2.00013598 8.6365725  7.76443017 2.49028035 9.30122896\n",
      " 6.56777954 4.01569402] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 5348 . [47300.75]\n",
      "[8.04243762 6.54567728 9.72284694 2.59465914 4.73874635 8.58703844\n",
      " 3.40612364 8.84626268] [42520.31523618] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[88326.25075242] [ 7.4853625   2.00024338 10.45196365  2.620206    4.66244171  7.70170846\n",
      "  3.82171086  8.68409201] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 2258 . [31125.75]\n",
      "[7.10731359 4.39252568 6.82182993 3.48153784 3.91557955 5.82419792\n",
      " 2.60837853 4.97694776] [57826.12845189] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[86173.72581708] [6.87134986 2.00072168 7.39087261 3.47257739 3.86256    5.40420134\n",
      " 2.74380108 4.94487298] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "Non greedy step\n",
      "[4.73444547 4.55713543 4.76768044 3.43126218 6.18119505 7.97143628\n",
      " 6.97046524 6.40215195] [51989.75142656] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[76337.79837592] [4.52153286 2.38804783 5.37723365 3.40268432 6.14334685 7.75844389\n",
      " 7.06101288 6.38333018] tensor([[0.5994]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 5207 . [40465.75]\n",
      "[7.63168609 5.96373284 2.15863472 3.58381163 6.85386144 7.45234988\n",
      " 8.66903784 3.46879002] [35588.95338002] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[73309.03429523] [7.29416653 2.00029515 3.65643495 3.55291147 6.7191173  7.17461302\n",
      " 8.68391253 3.48188815] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 687 . [11591.75]\n",
      "[4.48406123 2.84138945 6.15259486 2.40760362 7.76871409 9.821738\n",
      " 6.55722958 9.38768387] [70051.23631958] tensor([[0.9997]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[81143.11752599] [4.39229423 2.00059311 6.34067012 2.40285152 7.75945774 9.7406191\n",
      " 6.61471416 9.37935542] tensor([[0.9725]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 0 . [0.]\n",
      "[9.83432547 6.94863436 3.83738488 4.29967216 8.70712003 7.72073973\n",
      " 3.80252696 6.15572894] [34755.32172332] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[34755.32172332] [9.83432547 6.94863436 3.83738488 4.29967216 8.70712003 7.72073973\n",
      " 3.80252696 6.15572894] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "Non greedy step\n",
      "[4.73444547 4.55713543 4.76768044 3.43126218 6.18119505 7.97143628\n",
      " 6.97046524 6.40215195] [51989.75142656] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[76337.79837592] [4.52153286 2.38804783 5.37723365 3.40268432 6.14334685 7.75844389\n",
      " 7.06101288 6.38333018] tensor([[0.5994]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 3972 . [33281.]\n",
      "[7.95462855 5.20594564 2.04653061 7.76957186 8.74530383 7.54793829\n",
      " 5.16137886 7.01085736] [37898.16037398] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[68797.90852483] [7.73132528 2.00058436 3.16339209 7.68962515 8.66086158 7.21704695\n",
      " 5.27232236 6.99697346] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "Non greedy step\n",
      "[2.60843046 3.34646482 5.20140329 9.16694417 6.19210809 3.8501873\n",
      " 7.96071749 8.58640565] [69784.03388672] tensor([[0.0031]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[69799.31002231] [2.60834589 3.34529544 5.20175683 9.16688611 6.19211113 3.85003121\n",
      " 7.96072856 8.58640151] tensor([[0.0031]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 1219 . [10120.5]\n",
      "[2.08143832 5.49587179 6.29274552 9.61414548 9.6211931  5.716947\n",
      " 7.03471605 4.12567358] [49721.52987011] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[59498.44413124] [2.00000886 4.47186368 6.6105176  9.53687729 9.60444271 5.56417091\n",
      " 7.04849509 4.14080396] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[3.42529972 3.12091281 6.22450424 3.86772238 2.05499433 5.28858019\n",
      " 7.22884385 3.77197677] [75638.697986] tensor([[0.8266]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[78877.55572625] [3.40277027 2.87659861 6.28976789 3.8625242  2.05109853 5.25897233\n",
      " 7.23456284 3.77045961] tensor([[0.6010]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "Non greedy step\n",
      "Non greedy step\n",
      "[5.47555738 4.89731791 5.30721761 2.42642505 5.30376549 8.10282177\n",
      " 9.92544991 8.50215086] [50495.37112326] tensor([[0.9999]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[62516.11116081] [5.35396575 3.6931578  5.66011971 2.4160655  5.29501936 8.0058069\n",
      " 9.95657579 8.47893739] tensor([[0.5970]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 5290 . [45238.25]\n",
      "[5.51311011 6.26822205 3.42095021 9.23743387 7.05662242 6.82980833\n",
      " 8.57853598 3.86830712] [36603.64908281] tensor([[0.9748]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[79018.35370117] [5.22519568 2.00049955 4.89606377 8.99509888 6.9358162  6.45821482\n",
      " 8.56143747 3.90119981] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 1631 . [25612.]\n",
      "[6.41370253 3.84796082 6.6787151  4.50722332 2.41903965 5.59174267\n",
      " 3.64566932 3.20541394] [64393.03869039] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[87480.3020954] [6.24106954 2.00038514 7.12399916 4.48410173 2.3715501  5.30446221\n",
      " 3.72644428 3.18939501] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 0 . [0.]\n",
      "[9.46572723 8.31048572 9.53400387 8.08596509 8.53904379 8.14953446\n",
      " 2.57197839 2.32150929] [37446.26714939] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[37446.26714939] [9.46572723 8.31048572 9.53400387 8.08596509 8.53904379 8.14953446\n",
      " 2.57197839 2.32150929] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 795 . [18229.75]\n",
      "[3.84330328 3.11903088 8.63367165 8.88974916 8.81972552 3.69938965\n",
      " 8.75334237 8.74292895] [83527.50545338] tensor([[0.9991]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[100727.91931075] [3.74520435 2.00065464 8.89324905 8.84050737 8.8256509  3.51651045\n",
      " 8.78215399 8.75084274] tensor([[0.9979]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 2307 . [29302.5]\n",
      "[8.45286853 4.36425617 6.61358073 9.71639807 9.59923875 5.77161054\n",
      " 2.09760667 8.35827203] [53992.70835055] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[81236.7851904] [8.25987199 2.00021167 7.16701825 9.63978747 9.56760154 5.30168149\n",
      " 2.24260721 8.35260658] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 2547 . [22923.5]\n",
      "[9.4666134  4.14682708 2.14685041 2.35237228 2.33405622 9.83755059\n",
      " 4.03148476 3.2407511 ] [45221.4271827] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[65711.95675028] [9.2677177  2.00073251 2.79719773 2.4049019  2.21370077 9.72603942\n",
      " 4.15500607 3.1920225 ] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[7.90470929 6.3330749  8.19267455 4.62380641 8.59141207 7.24683375\n",
      " 9.46541789 6.35831765] [45409.78745304] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[86880.6302619] [7.47550623 2.32289674 9.11548403 4.53228214 8.5487103  6.62375826\n",
      " 9.59073042 6.37102624] tensor([[0.6005]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 16 . [7.5]\n",
      "[8.00038394 8.01040703 8.39539127 3.58537272 3.25838512 8.3279503\n",
      " 8.18537281 9.91936961] [37732.93436897] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[37743.24918267] [7.99988171 8.00674339 8.39670127 3.58537685 3.25834021 8.32717347\n",
      " 8.18543047 9.91866053] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more feasible directions at iteration 1609 . [22717.25]\n",
      "[2.77580922 3.72582298 2.45069058 8.23582683 9.28100114 6.83650414\n",
      " 4.17394074 6.97840703] [52127.04738712] tensor([[0.9998]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[73082.50716534] [2.64402005 2.00016076 3.00289291 8.16919477 9.25402261 6.65222449\n",
      " 4.24539455 6.98094141] tensor([[0.9925]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 1 . [0.75]\n",
      "[5.25904766 8.20173937 7.41872267 8.21652658 2.64033946 8.93704144\n",
      " 9.65310424 5.90079695] [37464.39119935] tensor([[0.9999]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[37464.920639] [5.25903557 8.20153937 7.41882135 8.21648967 2.6403218  8.937045\n",
      " 9.65308208 5.90076732] tensor([[0.9999]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 2164 . [33209.25]\n",
      "[5.5656446  4.48493532 9.23656697 7.38481998 7.2770088  6.54463189\n",
      " 2.93160599 9.79820983] [60089.75505921] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[91561.99560445] [5.32070827 2.0000599  9.70568487 7.31456898 7.26325397 6.07687465\n",
      " 3.10665025 9.76508986] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 428 . [9384.25]\n",
      "[2.32452262 2.58871554 9.32749721 9.99315489 4.52452962 7.02169385\n",
      " 4.2840132  2.97129558] [84851.00334216] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[93489.86238849] [2.26984971 2.00068021 9.43794247 9.96615327 4.51080201 6.94009985\n",
      " 4.31466145 2.97321733] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 718 . [12637.75]\n",
      "[6.06170208 2.87496314 6.90281982 2.70510471 7.12808567 6.70217093\n",
      " 3.84531715 3.59228077] [72612.93674228] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[84253.71984371] [5.96945847 2.000442   7.10300807 2.69966675 7.10835249 6.56853958\n",
      " 3.89729027 3.59546889] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 1609 . [22717.25]\n",
      "[2.77580922 3.72582298 2.45069058 8.23582683 9.28100114 6.83650414\n",
      " 4.17394074 6.97840703] [52127.04738712] tensor([[0.9998]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[73082.50716534] [2.64402005 2.00016076 3.00289291 8.16919477 9.25402261 6.65222449\n",
      " 4.24539455 6.98094141] tensor([[0.9925]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "Non greedy step\n",
      "[6.46193032 2.46136168 6.21853628 5.25788148 7.9013281  4.56285222\n",
      " 9.38669684 4.9114767 ] [80845.15288559] tensor([[0.7314]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[83538.87003492] [6.44496965 2.27340858 6.26817427 5.25339434 7.89982779 4.53706082\n",
      " 9.39018001 4.91383468] tensor([[0.5994]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 1 . [0.75]\n",
      "[5.25904766 8.20173937 7.41872267 8.21652658 2.64033946 8.93704144\n",
      " 9.65310424 5.90079695] [37464.39119935] tensor([[0.9999]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[37464.920639] [5.25903557 8.20153937 7.41882135 8.21648967 2.6403218  8.937045\n",
      " 9.65308208 5.90076732] tensor([[0.9999]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 2297 . [30576.75]\n",
      "[6.80300303 4.44188674 5.09232451 4.49702205 8.23280156 6.85857552\n",
      " 8.44098143 8.16072244] [52979.29588261] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[82029.50335483] [6.57534481 2.00066047 5.76917246 4.45804166 8.21705594 6.56551801\n",
      " 8.51878196 8.15653237] tensor([[0.9988]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 1 . [0.25]\n",
      "[2.41785327 8.26680725 4.55367652 4.91451848 5.77937036 9.39042502\n",
      " 2.42309088 4.50039664] [35336.2246027] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[35336.42206806] [2.41784573 8.26674864 4.55379519 4.91450455 5.77934201 9.39042384\n",
      " 2.42310625 4.50036603] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 0 . [0.]\n",
      "[2.10299112 9.24281642 8.95995349 3.61650842 8.46059085 9.63354476\n",
      " 5.77238648 9.81101014] [37160.47833932] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[37160.47833932] [2.10299112 9.24281642 8.95995349 3.61650842 8.46059085 9.63354476\n",
      " 5.77238648 9.81101014] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 2 . [0.75]\n",
      "[6.39951632 9.79564212 7.35208721 9.51407639 6.46764197 8.22852305\n",
      " 8.03919005 9.79319301] [35109.67271484] tensor([[0.9984]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[35110.07960205] [6.39956085 9.79587929 7.35218504 9.51400426 6.46763945 8.22851314\n",
      " 8.03913542 9.79312039] tensor([[0.9984]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 10 . [6.25]\n",
      "[3.73422297 7.87510607 2.50984863 7.37213942 8.75389144 7.97735359\n",
      " 7.53928296 8.55897645] [30111.24457931] tensor([[0.9986]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[30117.36047027] [3.73427509 7.87368631 2.51181988 7.37183818 8.75383894 7.97750084\n",
      " 7.53905515 8.55876594] tensor([[0.9986]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 1 . [1.]\n",
      "[4.35731951 7.89332195 6.7374508  3.37760199 5.12331635 9.62994404\n",
      " 6.41174055 5.39699171] [38477.73804802] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[38478.29457663] [4.35728704 7.89311737 6.73755603 3.37759381 5.12329507 9.62992971\n",
      " 6.41175242 5.39696343] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 1106 . [16311.25]\n",
      "[9.22545751 3.26607377 8.6440655  9.16676316 4.04581057 9.74295102\n",
      " 4.85377851 8.01883358] [64219.93497809] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[79548.62722134] [9.10610376 2.00103607 8.84264271 9.13726277 4.01321004 9.58713271\n",
      " 4.94305313 7.99871041] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 3407 . [31065.]\n",
      "[7.59534288 4.98718601 2.62978363 5.69085127 8.31056625 9.62661298\n",
      " 9.06865306 9.8931685 ] [40538.78925346] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[70632.07733244] [7.34101642 2.00074153 3.56696159 5.64789319 8.26856374 9.48936966\n",
      " 9.17396804 9.8550342 ] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 0 . [0.]\n",
      "[9.46572723 8.31048572 9.53400387 8.08596509 8.53904379 8.14953446\n",
      " 2.57197839 2.32150929] [37446.26714939] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[37446.26714939] [9.46572723 8.31048572 9.53400387 8.08596509 8.53904379 8.14953446\n",
      " 2.57197839 2.32150929] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 777 . [11845.]\n",
      "[7.64918917 2.89758583 7.32706996 4.79547298 5.36945525 9.89016881\n",
      " 4.4248102  5.38032035] [67514.34161202] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[78582.22601178] [7.55430584 2.00024405 7.49528028 4.78914897 5.34190248 9.78799158\n",
      " 4.49135415 5.37340151] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 922 . [14440.5]\n",
      "[6.70066522 3.04066453 3.44555526 2.19836695 3.57994109 6.26423593\n",
      " 4.54553226 7.60479519] [61452.49349498] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[74508.06704689] [6.60702917 2.00113415 3.7507643  2.20379967 3.56503714 6.14099177\n",
      " 4.59083658 7.5856336 ] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 222 . [4076.75]\n",
      "[4.39264145 2.27191115 2.46288536 3.14119009 3.8611725  6.42414569\n",
      " 6.1764177  6.65003673] [69413.7981996] tensor([[0.9999]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[73097.48452672] [4.36941485 2.00078087 2.54666661 3.13913205 3.85731167 6.39994049\n",
      " 6.18503193 6.64724503] tensor([[0.9998]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 5290 . [45238.25]\n",
      "[5.51311011 6.26822205 3.42095021 9.23743387 7.05662242 6.82980833\n",
      " 8.57853598 3.86830712] [36603.64908281] tensor([[0.9748]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[79018.35370117] [5.22519568 2.00049955 4.89606377 8.99509888 6.9358162  6.45821482\n",
      " 8.56143747 3.90119981] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 428 . [9384.25]\n",
      "[2.32452262 2.58871554 9.32749721 9.99315489 4.52452962 7.02169385\n",
      " 4.2840132  2.97129558] [84851.00334216] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[93489.86238849] [2.26984971 2.00068021 9.43794247 9.96615327 4.51080201 6.94009985\n",
      " 4.31466145 2.97321733] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more feasible directions at iteration 313 . [329.5]\n",
      "[3.35861851 8.00191927 9.67182006 7.29808488 9.83597288 6.28172953\n",
      " 2.87220783 4.36908157] [41710.29764203] tensor([[0.9999]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[42094.86808804] [3.34668313 7.90735773 9.70084855 7.28561002 9.83147491 6.23858798\n",
      " 2.88020454 4.36691632] tensor([[0.9999]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 3847 . [37961.]\n",
      "[4.98997542 5.41873414 3.61500436 6.05701488 8.23160727 7.29218542\n",
      " 3.97855472 9.33913974] [41508.4830794] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[77242.42510285] [4.70086684 2.00112981 4.68130943 5.98009335 8.1953207  6.84713329\n",
      " 4.16203674 9.2810231 ] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "Non greedy step\n",
      "[8.76254164 2.40324077 8.77518014 3.82322874 4.85104019 4.60577037\n",
      " 8.11653639 8.55373399] [86914.966021] tensor([[0.0250]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[86934.27860038] [8.76241116 2.401906   8.77546641 3.82321632 4.85103831 4.60555128\n",
      " 8.11658387 8.55372594] tensor([[0.0251]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 1786 . [22664.75]\n",
      "[7.85420766 3.86778237 6.71424169 4.16686453 5.8950817  9.45835324\n",
      " 4.84018509 3.60578086] [57448.06776166] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[78611.07588065] [7.65529651 2.00108131 7.10204035 4.15650691 5.82694939 9.24321013\n",
      " 4.96570831 3.60085514] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 673 . [13343.75]\n",
      "[3.41457626 2.88066793 9.95030403 6.65990444 5.80259326 7.74979651\n",
      " 2.11154534 2.27863421] [79292.20127463] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[91634.27842607] [ 3.32111588  2.00122664 10.09947702  6.63399798  5.77761355  7.610062\n",
      "  2.17564988  2.28127251] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[2.22830359 9.13134253 2.92830428 2.84712976 4.10700216 7.92714511\n",
      " 6.64395367 2.77072904] [33516.19616527] tensor([[1.7589e-08]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[33516.19616527] [2.22830359 9.13134253 2.92830428 2.84712976 4.10700216 7.92714511\n",
      " 6.64395367 2.77072904] tensor([[1.7589e-08]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 6091 . [41470.75]\n",
      "[7.73649831 6.41277343 6.98165062 9.38499875 7.25833299 9.01180244\n",
      " 2.22250342 2.63210608] [39947.30209288] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[78798.57280306] [7.33407762 2.00050273 7.90596717 9.19422597 7.00090705 8.29102937\n",
      " 2.56119967 2.62473276] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 2258 . [31125.75]\n",
      "[7.10731359 4.39252568 6.82182993 3.48153784 3.91557955 5.82419792\n",
      " 2.60837853 4.97694776] [57826.12845189] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[86173.72581708] [6.87134986 2.00072168 7.39087261 3.47257739 3.86256    5.40420134\n",
      " 2.74380108 4.94487298] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 622 . [8816.75]\n",
      "[9.49851485 2.66765712 2.51020965 4.38218088 4.97826409 7.07132962\n",
      " 5.61656206 6.3882133 ] [59438.08887181] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[67390.51896883] [9.44390576 2.00029418 2.7111495  4.38386216 4.96241146 7.00514648\n",
      " 5.64090224 6.38223437] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 3972 . [33281.]\n",
      "[7.95462855 5.20594564 2.04653061 7.76957186 8.74530383 7.54793829\n",
      " 5.16137886 7.01085736] [37898.16037398] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[68797.90852483] [7.73132528 2.00058436 3.16339209 7.68962515 8.66086158 7.21704695\n",
      " 5.27232236 6.99697346] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 5290 . [45238.25]\n",
      "[5.51311011 6.26822205 3.42095021 9.23743387 7.05662242 6.82980833\n",
      " 8.57853598 3.86830712] [36603.64908281] tensor([[0.9748]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[79018.35370117] [5.22519568 2.00049955 4.89606377 8.99509888 6.9358162  6.45821482\n",
      " 8.56143747 3.90119981] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 264 . [5940.25]\n",
      "[3.62956963 2.36034051 4.58837303 5.59544987 7.97397502 3.143072\n",
      " 7.47271901 8.66850752] [81705.44905655] tensor([[0.9990]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[87177.23764046] [3.59980298 2.00066015 4.69477694 5.58574427 7.97552457 3.09130499\n",
      " 7.48011657 8.66943408] tensor([[0.9992]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 2043 . [29207.75]\n",
      "[3.071609   4.24563298 5.1245792  7.19735674 6.06970858 7.5685513\n",
      " 4.64635842 9.32825589] [54983.59460925] tensor([[0.9990]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[82330.37288984] [2.87615919 2.00016448 5.71248047 7.12618587 6.048156   7.30955181\n",
      " 4.76427108 9.29211076] tensor([[0.8395]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 6488 . [41029.75]\n",
      "[8.47545423 6.57429383 4.68323216 6.30297731 6.40871524 9.50203937\n",
      " 8.31188424 7.28854553] [37353.47959419] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[77279.34207306] [8.06664944 2.00102964 5.98952778 6.20822312 6.2701292  9.18836584\n",
      " 8.47049823 7.20266125] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 7 . [4.]\n",
      "[6.31474184 7.9625008  9.62109694 5.82377057 6.71586524 9.89056473\n",
      " 6.83650778 5.31607151] [39084.78402322] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[39089.21078237] [6.31446368 7.9608543  9.62140807 5.82362696 6.71570891 9.89020991\n",
      " 6.83666022 5.3159332 ] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 1774 . [26372.25]\n",
      "[5.79794783 3.9627206  4.23564584 6.31557635 7.74409975 5.68866027\n",
      " 8.41799693 3.25907859] [56200.80982097] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[80570.69775226] [5.63419136 2.00108965 4.82615284 6.25563831 7.70686963 5.46341584\n",
      " 8.44402702 3.28687887] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 2203 . [33001.5]\n",
      "[7.55770164 4.50361243 8.21981413 9.34081355 7.3479328  6.0781645\n",
      " 8.63311071 5.71729505] [58989.89153337] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[90191.15753877] [7.33931517 2.00064856 8.76708636 9.23531573 7.3158755  5.70402524\n",
      " 8.69846952 5.7366544 ] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[3.66606175 6.9471738  5.86707696 7.66763302 6.5814646  7.81747032\n",
      " 9.11547434 4.66208075] [39549.04433874] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[86312.33541468] [3.24749743 2.20315766 7.24265915 7.39910167 6.47411279 7.37337083\n",
      " 9.17708912 4.66008864] tensor([[0.6008]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "Non greedy step\n",
      "[2.60843046 3.34646482 5.20140329 9.16694417 6.19210809 3.8501873\n",
      " 7.96071749 8.58640565] [69784.03388672] tensor([[0.0031]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[69799.31002231] [2.60834589 3.34529544 5.20175683 9.16688611 6.19211113 3.85003121\n",
      " 7.96072856 8.58640151] tensor([[0.0031]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[3.42529972 3.12091281 6.22450424 3.86772238 2.05499433 5.28858019\n",
      " 7.22884385 3.77197677] [75638.697986] tensor([[0.8266]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[78877.55572625] [3.40277027 2.87659861 6.28976789 3.8625242  2.05109853 5.25897233\n",
      " 7.23456284 3.77045961] tensor([[0.6010]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 0 . [0.]\n",
      "[9.46572723 8.31048572 9.53400387 8.08596509 8.53904379 8.14953446\n",
      " 2.57197839 2.32150929] [37446.26714939] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[37446.26714939] [9.46572723 8.31048572 9.53400387 8.08596509 8.53904379 8.14953446\n",
      " 2.57197839 2.32150929] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more feasible directions at iteration 622 . [8816.75]\n",
      "[9.49851485 2.66765712 2.51020965 4.38218088 4.97826409 7.07132962\n",
      " 5.61656206 6.3882133 ] [59438.08887181] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[67390.51896883] [9.44390576 2.00029418 2.7111495  4.38386216 4.96241146 7.00514648\n",
      " 5.64090224 6.38223437] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 1 . [0.25]\n",
      "[2.41785327 8.26680725 4.55367652 4.91451848 5.77937036 9.39042502\n",
      " 2.42309088 4.50039664] [35336.2246027] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[35336.42206806] [2.41784573 8.26674864 4.55379519 4.91450455 5.77934201 9.39042384\n",
      " 2.42310625 4.50036603] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[2.27568762 5.53307114 2.17336102 5.82804661 9.9896783  9.04718562\n",
      " 9.71145997 9.88106918] [38480.9356788] tensor([[0.9998]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[61984.45284162] [2.07206272 3.11813627 3.06780976 5.73164956 9.98141757 8.97233976\n",
      " 9.76553781 9.86128079] tensor([[0.6000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 524 . [8999.5]\n",
      "[7.80140724 2.62842748 5.54328779 9.13336153 7.01419669 5.73058897\n",
      " 4.2462284  6.93044481] [70484.34193358] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[78706.2056262] [7.75168239 2.00092855 5.69918419 9.11544853 7.00417124 5.63718285\n",
      " 4.27328562 6.9301966 ] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "Non greedy step\n",
      "[2.22830359 9.13134253 2.92830428 2.84712976 4.10700216 7.92714511\n",
      " 6.64395367 2.77072904] [33516.19616527] tensor([[1.7589e-08]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[33516.56357085] [2.22831475 9.13141683 2.92846693 2.84712027 4.10697924 7.92718886\n",
      " 6.64391663 2.77070186] tensor([[1.7587e-08]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 2399 . [35616.25]\n",
      "[6.33425839 4.68945772 8.33101047 5.04150204 3.63316753 6.47734776\n",
      " 7.13844727 3.96118992] [59877.30834563] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[93023.1305609] [6.06695884 2.00009822 8.91190741 4.98230689 3.57928796 6.08887144\n",
      " 7.23495599 3.95083213] tensor([[0.9999]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 854 . [13043.5]\n",
      "[8.73286359 2.99412554 8.71650172 5.39791854 9.41658602 9.58566976\n",
      " 4.35316024 4.88773315] [66809.49137333] tensor([[1.]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[79189.08344193] [8.62236172 2.00068642 8.88162577 5.38681757 9.38780944 9.44122186\n",
      " 4.43317112 4.89404136] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 4375 . [42227.25]\n",
      "[2.59348001 5.82705433 3.91710052 4.40979897 7.12855959 7.86119938\n",
      " 2.97554615 9.19828572] [40635.06974869] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[80500.91565927] [2.23302238 2.00109383 5.09579209 4.3371274  7.08399086 7.38560991\n",
      " 3.22997022 9.09927453] tensor([[0.9999]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 5207 . [40465.75]\n",
      "[7.63168609 5.96373284 2.15863472 3.58381163 6.85386144 7.45234988\n",
      " 8.66903784 3.46879002] [35588.95338002] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[73309.03429523] [7.29416653 2.00029515 3.65643495 3.55291147 6.7191173  7.17461302\n",
      " 8.68391253 3.48188815] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 1497 . [21464.5]\n",
      "[8.60608982 3.68830836 7.21414797 6.69336848 4.97587656 9.16413346\n",
      " 8.72876187 7.56794622] [61238.15230665] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[81611.14458295] [8.44480216 2.0004751  7.55766568 6.66110664 4.94377909 8.99970533\n",
      " 8.80486524 7.55359697] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[7.90470929 6.3330749  8.19267455 4.62380641 8.59141207 7.24683375\n",
      " 9.46541789 6.35831765] [45409.78745304] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[86880.6302619] [7.47550623 2.32289674 9.11548403 4.53228214 8.5487103  6.62375826\n",
      " 9.59073042 6.37102624] tensor([[0.6005]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 333 . [6008.25]\n",
      "[6.08523087 2.4149837  5.01563924 3.35185074 9.76801024 7.05517285\n",
      " 6.78332714 7.67508825] [73265.933092] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[78909.13385198] [6.04445347 2.00096217 5.12392818 3.34797798 9.76430363 7.00414662\n",
      " 6.80322651 7.67678293] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 0 . [0.]\n",
      "[2.10299112 9.24281642 8.95995349 3.61650842 8.46059085 9.63354476\n",
      " 5.77238648 9.81101014] [37160.47833932] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[37160.47833932] [2.10299112 9.24281642 8.95995349 3.61650842 8.46059085 9.63354476\n",
      " 5.77238648 9.81101014] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 222 . [4076.75]\n",
      "[4.39264145 2.27191115 2.46288536 3.14119009 3.8611725  6.42414569\n",
      " 6.1764177  6.65003673] [69413.7981996] tensor([[0.9999]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[73097.48452672] [4.36941485 2.00078087 2.54666661 3.13913205 3.85731167 6.39994049\n",
      " 6.18503193 6.64724503] tensor([[0.9998]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 2203 . [33001.5]\n",
      "[7.55770164 4.50361243 8.21981413 9.34081355 7.3479328  6.0781645\n",
      " 8.63311071 5.71729505] [58989.89153337] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[90191.15753877] [7.33931517 2.00064856 8.76708636 9.23531573 7.3158755  5.70402524\n",
      " 8.69846952 5.7366544 ] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 5207 . [40465.75]\n",
      "[7.63168609 5.96373284 2.15863472 3.58381163 6.85386144 7.45234988\n",
      " 8.66903784 3.46879002] [35588.95338002] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[73309.03429523] [7.29416653 2.00029515 3.65643495 3.55291147 6.7191173  7.17461302\n",
      " 8.68391253 3.48188815] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 0 . [0.]\n",
      "[9.83432547 6.94863436 3.83738488 4.29967216 8.70712003 7.72073973\n",
      " 3.80252696 6.15572894] [34755.32172332] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[34755.32172332] [9.83432547 6.94863436 3.83738488 4.29967216 8.70712003 7.72073973\n",
      " 3.80252696 6.15572894] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 4699 . [37489.75]\n",
      "[8.76154627 5.68712093 4.66332725 9.91320942 9.39240355 6.94184719\n",
      " 2.49427115 5.93361662] [40301.2220713] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[75019.62539432] [8.49561274 2.00054561 5.71267516 9.7715582  9.27256337 6.28461158\n",
      " 2.69704137 5.9335562 ] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 1375 . [22830.25]\n",
      "[2.15196124 4.35963291 9.26497618 7.90888302 2.78174951 5.73667108\n",
      " 4.98554116 7.05954161] [67870.91201759] tensor([[0.9996]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[89162.28972006] [2.00001255 2.71890125 9.60677412 7.839691   2.76723088 5.4780076\n",
      " 5.05879261 7.03483445] tensor([[0.9961]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 556 . [9683.25]\n",
      "[7.46514056 2.69036231 9.85009969 4.42559335 6.14373285 9.57614045\n",
      " 2.137895   8.20142437] [72397.90347908] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[81541.82895133] [7.38669754 2.00075799 9.94988377 4.42243181 6.12944449 9.4684385\n",
      " 2.20329761 8.1916134 ] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 1075 . [15188.]\n",
      "[3.45178366 3.16493323 3.21300463 9.45070468 8.7591459  8.33173528\n",
      " 2.20600069 6.42737392] [56510.52006091] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[70465.36227598] [3.35980291 2.00020115 3.53963782 9.40614807 8.72867322 8.21059529\n",
      " 2.27825668 6.42592998] tensor([[0.9999]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more feasible directions at iteration 777 . [11845.]\n",
      "[7.64918917 2.89758583 7.32706996 4.79547298 5.36945525 9.89016881\n",
      " 4.4248102  5.38032035] [67514.34161202] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[78582.22601178] [7.55430584 2.00024405 7.49528028 4.78914897 5.34190248 9.78799158\n",
      " 4.49135415 5.37340151] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 1497 . [21464.5]\n",
      "[8.60608982 3.68830836 7.21414797 6.69336848 4.97587656 9.16413346\n",
      " 8.72876187 7.56794622] [61238.15230665] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[81611.14458295] [8.44480216 2.0004751  7.55766568 6.66110664 4.94377909 8.99970533\n",
      " 8.80486524 7.55359697] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 1106 . [16311.25]\n",
      "[9.22545751 3.26607377 8.6440655  9.16676316 4.04581057 9.74295102\n",
      " 4.85377851 8.01883358] [64219.93497809] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[79548.62722134] [9.10610376 2.00103607 8.84264271 9.13726277 4.01321004 9.58713271\n",
      " 4.94305313 7.99871041] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 4307 . [40181.75]\n",
      "[7.91940831 5.75010687 4.99891093 8.70989058 9.0463968  6.72622391\n",
      " 6.36021882 9.53906911] [41795.74468662] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[80196.6212996] [7.63089449 2.00126525 6.07237853 8.57477028 9.02016847 6.16903961\n",
      " 6.49094386 9.50953263] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 1534 . [21974.]\n",
      "[6.7654802  3.66965415 2.85853634 3.91202373 9.98809675 6.17321435\n",
      " 9.28874301 5.78085487] [53814.6229849] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[74398.85771367] [6.61921564 2.0007284  3.40247689 3.88766228 9.96988965 6.00103013\n",
      " 9.31614426 5.80283129] tensor([[0.9838]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 0 . [0.]\n",
      "[7.96228433 8.94982755 8.14662728 4.20372935 6.1152242  7.86171944\n",
      " 6.43489365 6.10629442] [37796.29800406] tensor([[0.9998]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[37796.29800406] [7.96228433 8.94982755 8.14662728 4.20372935 6.1152242  7.86171944\n",
      " 6.43489365 6.10629442] tensor([[0.9998]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 698 . [14525.]\n",
      "[2.18813213 2.9292535  9.03466641 7.78791587 3.77742236 6.61714789\n",
      " 2.36987329 4.4287817 ] [80686.10531804] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[94011.91415589] [2.09848836 2.00012279 9.21423841 7.75525557 3.75848111 6.47457715\n",
      " 2.42698416 4.42345814] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 0 . [0.]\n",
      "[9.37117828 8.91561452 9.63227616 3.66125118 9.98231288 9.72152193\n",
      " 9.9229497  7.03697657] [36585.89983906] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[36585.89983906] [9.37117828 8.91561452 9.63227616 3.66125118 9.98231288 9.72152193\n",
      " 9.9229497  7.03697657] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 2164 . [33209.25]\n",
      "[5.5656446  4.48493532 9.23656697 7.38481998 7.2770088  6.54463189\n",
      " 2.93160599 9.79820983] [60089.75505921] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[91561.99560445] [5.32070827 2.0000599  9.70568487 7.31456898 7.26325397 6.07687465\n",
      " 3.10665025 9.76508986] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 1944 . [30476.]\n",
      "[2.59143951 4.29600963 9.92058926 7.27019206 7.07648428 9.88620264\n",
      " 5.32217423 2.8871521 ] [63190.0812215] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[92306.99227772] [ 2.33737395  2.00068581 10.29433763  7.17861473  7.00776187  9.60257933\n",
      "  5.48307754  2.89948921] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 3407 . [31065.]\n",
      "[7.59534288 4.98718601 2.62978363 5.69085127 8.31056625 9.62661298\n",
      " 9.06865306 9.8931685 ] [40538.78925346] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[70632.07733244] [7.34101642 2.00074153 3.56696159 5.64789319 8.26856374 9.48936966\n",
      " 9.17396804 9.8550342 ] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 1847 . [25869.25]\n",
      "[9.25067508 3.96535082 6.35253056 7.82715472 7.14608988 4.61303537\n",
      " 3.0952779  3.68570653] [59829.49759734] tensor([[0.9920]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[83256.39937218] [9.08803392 2.00101598 6.84351464 7.78107063 7.09790261 4.23444207\n",
      " 3.17587189 3.69832145] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "Non greedy step\n",
      "[3.25988679 4.16643163 6.11047137 9.05088628 3.3246417  7.48755927\n",
      " 9.4141829  8.73158176] [59290.48691094] tensor([[0.9619]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[73702.15318714] [3.1633711  2.96186213 6.42016273 8.99397292 3.31540776 7.38734308\n",
      " 9.43916055 8.71286123] tensor([[0.5999]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 2547 . [22923.5]\n",
      "[9.4666134  4.14682708 2.14685041 2.35237228 2.33405622 9.83755059\n",
      " 4.03148476 3.2407511 ] [45221.4271827] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[65711.95675028] [9.2677177  2.00073251 2.79719773 2.4049019  2.21370077 9.72603942\n",
      " 4.15500607 3.1920225 ] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "Non greedy step\n",
      "[3.07421164 7.20075127 6.37380909 4.27023187 5.28944986 7.86059427\n",
      " 6.30914519 7.51564167] [40314.0375437] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[81859.87519325] [2.60858927 2.70348073 7.64957189 4.13527244 5.22983368 7.28794254\n",
      " 6.50909986 7.38544351] tensor([[0.5993]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 0 . [0.]\n",
      "[9.46572723 8.31048572 9.53400387 8.08596509 8.53904379 8.14953446\n",
      " 2.57197839 2.32150929] [37446.26714939] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[37446.26714939] [9.46572723 8.31048572 9.53400387 8.08596509 8.53904379 8.14953446\n",
      " 2.57197839 2.32150929] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 419 . [8068.]\n",
      "[7.80505753 2.54229604 8.5460933  4.07310714 7.07032581 7.27334893\n",
      " 7.69894332 2.86202971] [79805.71244505] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[87340.77467672] [7.74767854 2.00003303 8.65443813 4.06560846 7.05831885 7.19871346\n",
      " 7.72392431 2.86805569] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 687 . [11591.75]\n",
      "[4.48406123 2.84138945 6.15259486 2.40760362 7.76871409 9.821738\n",
      " 6.55722958 9.38768387] [70051.23631958] tensor([[0.9997]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[81143.11752599] [4.39229423 2.00059311 6.34067012 2.40285152 7.75945774 9.7406191\n",
      " 6.61471416 9.37935542] tensor([[0.9725]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "Non greedy step\n",
      "[8.620305   6.52387304 3.88667836 4.19059542 8.64203901 6.0080067\n",
      " 6.58945111 9.07213468] [37019.94436146] tensor([[0.3645]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[37021.88304141] [8.62027855 6.52349594 3.88689023 4.19059528 8.6420374  6.0079312\n",
      " 6.58944777 9.07211602] tensor([[0.3656]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 47 . [48.25]\n",
      "[4.03335766 7.63764846 6.75125065 4.09463841 9.23440151 6.63295844\n",
      " 4.6558113  5.67161037] [39804.52018197] tensor([[0.9999]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[39858.91171564] [4.03170027 7.62408533 6.75825804 4.09383023 9.23393143 6.62868805\n",
      " 4.65639029 5.67100841] tensor([[0.9999]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 3 . [2.75]\n",
      "[8.14316892 7.45950784 8.23798354 9.26559455 2.14234778 9.39906092\n",
      " 7.67183702 4.93955909] [39688.59887872] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[39691.13278583] [8.14310967 7.45868092 8.23819375 9.26550182 2.14225971 9.39898967\n",
      " 7.67183257 4.93947446] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more feasible directions at iteration 2043 . [29207.75]\n",
      "[3.071609   4.24563298 5.1245792  7.19735674 6.06970858 7.5685513\n",
      " 4.64635842 9.32825589] [54983.59460925] tensor([[0.9990]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[82330.37288984] [2.87615919 2.00016448 5.71248047 7.12618587 6.048156   7.30955181\n",
      " 4.76427108 9.29211076] tensor([[0.8395]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[4.20966833 4.82755058 8.33112372 6.05386772 6.45425962 7.42645223\n",
      " 9.66165679 9.32013025] [58803.94264895] tensor([[0.9999]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[73073.45767184] [4.0835274  3.55766018 8.62038701 6.00814834 6.4537722  7.26956812\n",
      " 9.70461783 9.3059548 ] tensor([[0.6007]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 2459 . [30459.5]\n",
      "[7.86588373 4.44265088 3.64158435 8.98533422 5.7447965  5.47726131\n",
      " 7.45525759 3.07284886] [48959.30667726] tensor([[0.9994]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[76587.56290004] [7.69991863 2.00103508 4.40672603 8.89323023 5.67831691 5.19430756\n",
      " 7.46447211 3.09431477] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 4167 . [43346.5]\n",
      "[4.14827727 5.9201598  7.34836891 9.24578232 4.57740199 9.50791698\n",
      " 7.98383891 4.3721734 ] [46323.79039916] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[87794.6055046] [3.78893587 2.00105512 8.19171818 9.0370904  4.45267537 9.1877288\n",
      " 8.13430574 4.34917316] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 3972 . [33281.]\n",
      "[7.95462855 5.20594564 2.04653061 7.76957186 8.74530383 7.54793829\n",
      " 5.16137886 7.01085736] [37898.16037398] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[68797.90852483] [7.73132528 2.00058436 3.16339209 7.68962515 8.66086158 7.21704695\n",
      " 5.27232236 6.99697346] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 72 . [1377.25]\n",
      "[3.44278251 2.09204241 3.67034471 5.5405317  3.84293424 8.57451087\n",
      " 9.26924851 2.9502659 ] [75897.59880934] tensor([[0.9998]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[77160.32257784] [3.43472991 2.00065459 3.69555281 5.5381986  3.84053948 8.56980531\n",
      " 9.27151917 2.95056265] tensor([[0.9997]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 1774 . [26372.25]\n",
      "[5.79794783 3.9627206  4.23564584 6.31557635 7.74409975 5.68866027\n",
      " 8.41799693 3.25907859] [56200.80982097] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[80570.69775226] [5.63419136 2.00108965 4.82615284 6.25563831 7.70686963 5.46341584\n",
      " 8.44402702 3.28687887] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "Non greedy step\n",
      "[6.46193032 2.46136168 6.21853628 5.25788148 7.9013281  4.56285222\n",
      " 9.38669684 4.9114767 ] [80845.15288559] tensor([[0.7314]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[83538.87003492] [6.44496965 2.27340858 6.26817427 5.25339434 7.89982779 4.53706082\n",
      " 9.39018001 4.91383468] tensor([[0.5994]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 0 . [0.]\n",
      "[4.24420862 9.87886603 9.85150662 9.96195105 6.38345437 6.98902191\n",
      " 3.50112154 8.66333995] [37018.52176964] tensor([[0.9999]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[37018.52176964] [4.24420862 9.87886603 9.85150662 9.96195105 6.38345437 6.98902191\n",
      " 3.50112154 8.66333995] tensor([[0.9999]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 60 . [57.]\n",
      "[4.34722906 7.37824988 2.26491685 9.36257865 8.89383302 6.9966811\n",
      " 6.60915278 8.03253647] [30127.7449043] tensor([[0.9990]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[30186.70535665] [4.34767228 7.36500833 2.27792613 9.36022214 8.89338584 6.99610185\n",
      " 6.60775246 8.03158032] tensor([[0.9991]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 255 . [4089.75]\n",
      "[4.42431165 2.29500017 2.62214858 6.27300698 7.97761983 8.26637821\n",
      " 4.20514328 5.36314825] [65466.68367518] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[69210.08099313] [4.39917574 2.00089508 2.70764406 6.26692085 7.97004536 8.24059708\n",
      " 4.2202927  5.36367009] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 2 . [1.25]\n",
      "[2.23961504 8.3725092  5.76464559 8.49609801 5.52093922 8.51519405\n",
      " 6.79952069 5.19043906] [35593.61830851] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[35594.41886232] [2.23960774 8.37222468 5.76490437 8.49600976 5.52090212 8.51519529\n",
      " 6.79949111 5.19039338] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 1375 . [22830.25]\n",
      "[2.15196124 4.35963291 9.26497618 7.90888302 2.78174951 5.73667108\n",
      " 4.98554116 7.05954161] [67870.91201759] tensor([[0.9996]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[89162.28972006] [2.00001255 2.71890125 9.60677412 7.839691   2.76723088 5.4780076\n",
      " 5.05879261 7.03483445] tensor([[0.9961]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 2164 . [33209.25]\n",
      "[5.5656446  4.48493532 9.23656697 7.38481998 7.2770088  6.54463189\n",
      " 2.93160599 9.79820983] [60089.75505921] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[91561.99560445] [5.32070827 2.0000599  9.70568487 7.31456898 7.26325397 6.07687465\n",
      " 3.10665025 9.76508986] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 2297 . [30576.75]\n",
      "[6.80300303 4.44188674 5.09232451 4.49702205 8.23280156 6.85857552\n",
      " 8.44098143 8.16072244] [52979.29588261] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[82029.50335483] [6.57534481 2.00066047 5.76917246 4.45804166 8.21705594 6.56551801\n",
      " 8.51878196 8.15653237] tensor([[0.9988]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 353 . [6423.75]\n",
      "[7.09430523 2.42307718 2.93407938 6.63712449 4.52669832 3.66444724\n",
      " 4.90107397 5.32715955] [69657.84545424] tensor([[0.9994]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[75356.81745315] [7.06400488 2.00010621 3.06715411 6.6291029  4.52040409 3.60631867\n",
      " 4.90890975 5.32635285] tensor([[0.9997]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 3847 . [37961.]\n",
      "[4.98997542 5.41873414 3.61500436 6.05701488 8.23160727 7.29218542\n",
      " 3.97855472 9.33913974] [41508.4830794] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[77242.42510285] [4.70086684 2.00112981 4.68130943 5.98009335 8.1953207  6.84713329\n",
      " 4.16203674 9.2810231 ] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "Non greedy step\n",
      "[2.42158645 5.36692403 4.19314939 9.88923386 3.3281671  7.72567782\n",
      " 2.5713273  4.58021405] [44349.90385502] tensor([[0.9881]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[73559.59632856] [2.21468903 2.46706406 5.04316195 9.73416932 3.22180092 7.43148973\n",
      " 2.70401317 4.52378761] tensor([[0.5997]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 1847 . [25869.25]\n",
      "[9.25067508 3.96535082 6.35253056 7.82715472 7.14608988 4.61303537\n",
      " 3.0952779  3.68570653] [59829.49759734] tensor([[0.9920]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[83256.39937218] [9.08803392 2.00101598 6.84351464 7.78107063 7.09790261 4.23444207\n",
      " 3.17587189 3.69832145] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 524 . [8999.5]\n",
      "[7.80140724 2.62842748 5.54328779 9.13336153 7.01419669 5.73058897\n",
      " 4.2462284  6.93044481] [70484.34193358] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[78706.2056262] [7.75168239 2.00092855 5.69918419 9.11544853 7.00417124 5.63718285\n",
      " 4.27328562 6.9301966 ] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 2281 . [27040.25]\n",
      "[5.85864825 4.2843192  6.14918803 6.69545209 5.19060212 9.4347719\n",
      " 3.21813603 3.40114842] [53668.64926348] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[78693.21909327] [5.63663181 2.00038791 6.64782055 6.64450937 5.0967958  9.1820932\n",
      " 3.37650546 3.3864251 ] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more feasible directions at iteration 659 . [11627.5]\n",
      "[7.69025148 2.81017694 7.48039875 7.10897734 7.33196814 6.50889107\n",
      " 4.53468652 6.53750297] [73055.49401906] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[83839.93886477] [7.61472645 2.00051189 7.65279317 7.09161085 7.31891098 6.3814551\n",
      " 4.57885283 6.5377389 ] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 4719 . [19304.]\n",
      "[2.11829325 6.82476744 2.0357269  9.60730567 3.60323836 8.49364242\n",
      " 6.92712904 3.5485901 ] [32835.25559196] tensor([[0.9998]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[50815.44729406] [2.0000387  4.27432686 3.21081951 9.37976132 3.45460939 8.53811302\n",
      " 6.88542586 3.48032127] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 2 . [0.75]\n",
      "[4.91825354 8.84363622 8.66625149 2.02358779 9.74090145 9.86560545\n",
      " 7.37689289 9.83481264] [36816.27657038] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[36816.58862079] [4.9181799  8.84345315 8.66636022 2.02358389 9.74090189 9.86552745\n",
      " 7.37693801 9.8347534 ] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 1534 . [21974.]\n",
      "[6.7654802  3.66965415 2.85853634 3.91202373 9.98809675 6.17321435\n",
      " 9.28874301 5.78085487] [53814.6229849] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[74398.85771367] [6.61921564 2.0007284  3.40247689 3.88766228 9.96988965 6.00103013\n",
      " 9.31614426 5.80283129] tensor([[0.9838]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 718 . [12637.75]\n",
      "[6.06170208 2.87496314 6.90281982 2.70510471 7.12808567 6.70217093\n",
      " 3.84531715 3.59228077] [72612.93674228] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[84253.71984371] [5.96945847 2.000442   7.10300807 2.69966675 7.10835249 6.56853958\n",
      " 3.89729027 3.59546889] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 0 . [0.]\n",
      "[9.85340823 9.26563236 7.95923307 3.64831335 9.36963147 7.99682817\n",
      " 9.43597667 6.95061809] [36640.6691726] tensor([[0.9994]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[36640.6691726] [9.85340823 9.26563236 7.95923307 3.64831335 9.36963147 7.99682817\n",
      " 9.43597667 6.95061809] tensor([[0.9994]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 4167 . [43346.5]\n",
      "[4.14827727 5.9201598  7.34836891 9.24578232 4.57740199 9.50791698\n",
      " 7.98383891 4.3721734 ] [46323.79039916] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[87794.6055046] [3.78893587 2.00105512 8.19171818 9.0370904  4.45267537 9.1877288\n",
      " 8.13430574 4.34917316] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 1 . [0.25]\n",
      "[4.97407362 8.85392438 9.0420079  3.11567614 3.9913667  9.06772489\n",
      " 5.74886625 5.50569836] [38902.04264295] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[38902.17653558] [4.97404558 8.85383911 9.04205218 3.1156702  3.99134827 9.06768532\n",
      " 5.7488784  5.50566255] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "Non greedy step\n",
      "Non greedy step\n",
      "Non greedy step\n",
      "[7.90470929 6.3330749  8.19267455 4.62380641 8.59141207 7.24683375\n",
      " 9.46541789 6.35831765] [45409.78745304] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[86939.30578373] [7.47508409 2.31884821 9.11629962 4.53220745 8.54867795 6.62318208\n",
      " 9.59089462 6.37106306] tensor([[0.5939]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 353 . [6423.75]\n",
      "[7.09430523 2.42307718 2.93407938 6.63712449 4.52669832 3.66444724\n",
      " 4.90107397 5.32715955] [69657.84545424] tensor([[0.9994]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[75356.81745315] [7.06400488 2.00010621 3.06715411 6.6291029  4.52040409 3.60631867\n",
      " 4.90890975 5.32635285] tensor([[0.9997]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 0 . [0.]\n",
      "[7.96228433 8.94982755 8.14662728 4.20372935 6.1152242  7.86171944\n",
      " 6.43489365 6.10629442] [37796.29800406] tensor([[0.9998]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[37796.29800406] [7.96228433 8.94982755 8.14662728 4.20372935 6.1152242  7.86171944\n",
      " 6.43489365 6.10629442] tensor([[0.9998]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 3 . [2.75]\n",
      "[8.14316892 7.45950784 8.23798354 9.26559455 2.14234778 9.39906092\n",
      " 7.67183702 4.93955909] [39688.59887872] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[39691.13278583] [8.14310967 7.45868092 8.23819375 9.26550182 2.14225971 9.39898967\n",
      " 7.67183257 4.93947446] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 3407 . [31065.]\n",
      "[7.59534288 4.98718601 2.62978363 5.69085127 8.31056625 9.62661298\n",
      " 9.06865306 9.8931685 ] [40538.78925346] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[70632.07733244] [7.34101642 2.00074153 3.56696159 5.64789319 8.26856374 9.48936966\n",
      " 9.17396804 9.8550342 ] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 1534 . [21974.]\n",
      "[6.7654802  3.66965415 2.85853634 3.91202373 9.98809675 6.17321435\n",
      " 9.28874301 5.78085487] [53814.6229849] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[74398.85771367] [6.61921564 2.0007284  3.40247689 3.88766228 9.96988965 6.00103013\n",
      " 9.31614426 5.80283129] tensor([[0.9838]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 1497 . [21464.5]\n",
      "[8.60608982 3.68830836 7.21414797 6.69336848 4.97587656 9.16413346\n",
      " 8.72876187 7.56794622] [61238.15230665] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[81611.14458295] [8.44480216 2.0004751  7.55766568 6.66110664 4.94377909 8.99970533\n",
      " 8.80486524 7.55359697] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 333 . [6008.25]\n",
      "[6.08523087 2.4149837  5.01563924 3.35185074 9.76801024 7.05517285\n",
      " 6.78332714 7.67508825] [73265.933092] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[78909.13385198] [6.04445347 2.00096217 5.12392818 3.34797798 9.76430363 7.00414662\n",
      " 6.80322651 7.67678293] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 1786 . [22664.75]\n",
      "[7.85420766 3.86778237 6.71424169 4.16686453 5.8950817  9.45835324\n",
      " 4.84018509 3.60578086] [57448.06776166] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[78611.07588065] [7.65529651 2.00108131 7.10204035 4.15650691 5.82694939 9.24321013\n",
      " 4.96570831 3.60085514] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 854 . [13043.5]\n",
      "[8.73286359 2.99412554 8.71650172 5.39791854 9.41658602 9.58566976\n",
      " 4.35316024 4.88773315] [66809.49137333] tensor([[1.]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[79189.08344193] [8.62236172 2.00068642 8.88162577 5.38681757 9.38780944 9.44122186\n",
      " 4.43317112 4.89404136] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 5987 . [46671.75]\n",
      "[4.82380255 6.74118993 8.26776052 5.40919866 6.95306475 9.69790705\n",
      " 2.83948251 5.78381106] [41359.0444182] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[86595.43934852] [4.27333721 2.00117448 9.15966946 5.29052899 6.79038612 8.95536474\n",
      " 3.28297953 5.69482431] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 2164 . [33209.25]\n",
      "[5.5656446  4.48493532 9.23656697 7.38481998 7.2770088  6.54463189\n",
      " 2.93160599 9.79820983] [60089.75505921] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[91561.99560445] [5.32070827 2.0000599  9.70568487 7.31456898 7.26325397 6.07687465\n",
      " 3.10665025 9.76508986] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 7338 . [52533.75]\n",
      "[4.63909766 7.2714982  6.86832483 2.26021714 3.85604358 7.69309272\n",
      " 2.95277771 3.86600877] [41120.59218981] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[90359.62214405] [4.03139843 2.00023094 8.23606663 2.23274222 3.65984527 6.78743037\n",
      " 3.30939011 3.71525932] tensor([[0.9999]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "Non greedy step\n",
      "Non greedy step\n",
      "[3.42529972 3.12091281 6.22450424 3.86772238 2.05499433 5.28858019\n",
      " 7.22884385 3.77197677] [75638.697986] tensor([[0.8266]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[78910.90365201] [3.40254225 2.87413019 6.29042094 3.86247225 2.05105978 5.25867336\n",
      " 7.23462247 3.7704456 ] tensor([[0.5977]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more feasible directions at iteration 3972 . [33281.]\n",
      "[7.95462855 5.20594564 2.04653061 7.76957186 8.74530383 7.54793829\n",
      " 5.16137886 7.01085736] [37898.16037398] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[68797.90852483] [7.73132528 2.00058436 3.16339209 7.68962515 8.66086158 7.21704695\n",
      " 5.27232236 6.99697346] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 4307 . [40181.75]\n",
      "[7.91940831 5.75010687 4.99891093 8.70989058 9.0463968  6.72622391\n",
      " 6.36021882 9.53906911] [41795.74468662] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[80196.6212996] [7.63089449 2.00126525 6.07237853 8.57477028 9.02016847 6.16903961\n",
      " 6.49094386 9.50953263] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 0 . [0.]\n",
      "[6.412598   8.27427726 8.51503918 8.93291554 4.91323368 8.51247272\n",
      " 2.84247125 8.45886467] [36565.23176888] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[36565.23176888] [6.412598   8.27427726 8.51503918 8.93291554 4.91323368 8.51247272\n",
      " 2.84247125 8.45886467] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 2281 . [27040.25]\n",
      "[5.85864825 4.2843192  6.14918803 6.69545209 5.19060212 9.4347719\n",
      " 3.21813603 3.40114842] [53668.64926348] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[78693.21909327] [5.63663181 2.00038791 6.64782055 6.64450937 5.0967958  9.1820932\n",
      " 3.37650546 3.3864251 ] tensor([[1.0000]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "Non greedy step\n",
      "Non greedy step\n",
      "[8.76254164 2.40324077 8.77518014 3.82322874 4.85104019 4.60577037\n",
      " 8.11653639 8.55373399] [86914.966021] tensor([[0.0250]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[86953.59870144] [8.76228065 2.40057096 8.77575272 3.82320391 4.85103642 4.60533216\n",
      " 8.11663137 8.55371788] tensor([[0.0251]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "No more feasible directions at iteration 72 . [1377.25]\n",
      "[3.44278251 2.09204241 3.67034471 5.5405317  3.84293424 8.57451087\n",
      " 9.26924851 2.9502659 ] [75897.59880934] tensor([[0.9998]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "[77160.32257784] [3.43472991 2.00065459 3.69555281 5.5381986  3.84053948 8.56980531\n",
      " 9.27151917 2.95056265] tensor([[0.9997]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "samples = filtered_df.iloc[np.random.choice(len(filtered_df), 197), :8]\n",
    "for i in range(len(samples)):\n",
    "    X0 = np.array(samples.iloc[i])\n",
    "    result = coordinateAscentQuad(classif, ridge, X0, confidence = .6)\n",
    "    print(X0, ridge.predict(poly.fit_transform([X0])), classif(torch.from_numpy(poly.fit_transform([X0]))))\n",
    "    print(ridge.predict(poly.fit_transform([result])), result, classif(torch.from_numpy(poly.fit_transform([result]))))\n",
    "    results.append((X0, result, ridge.predict(poly.fit_transform([result])), ridge.predict(poly.fit_transform([X0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3677fda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(results, key = lambda x: x[2], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7537bebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[87504.62,\n",
       " 86722.18,\n",
       " 85846.05,\n",
       " 85234.52,\n",
       " 83528.12,\n",
       " 83442.09,\n",
       " 83051.34,\n",
       " 82927.4,\n",
       " 82061.53,\n",
       " 81642.69,\n",
       " 81555.23,\n",
       " 81538.36,\n",
       " 80338.0,\n",
       " 79837.12,\n",
       " 79274.52,\n",
       " 78784.51,\n",
       " 78464.02,\n",
       " 78285.78,\n",
       " 78227.84,\n",
       " 77779.3,\n",
       " 77117.16,\n",
       " 76251.66,\n",
       " 75869.35,\n",
       " 74482.89,\n",
       " 74264.98,\n",
       " 74121.92,\n",
       " 74057.05,\n",
       " 73939.99,\n",
       " 73631.16,\n",
       " 73446.17,\n",
       " 73263.34,\n",
       " 72485.31,\n",
       " 71832.66,\n",
       " 71412.88,\n",
       " 70886.62,\n",
       " 70830.75,\n",
       " 68788.69,\n",
       " 68580.09,\n",
       " 68412.84,\n",
       " 67842.77,\n",
       " 67582.01,\n",
       " 67541.22,\n",
       " 65967.9,\n",
       " 65755.58,\n",
       " 65196.87,\n",
       " 64191.2,\n",
       " 63832.05,\n",
       " 63545.63,\n",
       " 63428.37,\n",
       " 62995.52,\n",
       " 62259.07,\n",
       " 61916.03,\n",
       " 61777.55,\n",
       " 60676.16,\n",
       " 60241.04,\n",
       " 59576.45,\n",
       " 59528.58,\n",
       " 59376.38,\n",
       " 59242.72,\n",
       " 59079.92,\n",
       " 58941.78,\n",
       " 58933.1,\n",
       " 58280.82,\n",
       " 58009.89,\n",
       " 57964.61,\n",
       " 57829.19,\n",
       " 57613.95,\n",
       " 57492.59,\n",
       " 57476.48,\n",
       " 57130.39,\n",
       " 56982.9,\n",
       " 56945.66,\n",
       " 56933.59,\n",
       " 56623.36,\n",
       " 56583.86,\n",
       " 56483.61,\n",
       " 56466.14,\n",
       " 55940.92,\n",
       " 55933.92,\n",
       " 55512.09,\n",
       " 55372.77,\n",
       " 55176.56,\n",
       " 55084.65,\n",
       " 54923.63,\n",
       " 54917.07,\n",
       " 54591.75,\n",
       " 54481.37,\n",
       " 54169.0,\n",
       " 54082.5,\n",
       " 53435.33,\n",
       " 53361.03,\n",
       " 53131.57,\n",
       " 53128.57,\n",
       " 52277.82,\n",
       " 52040.86,\n",
       " 51484.24,\n",
       " 51478.34,\n",
       " 51216.0,\n",
       " 50891.45,\n",
       " 50145.55,\n",
       " 50091.75,\n",
       " 50014.9,\n",
       " 49986.26,\n",
       " 49868.83,\n",
       " 49796.77,\n",
       " 49490.61,\n",
       " 49161.92,\n",
       " 48776.66,\n",
       " 48273.97,\n",
       " 48261.59,\n",
       " 48190.71,\n",
       " 48098.7,\n",
       " 46953.95,\n",
       " 46524.35,\n",
       " 46129.22,\n",
       " 46091.66,\n",
       " 45922.62,\n",
       " 45878.44,\n",
       " 45864.03,\n",
       " 45782.82,\n",
       " 45501.43,\n",
       " 44826.27,\n",
       " 44633.94,\n",
       " 43917.88,\n",
       " 43852.49,\n",
       " 43735.25,\n",
       " 43498.45,\n",
       " 43223.52,\n",
       " 42834.68,\n",
       " 42826.96,\n",
       " 42712.16,\n",
       " 42488.39,\n",
       " 42392.09,\n",
       " 42310.96,\n",
       " 42216.71,\n",
       " 42126.63,\n",
       " 41955.81,\n",
       " 41604.7,\n",
       " 41575.52,\n",
       " 41272.68,\n",
       " 41209.06,\n",
       " 41014.12,\n",
       " 40838.39,\n",
       " 40674.69,\n",
       " 40365.63,\n",
       " 40312.21,\n",
       " 40240.8,\n",
       " 40218.37,\n",
       " 40209.09,\n",
       " 39934.32,\n",
       " 39898.93,\n",
       " 39893.78,\n",
       " 39812.22,\n",
       " 39772.36,\n",
       " 39632.97,\n",
       " 39455.15,\n",
       " 38696.92,\n",
       " 38631.77,\n",
       " 38479.91,\n",
       " 38288.45,\n",
       " 38272.51,\n",
       " 38228.54,\n",
       " 38203.33,\n",
       " 38061.4,\n",
       " 38055.51,\n",
       " 38007.75,\n",
       " 37963.37,\n",
       " 37868.46,\n",
       " 37774.8,\n",
       " 37664.28,\n",
       " 37555.24,\n",
       " 37259.05,\n",
       " 37199.73,\n",
       " 37090.28,\n",
       " 36968.19,\n",
       " 36848.17,\n",
       " 36297.46,\n",
       " 36190.71,\n",
       " 35755.04,\n",
       " 35681.61,\n",
       " 35386.52,\n",
       " 35125.81,\n",
       " 34622.34,\n",
       " 34621.07,\n",
       " 34553.53,\n",
       " 34098.0,\n",
       " 34050.94,\n",
       " 34024.41,\n",
       " 33497.77,\n",
       " 33448.64,\n",
       " 33214.96,\n",
       " 32969.1,\n",
       " 32915.28,\n",
       " 32312.23,\n",
       " 32187.64,\n",
       " 31592.12,\n",
       " 29418.82,\n",
       " 18493.71,\n",
       " 4246.804,\n",
       " 4182.584,\n",
       " 4169.862,\n",
       " 4146.092,\n",
       " 4085.169,\n",
       " 4028.038,\n",
       " 3919.829,\n",
       " 3914.767,\n",
       " 0.004902713,\n",
       " 0.004818215,\n",
       " 0.004691108,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(df['Altitude'], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "14528868",
   "metadata": {},
   "outputs": [],
   "source": [
    "best = [3.79774661, 2.50143278, 8.75329661, 8.87230654, 8.79869793,\n",
    "         3.59618521, 8.76162157, 8.74398984]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "0981ef8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9999]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classif(torch.from_numpy(poly.fit_transform([best])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "8b0363a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Interaction terms help marginally. \n",
    "#X_train, X_test = df.iloc[100:300, :-15], df.iloc[300:, :-15]\n",
    "#X_train, X_test = df.iloc[100:, :-15], df.iloc[:100, :-15]\n",
    "idxs = np.array(list(range(200, 400)) + list(range(100)))\n",
    "X_train, X_test = poly.fit_transform(df.iloc[idxs, :-15]), poly.fit_transform(df.iloc[100:200, :-15])\n",
    "#X_train, X_test = df.iloc[100:300, :-15], df.iloc[300:, :-15]\n",
    "\n",
    "# Stability level 1: cluster into flight time of 20 and 90.\n",
    "#Y_train, Y_test = df.iloc[:300, -1] > 90, df.iloc[300:, -1] > 90\n",
    "#Y_train, Y_test = df.iloc[100:, -1] > 90, df.iloc[:100, -1] > 90\n",
    "Y_train, Y_test = df.iloc[idxs, -1] > 90, df.iloc[100:200, -1] > 90\n",
    "#Y_train, Y_test = df.iloc[:300, -1] > 90, df.iloc[300:, -1] > 90\n",
    "\n",
    "reg = LR(penalty = 'l2')\n",
    "reg.fit(X_train, Y_train)\n",
    "sum(reg.predict(X_test) == Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "daf95079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>PolynomialFeatures()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PolynomialFeatures</label><div class=\"sk-toggleable__content\"><pre>PolynomialFeatures()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "PolynomialFeatures()"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "83492a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schord\n",
      "Sspan\n",
      "Ssweep\n",
      "Stip\n",
      "Bchord\n",
      "Bspan\n",
      "Bsweep\n",
      "Btip\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c4f951",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
